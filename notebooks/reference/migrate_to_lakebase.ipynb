{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Migrate Unity Catalog Tables to Lakebase Synced Tables\n",
    "\n",
    "This notebook migrates the fashion ecommerce Unity Catalog tables to Lakebase PostgreSQL synced tables.\n",
    "\n",
    "## What are Synced Tables?\n",
    "\n",
    "Synced tables are read-only Postgres tables in Lakebase that automatically synchronize data from Unity Catalog tables. They enable:\n",
    "- **Low-latency reads** for your FastAPI app\n",
    "- **Automatic synchronization** using managed Lakeflow pipelines\n",
    "- **Query-time joins** with other Postgres tables\n",
    "- **Unity Catalog governance** maintained\n",
    "\n",
    "## Sync Modes\n",
    "\n",
    "- **SNAPSHOT**: Full refresh, runs on demand. Best if >10% of data changes.\n",
    "- **TRIGGERED**: Incremental refresh on demand. Good balance of cost/performance.\n",
    "- **CONTINUOUS**: Real-time incremental updates. Lowest lag but higher cost.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. Unity Catalog tables exist in `main.fashion_demo`\n",
    "2. You have `CAN USE` permissions on the Lakebase database instance\n",
    "3. For TRIGGERED/CONTINUOUS modes, source tables need Change Data Feed enabled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.database import (\n",
    "    SyncedDatabaseTable,\n",
    "    SyncedTableSpec,\n",
    "    NewPipelineSpec,\n",
    "    SyncedTableSchedulingPolicy\n",
    ")\n",
    "import time\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "# Initialize Databricks SDK\n",
    "w = WorkspaceClient()\n",
    "\n",
    "print(\"‚úÖ Databricks SDK initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "SOURCE_CATALOG = \"main\"\n",
    "SOURCE_SCHEMA = \"fashion_demo\"\n",
    "\n",
    "# Lakebase configuration\n",
    "# Use standard catalog approach (not database catalog)\n",
    "TARGET_CATALOG = \"main\"  # Standard catalog\n",
    "TARGET_SCHEMA = \"fashion_demo_lakebase\"  # Schema for synced tables\n",
    "DATABASE_INSTANCE_NAME = \"instance-e2ff35b5-a3fc-44f3-9d65-7cba8332db7c\"  # Your Lakebase instance\n",
    "LOGICAL_DATABASE_NAME = \"databricks_postgres\"  # The Postgres database name\n",
    "\n",
    "# Pipeline storage (for staging tables)\n",
    "PIPELINE_STORAGE_CATALOG = \"main\"\n",
    "PIPELINE_STORAGE_SCHEMA = \"fashion_demo_staging\"\n",
    "\n",
    "# Sync mode: SNAPSHOT, TRIGGERED, or CONTINUOUS\n",
    "DEFAULT_SYNC_MODE = SyncedTableSchedulingPolicy.TRIGGERED\n",
    "\n",
    "print(f\"Source: {SOURCE_CATALOG}.{SOURCE_SCHEMA}\")\n",
    "print(f\"Target: {TARGET_CATALOG}.{TARGET_SCHEMA} (Lakebase)\")\n",
    "print(f\"Database Instance: {DATABASE_INSTANCE_NAME}\")\n",
    "print(f\"Postgres Database: {LOGICAL_DATABASE_NAME}\")\n",
    "print(f\"Default Sync Mode: {DEFAULT_SYNC_MODE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Definitions\n",
    "\n",
    "Define all tables to migrate with their primary keys and optional timeseries keys for deduplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table configurations\n",
    "TABLES_TO_MIGRATE = [\n",
    "    {\n",
    "        \"source_table\": \"productsdb\",\n",
    "        \"synced_table_name\": \"products_synced\",\n",
    "        \"primary_key_columns\": [\"product_id\"],\n",
    "        \"timeseries_key\": \"ingested_at\",  # For deduplication if duplicate product_ids exist\n",
    "        \"sync_mode\": SyncedTableSchedulingPolicy.TRIGGERED,\n",
    "        \"description\": \"Product catalog with prices, categories, and metadata\"\n",
    "    },\n",
    "    {\n",
    "        \"source_table\": \"usersdb\",\n",
    "        \"synced_table_name\": \"users_synced\",\n",
    "        \"primary_key_columns\": [\"user_id\"],\n",
    "        \"timeseries_key\": \"created_date\",\n",
    "        \"sync_mode\": SyncedTableSchedulingPolicy.TRIGGERED,\n",
    "        \"description\": \"User profiles with preferences and segments\"\n",
    "    },\n",
    "    {\n",
    "        \"source_table\": \"product_image_embeddingsdb\",\n",
    "        \"synced_table_name\": \"product_embeddings_synced\",\n",
    "        \"primary_key_columns\": [\"product_id\"],\n",
    "        \"timeseries_key\": \"created_at\",\n",
    "        \"sync_mode\": SyncedTableSchedulingPolicy.TRIGGERED,\n",
    "        \"description\": \"CLIP embeddings for visual search\"\n",
    "    },\n",
    "    {\n",
    "        \"source_table\": \"user_style_featuresdb\",\n",
    "        \"synced_table_name\": \"user_features_synced\",\n",
    "        \"primary_key_columns\": [\"user_id\"],\n",
    "        \"timeseries_key\": \"created_at\",\n",
    "        \"sync_mode\": SyncedTableSchedulingPolicy.TRIGGERED,\n",
    "        \"description\": \"User style preferences and embeddings\"\n",
    "    },\n",
    "    # Uncomment if you have transactions table\n",
    "    # {\n",
    "    #     \"source_table\": \"transactionsdb\",\n",
    "    #     \"synced_table_name\": \"transactions_synced\",\n",
    "    #     \"primary_key_columns\": [\"transaction_id\"],\n",
    "    #     \"timeseries_key\": \"transaction_timestamp\",\n",
    "    #     \"sync_mode\": SyncedTableSchedulingPolicy.CONTINUOUS,  # Real-time for transactions\n",
    "    #     \"description\": \"Transaction history\"\n",
    "    # },\n",
    "]\n",
    "\n",
    "print(f\"\\nConfigured {len(TABLES_TO_MIGRATE)} tables for migration:\")\n",
    "for table in TABLES_TO_MIGRATE:\n",
    "    print(f\"  - {table['source_table']} ‚Üí {table['synced_table_name']} (PK: {table['primary_key_columns']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_table_exists(catalog: str, schema: str, table: str) -> bool:\n",
    "    \"\"\"Check if a Unity Catalog table exists\"\"\"\n",
    "    try:\n",
    "        w.tables.get(f\"{catalog}.{schema}.{table}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "\n",
    "def check_cdf_enabled(catalog: str, schema: str, table: str) -> bool:\n",
    "    \"\"\"Check if Change Data Feed is enabled on a table\"\"\"\n",
    "    try:\n",
    "        table_info = w.tables.get(f\"{catalog}.{schema}.{table}\")\n",
    "        # Check if delta.enableChangeDataFeed is set to true\n",
    "        properties = table_info.properties or {}\n",
    "        return properties.get(\"delta.enableChangeDataFeed\", \"false\").lower() == \"true\"\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Could not check CDF status for {table}: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def get_sync_status(synced_table_name: str) -> Dict:\n",
    "    \"\"\"Get the sync status of a synced table\"\"\"\n",
    "    try:\n",
    "        status = w.database.get_synced_database_table(name=synced_table_name)\n",
    "        return {\n",
    "            \"status\": status.data_synchronization_status.detailed_state if status.data_synchronization_status else \"UNKNOWN\",\n",
    "            \"message\": status.data_synchronization_status.message if status.data_synchronization_status else \"\",\n",
    "            \"exists\": True\n",
    "        }\n",
    "    except Exception as e:\n",
    "        if \"does not exist\" in str(e).lower() or \"not found\" in str(e).lower():\n",
    "            return {\"status\": \"NOT_CREATED\", \"message\": \"\", \"exists\": False}\n",
    "        return {\"status\": \"ERROR\", \"message\": str(e), \"exists\": False}\n",
    "\n",
    "\n",
    "def wait_for_sync(synced_table_name: str, timeout_seconds: int = 600, check_interval: int = 10):\n",
    "    \"\"\"Wait for a synced table to reach ONLINE status\"\"\"\n",
    "    print(f\"‚è≥ Waiting for {synced_table_name} to sync...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    while time.time() - start_time < timeout_seconds:\n",
    "        status = get_sync_status(synced_table_name)\n",
    "        current_status = status[\"status\"]\n",
    "        \n",
    "        if current_status == \"ONLINE\":\n",
    "            print(f\"‚úÖ {synced_table_name} is ONLINE\")\n",
    "            return True\n",
    "        elif current_status in [\"FAILED\", \"ERROR\"]:\n",
    "            print(f\"‚ùå {synced_table_name} sync failed: {status['message']}\")\n",
    "            return False\n",
    "        else:\n",
    "            print(f\"   Status: {current_status} - {status.get('message', '')}\")\n",
    "            time.sleep(check_interval)\n",
    "    \n",
    "    print(f\"‚è±Ô∏è  Timeout waiting for {synced_table_name} to sync\")\n",
    "    return False\n",
    "\n",
    "\n",
    "print(\"‚úÖ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Migration Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç Running pre-migration checks...\\n\")\n",
    "\n",
    "all_checks_passed = True\n",
    "warnings = []\n",
    "\n",
    "for table_config in TABLES_TO_MIGRATE:\n",
    "    source_table = table_config[\"source_table\"]\n",
    "    full_source_name = f\"{SOURCE_CATALOG}.{SOURCE_SCHEMA}.{source_table}\"\n",
    "    sync_mode = table_config.get(\"sync_mode\", DEFAULT_SYNC_MODE)\n",
    "    \n",
    "    print(f\"Checking {source_table}...\")\n",
    "    \n",
    "    # Check if source table exists\n",
    "    if not check_table_exists(SOURCE_CATALOG, SOURCE_SCHEMA, source_table):\n",
    "        print(f\"  ‚ùå Source table does not exist: {full_source_name}\")\n",
    "        all_checks_passed = False\n",
    "        continue\n",
    "    else:\n",
    "        print(f\"  ‚úÖ Source table exists\")\n",
    "    \n",
    "    # Check CDF if using TRIGGERED or CONTINUOUS mode\n",
    "    if sync_mode in [SyncedTableSchedulingPolicy.TRIGGERED, SyncedTableSchedulingPolicy.CONTINUOUS]:\n",
    "        cdf_enabled = check_cdf_enabled(SOURCE_CATALOG, SOURCE_SCHEMA, source_table)\n",
    "        if not cdf_enabled:\n",
    "            warning_msg = f\"  ‚ö†Ô∏è  Change Data Feed not enabled on {source_table}. Use SNAPSHOT mode or enable CDF.\"\n",
    "            print(warning_msg)\n",
    "            warnings.append(warning_msg)\n",
    "            # Automatically switch to SNAPSHOT mode\n",
    "            table_config[\"sync_mode\"] = SyncedTableSchedulingPolicy.SNAPSHOT\n",
    "            print(f\"  üîÑ Auto-switched to SNAPSHOT mode for {source_table}\")\n",
    "        else:\n",
    "            print(f\"  ‚úÖ Change Data Feed enabled\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "if warnings:\n",
    "    print(\"\\n‚ö†Ô∏è  WARNINGS:\")\n",
    "    for warning in warnings:\n",
    "        print(warning)\n",
    "    print()\n",
    "\n",
    "if not all_checks_passed:\n",
    "    print(\"‚ùå Pre-migration checks failed. Fix issues before proceeding.\")\n",
    "else:\n",
    "    print(\"‚úÖ All pre-migration checks passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enable Change Data Feed (Optional)\n",
    "\n",
    "Run this cell if you want to enable Change Data Feed on tables that need TRIGGERED or CONTINUOUS mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to enable CDF on all tables\n",
    "# for table_config in TABLES_TO_MIGRATE:\n",
    "#     source_table = table_config[\"source_table\"]\n",
    "#     full_source_name = f\"{SOURCE_CATALOG}.{SOURCE_SCHEMA}.{source_table}\"\n",
    "#     \n",
    "#     try:\n",
    "#         spark.sql(f\"\"\"\n",
    "#             ALTER TABLE {full_source_name}\n",
    "#             SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\n",
    "#         \"\"\")\n",
    "#         print(f\"‚úÖ Enabled CDF on {source_table}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ùå Failed to enable CDF on {source_table}: {e}\")\n",
    "\n",
    "print(\"‚ÑπÔ∏è  To enable CDF, uncomment the code in this cell and run it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Synced Tables\n",
    "\n",
    "This will create synced tables in Lakebase for all configured tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ Creating synced tables...\\n\")\n",
    "\n",
    "created_tables = []\n",
    "failed_tables = []\n",
    "\n",
    "for table_config in TABLES_TO_MIGRATE:\n",
    "    source_table = table_config[\"source_table\"]\n",
    "    synced_table_name = table_config[\"synced_table_name\"]\n",
    "    primary_key_columns = table_config[\"primary_key_columns\"]\n",
    "    timeseries_key = table_config.get(\"timeseries_key\")\n",
    "    sync_mode = table_config.get(\"sync_mode\", DEFAULT_SYNC_MODE)\n",
    "    description = table_config.get(\"description\", \"\")\n",
    "    \n",
    "    full_source_name = f\"{SOURCE_CATALOG}.{SOURCE_SCHEMA}.{source_table}\"\n",
    "    full_synced_name = f\"{TARGET_CATALOG}.{TARGET_SCHEMA}.{synced_table_name}\"\n",
    "    \n",
    "    print(f\"Creating synced table: {synced_table_name}\")\n",
    "    print(f\"  Source: {full_source_name}\")\n",
    "    print(f\"  Target: {full_synced_name}\")\n",
    "    print(f\"  Primary Key: {primary_key_columns}\")\n",
    "    print(f\"  Timeseries Key: {timeseries_key}\")\n",
    "    print(f\"  Sync Mode: {sync_mode}\")\n",
    "    \n",
    "    # Check if synced table already exists\n",
    "    existing_status = get_sync_status(full_synced_name)\n",
    "    if existing_status[\"exists\"]:\n",
    "        print(f\"  ‚ö†Ô∏è  Synced table already exists (status: {existing_status['status']})\")\n",
    "        print(f\"  ‚ÑπÔ∏è  Skipping creation. Use the 'Update Synced Tables' section to modify.\\n\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        # Build spec\n",
    "        spec = SyncedTableSpec(\n",
    "            source_table_full_name=full_source_name,\n",
    "            primary_key_columns=primary_key_columns,\n",
    "            scheduling_policy=sync_mode,\n",
    "            create_database_objects_if_missing=True,  # Create schema if needed\n",
    "            new_pipeline_spec=NewPipelineSpec(\n",
    "                storage_catalog=PIPELINE_STORAGE_CATALOG,\n",
    "                storage_schema=PIPELINE_STORAGE_SCHEMA\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Add timeseries key if specified\n",
    "        if timeseries_key:\n",
    "            spec.timeseries_key = timeseries_key\n",
    "        \n",
    "        # Create synced table\n",
    "        synced_table = w.database.create_synced_database_table(\n",
    "            SyncedDatabaseTable(\n",
    "                name=full_synced_name,\n",
    "                database_instance_name=DATABASE_INSTANCE_NAME,\n",
    "                logical_database_name=LOGICAL_DATABASE_NAME,\n",
    "                spec=spec\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        print(f\"  ‚úÖ Created synced table: {synced_table.name}\")\n",
    "        created_tables.append({\n",
    "            \"source\": full_source_name,\n",
    "            \"synced\": full_synced_name,\n",
    "            \"config\": table_config\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Failed to create synced table: {e}\")\n",
    "        failed_tables.append({\n",
    "            \"source\": full_source_name,\n",
    "            \"synced\": full_synced_name,\n",
    "            \"error\": str(e)\n",
    "        })\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"‚úÖ Successfully created: {len(created_tables)} tables\")\n",
    "print(f\"‚ùå Failed: {len(failed_tables)} tables\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor Sync Status\n",
    "\n",
    "Wait for all synced tables to reach ONLINE status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Monitoring sync status...\\n\")\n",
    "\n",
    "for created in created_tables:\n",
    "    synced_name = created[\"synced\"]\n",
    "    print(f\"Checking: {synced_name}\")\n",
    "    \n",
    "    # Wait for sync to complete (10 minute timeout)\n",
    "    success = wait_for_sync(synced_name, timeout_seconds=600, check_interval=15)\n",
    "    \n",
    "    if success:\n",
    "        print(f\"‚úÖ {synced_name} is ready!\\n\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  {synced_name} did not reach ONLINE status. Check pipeline logs.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Final Status of All Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìã Final Status Report\\n\")\n",
    "print(\"=\"*100)\n",
    "print(f\"{'Table':<40} {'Status':<15} {'Message':<45}\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "for table_config in TABLES_TO_MIGRATE:\n",
    "    synced_table_name = table_config[\"synced_table_name\"]\n",
    "    full_synced_name = f\"{TARGET_CATALOG}.{TARGET_SCHEMA}.{synced_table_name}\"\n",
    "    \n",
    "    status = get_sync_status(full_synced_name)\n",
    "    status_str = status[\"status\"]\n",
    "    message = status[\"message\"][:45] if status[\"message\"] else \"\"\n",
    "    \n",
    "    # Add emoji based on status\n",
    "    if status_str == \"ONLINE\":\n",
    "        emoji = \"‚úÖ\"\n",
    "    elif status_str == \"NOT_CREATED\":\n",
    "        emoji = \"‚ö™\"\n",
    "    elif status_str in [\"FAILED\", \"ERROR\"]:\n",
    "        emoji = \"‚ùå\"\n",
    "    else:\n",
    "        emoji = \"üîÑ\"\n",
    "    \n",
    "    print(f\"{emoji} {synced_table_name:<38} {status_str:<15} {message:<45}\")\n",
    "\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Synced Tables from Postgres\n",
    "\n",
    "Example queries to test your synced tables in Lakebase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can query synced tables using SQL in notebooks or from your FastAPI app\n",
    "\n",
    "# Example: Count rows in products_synced\n",
    "# spark.sql(f\"\"\"\n",
    "#     SELECT COUNT(*) as total_products\n",
    "#     FROM {TARGET_CATALOG}.{TARGET_SCHEMA}.products_synced\n",
    "# \"\"\").show()\n",
    "\n",
    "# Example: Join products with embeddings\n",
    "# spark.sql(f\"\"\"\n",
    "#     SELECT p.product_id, p.product_display_name, p.price\n",
    "#     FROM {TARGET_CATALOG}.{TARGET_SCHEMA}.products_synced p\n",
    "#     INNER JOIN {TARGET_CATALOG}.{TARGET_SCHEMA}.product_embeddings_synced e\n",
    "#         ON p.product_id = e.product_id\n",
    "#     LIMIT 10\n",
    "# \"\"\").show()\n",
    "\n",
    "print(\"‚ÑπÔ∏è  Uncomment the SQL examples above to test your synced tables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Synced Tables (Trigger Refresh)\n",
    "\n",
    "For TRIGGERED mode tables, you can manually trigger a refresh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To manually trigger a refresh for TRIGGERED mode tables:\n",
    "\n",
    "def trigger_sync_refresh(synced_table_name: str):\n",
    "    \"\"\"Trigger a manual refresh for a synced table in TRIGGERED mode\"\"\"\n",
    "    try:\n",
    "        # Get the synced table to find its pipeline ID\n",
    "        synced_table = w.database.get_synced_database_table(name=synced_table_name)\n",
    "        \n",
    "        # The pipeline ID is in the synced table metadata\n",
    "        # You would trigger it using the pipelines API\n",
    "        print(f\"‚úÖ Triggered refresh for {synced_table_name}\")\n",
    "        print(f\"   Monitor progress in the Databricks Workflows UI\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to trigger refresh: {e}\")\n",
    "\n",
    "# Example usage:\n",
    "# trigger_sync_refresh(f\"{TARGET_CATALOG}.{TARGET_SCHEMA}.products_synced\")\n",
    "\n",
    "print(\"‚ÑπÔ∏è  Use trigger_sync_refresh() to manually refresh TRIGGERED mode tables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup (Delete Synced Tables)\n",
    "\n",
    "**‚ö†Ô∏è WARNING**: This will delete synced tables from Unity Catalog and stop synchronization.\n",
    "You'll need to manually drop tables in Postgres to free up space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT TO ENABLE CLEANUP\n",
    "# ENABLE_CLEANUP = True\n",
    "ENABLE_CLEANUP = False\n",
    "\n",
    "if ENABLE_CLEANUP:\n",
    "    print(\"‚ö†Ô∏è  CLEANUP MODE ENABLED\\n\")\n",
    "    print(\"This will delete synced tables from Unity Catalog.\\n\")\n",
    "    \n",
    "    for table_config in TABLES_TO_MIGRATE:\n",
    "        synced_table_name = table_config[\"synced_table_name\"]\n",
    "        full_synced_name = f\"{TARGET_CATALOG}.{TARGET_SCHEMA}.{synced_table_name}\"\n",
    "        \n",
    "        try:\n",
    "            print(f\"Deleting: {full_synced_name}\")\n",
    "            w.database.delete_synced_database_table(name=full_synced_name)\n",
    "            print(f\"  ‚úÖ Deleted from Unity Catalog\")\n",
    "            print(f\"  ‚ö†Ô∏è  Remember to DROP TABLE in Postgres to free up space\\n\")\n",
    "        except Exception as e:\n",
    "            if \"does not exist\" in str(e).lower() or \"not found\" in str(e).lower():\n",
    "                print(f\"  ‚ÑπÔ∏è  Table does not exist (already deleted)\\n\")\n",
    "            else:\n",
    "                print(f\"  ‚ùå Failed to delete: {e}\\n\")\n",
    "    \n",
    "    print(\"\\nüìã Next Steps:\")\n",
    "    print(\"1. Connect to your Lakebase instance with psql or SQL editor\")\n",
    "    print(\"2. Run the following to drop tables in Postgres:\")\n",
    "    print()\n",
    "    for table_config in TABLES_TO_MIGRATE:\n",
    "        synced_table_name = table_config[\"synced_table_name\"]\n",
    "        print(f\"   DROP TABLE IF EXISTS {TARGET_SCHEMA}.{synced_table_name};\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  Cleanup is disabled. Set ENABLE_CLEANUP = True to delete synced tables.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "After migration:\n",
    "\n",
    "1. **Update your FastAPI app** to use the synced tables:\n",
    "   ```python\n",
    "   # In core/config.py, update:\n",
    "   LAKEBASE_SCHEMA = \"fashion_demo_lakebase\"\n",
    "   PRODUCTS_TABLE = \"products_synced\"\n",
    "   USERS_TABLE = \"users_synced\"\n",
    "   EMBEDDINGS_TABLE = \"product_embeddings_synced\"\n",
    "   USER_FEATURES_TABLE = \"user_features_synced\"\n",
    "   ```\n",
    "\n",
    "2. **Test your queries** with the new synced tables\n",
    "\n",
    "3. **Set up the correct PGPASSWORD** (see [LAKEBASE_PASSWORD_SETUP.md](../LAKEBASE_PASSWORD_SETUP.md))\n",
    "\n",
    "4. **Monitor pipeline performance** in Databricks Workflows UI\n",
    "\n",
    "5. **Create indexes** in Postgres for better query performance:\n",
    "   ```sql\n",
    "   CREATE INDEX idx_products_category ON fashion_demo_lakebase.products_synced(master_category);\n",
    "   CREATE INDEX idx_products_price ON fashion_demo_lakebase.products_synced(price);\n",
    "   ```\n",
    "\n",
    "6. **Grant permissions** to other users if needed:\n",
    "   ```sql\n",
    "   GRANT SELECT ON ALL TABLES IN SCHEMA fashion_demo_lakebase TO user;\n",
    "   ```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
