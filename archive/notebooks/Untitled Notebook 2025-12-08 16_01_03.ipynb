{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e69b69e2-6039-4aef-abdc-415a76aa744e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "services/clip_service.py"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CLIP Model Serving service for generating embeddings\n",
    "Endpoint: clip-image-encoder\n",
    "Dimension: 512 (ViT-B/32)\n",
    "\"\"\"\n",
    "import base64\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class CLIPService:\n",
    "    \"\"\"Service for interacting with CLIP model serving endpoint\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.endpoint_name = \"clip-image-encoder\"\n",
    "        self.workspace_host = os.getenv(\"DATABRICKS_HOST\", \"\")\n",
    "        if not self.workspace_host.startswith(\"http\"):\n",
    "            self.workspace_host = f\"https://{self.workspace_host}\"\n",
    "        self.embedding_dim = 512  # CLIP ViT-B/32\n",
    "        \n",
    "    def _get_endpoint_url(self) -> str:\n",
    "        \"\"\"Construct the full endpoint URL\"\"\"\n",
    "        return f\"{self.workspace_host}/serving-endpoints/{self.endpoint_name}/invocations\"\n",
    "    \n",
    "    def _get_auth_headers(self) -> dict:\n",
    "        \"\"\"Get authorization headers with fresh OAuth token\"\"\"\n",
    "        from databricks.sdk import WorkspaceClient\n",
    "        w = WorkspaceClient()\n",
    "        token = w.config.oauth_token().access_token\n",
    "        return {\n",
    "            \"Authorization\": f\"Bearer {token}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "    \n",
    "    async def get_image_embedding(self, image_bytes: bytes) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate CLIP embedding for an image\n",
    "        \n",
    "        Args:\n",
    "            image_bytes: Raw image bytes (JPEG, PNG, etc.)\n",
    "            \n",
    "        Returns:\n",
    "            numpy array of shape (512,) - L2 normalized embedding\n",
    "        \"\"\"\n",
    "        import aiohttp\n",
    "        \n",
    "        try:\n",
    "            # Encode image to base64\n",
    "            image_b64 = base64.b64encode(image_bytes).decode(\"utf-8\")\n",
    "            \n",
    "            # Prepare payload in dataframe_records format (pyfunc model)\n",
    "            payload = {\n",
    "                \"dataframe_records\": [{\"image\": image_b64}]\n",
    "            }\n",
    "            \n",
    "            logger.info(f\"Calling CLIP endpoint for image embedding (size: {len(image_bytes)} bytes)\")\n",
    "            \n",
    "            # Call Model Serving endpoint\n",
    "            timeout = aiohttp.ClientTimeout(total=30)\n",
    "            async with aiohttp.ClientSession(timeout=timeout) as session:\n",
    "                async with session.post(\n",
    "                    self._get_endpoint_url(),\n",
    "                    json=payload,\n",
    "                    headers=self._get_auth_headers()\n",
    "                ) as response:\n",
    "                    if response.status != 200:\n",
    "                        error_text = await response.text()\n",
    "                        logger.error(f\"CLIP endpoint error {response.status}: {error_text}\")\n",
    "                        raise Exception(f\"CLIP endpoint returned {response.status}\")\n",
    "                    \n",
    "                    result = await response.json()\n",
    "            \n",
    "            # Parse response - handle different formats\n",
    "            if isinstance(result, dict) and \"predictions\" in result:\n",
    "                embedding = np.array(result[\"predictions\"][0], dtype=np.float32)\n",
    "            elif isinstance(result, list):\n",
    "                embedding = np.array(result[0] if result else [], dtype=np.float32)\n",
    "            else:\n",
    "                embedding = np.array(result, dtype=np.float32)\n",
    "            \n",
    "            # Ensure L2 normalization (for cosine similarity on VS)\n",
    "            norm = np.linalg.norm(embedding)\n",
    "            if norm > 0:\n",
    "                embedding = embedding / norm\n",
    "            \n",
    "            logger.info(f\"Generated image embedding: shape={embedding.shape}, norm={np.linalg.norm(embedding):.4f}\")\n",
    "            \n",
    "            return embedding\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating image embedding: {type(e).__name__}: {e}\")\n",
    "            raise\n",
    "    \n",
    "    async def get_text_embedding(self, text: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate CLIP embedding for text query\n",
    "        \n",
    "        Args:\n",
    "            text: Text query (e.g., \"red summer dress\")\n",
    "            \n",
    "        Returns:\n",
    "            numpy array of shape (512,) - L2 normalized embedding\n",
    "        \"\"\"\n",
    "        import aiohttp\n",
    "        \n",
    "        try:\n",
    "            # Prepare payload for text mode\n",
    "            payload = {\n",
    "                \"dataframe_records\": [{\"text\": text}]\n",
    "            }\n",
    "            \n",
    "            logger.info(f\"Calling CLIP endpoint for text embedding: '{text}'\")\n",
    "            \n",
    "            # Call Model Serving endpoint\n",
    "            timeout = aiohttp.ClientTimeout(total=30)\n",
    "            async with aiohttp.ClientSession(timeout=timeout) as session:\n",
    "                async with session.post(\n",
    "                    self._get_endpoint_url(),\n",
    "                    json=payload,\n",
    "                    headers=self._get_auth_headers()\n",
    "                ) as response:\n",
    "                    if response.status != 200:\n",
    "                        error_text = await response.text()\n",
    "                        logger.error(f\"CLIP endpoint error {response.status}: {error_text}\")\n",
    "                        raise Exception(f\"CLIP endpoint returned {response.status}\")\n",
    "                    \n",
    "                    result = await response.json()\n",
    "            \n",
    "            # Parse response\n",
    "            if isinstance(result, dict) and \"predictions\" in result:\n",
    "                embedding = np.array(result[\"predictions\"][0], dtype=np.float32)\n",
    "            elif isinstance(result, list):\n",
    "                embedding = np.array(result[0] if result else [], dtype=np.float32)\n",
    "            else:\n",
    "                embedding = np.array(result, dtype=np.float32)\n",
    "            \n",
    "            # Ensure L2 normalization\n",
    "            norm = np.linalg.norm(embedding)\n",
    "            if norm > 0:\n",
    "                embedding = embedding / norm\n",
    "            \n",
    "            logger.info(f\"Generated text embedding: shape={embedding.shape}, norm={np.linalg.norm(embedding):.4f}\")\n",
    "            \n",
    "            return embedding\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating text embedding: {type(e).__name__}: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "# Singleton instance\n",
    "clip_service = CLIPService()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "faf3bbf3-6587-4e21-b3fe-b3b5ed24334b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "services/vector_search_service.py"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Vector Search service for similarity queries\n",
    "Endpoint: fashion_vector_search (4d329fc8-1924-4131-ace8-14b542f8c14b)\n",
    "Index: main.fashion_demo.product_embeddings_index\n",
    "\"\"\"\n",
    "import logging\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Optional\n",
    "from databricks.vector_search.client import VectorSearchClient\n",
    "import os\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class VectorSearchService:\n",
    "    \"\"\"Service for Vector Search similarity queries\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.endpoint_name = \"fashion_vector_search\"\n",
    "        self.endpoint_id = \"4d329fc8-1924-4131-ace8-14b542f8c14b\"\n",
    "        self.index_name = \"main.fashion_demo.product_embeddings_index\"\n",
    "        self.embedding_dim = 512\n",
    "        self.workspace_host = os.getenv(\"DATABRICKS_HOST\", \"\")\n",
    "        if not self.workspace_host.startswith(\"http\"):\n",
    "            self.workspace_host = f\"https://{self.workspace_host}\"\n",
    "        self._client = None\n",
    "        self._index = None\n",
    "    \n",
    "    def _get_client(self) -> VectorSearchClient:\n",
    "        \"\"\"Get or create Vector Search client\"\"\"\n",
    "        if self._client is None:\n",
    "            # VectorSearchClient uses workspace auth automatically in Databricks Apps\n",
    "            self._client = VectorSearchClient(\n",
    "                workspace_url=self.workspace_host,\n",
    "                disable_notice=True\n",
    "            )\n",
    "            logger.info(f\"Created Vector Search client for {self.workspace_host}\")\n",
    "        return self._client\n",
    "    \n",
    "    def _get_index(self):\n",
    "        \"\"\"Get Vector Search index\"\"\"\n",
    "        if self._index is None:\n",
    "            client = self._get_client()\n",
    "            self._index = client.get_index(self.index_name)\n",
    "            logger.info(f\"Connected to Vector Search index: {self.index_name}\")\n",
    "        return self._index\n",
    "    \n",
    "    async def similarity_search(\n",
    "        self,\n",
    "        query_vector: np.ndarray,\n",
    "        num_results: int = 20,\n",
    "        filters: Optional[Dict[str, Any]] = None\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Search for similar products using vector similarity\n",
    "        \n",
    "        Args:\n",
    "            query_vector: Normalized embedding vector (512 dims)\n",
    "            num_results: Number of results to return\n",
    "            filters: Optional filters (e.g., {\"price >= \": 50})\n",
    "            \n",
    "        Returns:\n",
    "            List of product dictionaries with similarity scores\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Ensure vector is normalized and correct shape\n",
    "            if query_vector.shape != (self.embedding_dim,):\n",
    "                raise ValueError(f\"Expected vector shape ({self.embedding_dim},), got {query_vector.shape}\")\n",
    "            \n",
    "            # Ensure L2 normalization for cosine-like similarity\n",
    "            norm = np.linalg.norm(query_vector)\n",
    "            if norm > 0:\n",
    "                query_vector = query_vector / norm\n",
    "            \n",
    "            logger.info(f\"Vector Search query: dim={query_vector.shape[0]}, norm={np.linalg.norm(query_vector):.4f}, filters={filters}\")\n",
    "            \n",
    "            # Get index and perform similarity search\n",
    "            index = self._get_index()\n",
    "            \n",
    "            # Columns to return from the index\n",
    "            columns = [\n",
    "                \"product_id\",\n",
    "                \"product_display_name\", \n",
    "                \"master_category\",\n",
    "                \"sub_category\",\n",
    "                \"article_type\",\n",
    "                \"base_color\",\n",
    "                \"price\",\n",
    "                \"image_path\",\n",
    "                \"gender\",\n",
    "                \"season\",\n",
    "                \"usage\",\n",
    "                \"year\"\n",
    "            ]\n",
    "            \n",
    "            # Perform similarity search\n",
    "            # Note: Vector Search SDK is synchronous, wrap in executor\n",
    "            import asyncio\n",
    "            loop = asyncio.get_event_loop()\n",
    "            \n",
    "            def do_search():\n",
    "                return index.similarity_search(\n",
    "                    query_vector=query_vector.tolist(),\n",
    "                    columns=columns,\n",
    "                    num_results=num_results,\n",
    "                    filters=filters\n",
    "                )\n",
    "            \n",
    "            results = await loop.run_in_executor(None, do_search)\n",
    "            \n",
    "            # Parse results\n",
    "            if \"result\" in results and \"data_array\" in results[\"result\"]:\n",
    "                data_array = results[\"result\"][\"data_array\"]\n",
    "                logger.info(f\"Vector Search returned {len(data_array)} results\")\n",
    "                \n",
    "                # Convert to list of dicts\n",
    "                products = []\n",
    "                for row in data_array:\n",
    "                    product = dict(zip(columns, row))\n",
    "                    products.append(product)\n",
    "                \n",
    "                return products\n",
    "            else:\n",
    "                logger.warning(f\"Unexpected Vector Search response format: {results}\")\n",
    "                return []\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Vector Search error: {type(e).__name__}: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "# Singleton instance\n",
    "vector_search_service = VectorSearchService()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b64f64f-80c6-4f2e-9f1d-9d1d0ffabb8e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "routes/v1/search.py - With Vector Search"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Search API routes with CLIP + Vector Search integration\n",
    "\"\"\"\n",
    "from fastapi import APIRouter, HTTPException, UploadFile, File, Form, Depends\n",
    "from sqlalchemy.ext.asyncio import AsyncSession\n",
    "from typing import Optional\n",
    "from models.schemas import SearchRequest, SearchResponse, ProductDetail\n",
    "from repositories.lakebase import LakebaseRepository\n",
    "from core.database import get_async_db\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "router = APIRouter(prefix=\"/search\", tags=[\"search\"])\n",
    "\n",
    "# Get workspace host for constructing Files API URLs\n",
    "WORKSPACE_HOST = os.getenv(\"DATABRICKS_HOST\", \"\")\n",
    "if WORKSPACE_HOST and not WORKSPACE_HOST.startswith(\"http\"):\n",
    "    WORKSPACE_HOST = f\"https://{WORKSPACE_HOST}\"\n",
    "\n",
    "\n",
    "def get_image_url(product_id: int) -> str:\n",
    "    \"\"\"\n",
    "    Construct direct Files API URL for product image\n",
    "    \"\"\"\n",
    "    return f\"{WORKSPACE_HOST}/ajax-api/2.0/fs/files/Volumes/main/fashion_demo/raw_data/images/{product_id}.jpg\"\n",
    "\n",
    "\n",
    "@router.post(\"/text\", response_model=SearchResponse)\n",
    "async def search_by_text(\n",
    "    request: SearchRequest,\n",
    "    db: AsyncSession = Depends(get_async_db)\n",
    "):\n",
    "    \"\"\"\n",
    "    Semantic text search using CLIP embeddings + Vector Search\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from services.clip_service import clip_service\n",
    "        from services.vector_search_service import vector_search_service\n",
    "        \n",
    "        logger.info(f\"Text search request: '{request.query}' (limit={request.limit})\")\n",
    "        \n",
    "        # Generate text embedding using CLIP\n",
    "        text_embedding = await clip_service.get_text_embedding(request.query)\n",
    "        \n",
    "        # Search for similar products using Vector Search\n",
    "        products_data = await vector_search_service.similarity_search(\n",
    "            query_vector=text_embedding,\n",
    "            num_results=request.limit,\n",
    "            filters=request.filters\n",
    "        )\n",
    "        \n",
    "        # Convert to ProductDetail\n",
    "        products = []\n",
    "        for p in products_data:\n",
    "            product = ProductDetail(**p)\n",
    "            product.image_url = get_image_url(int(product.product_id))\n",
    "            # Similarity score comes from Vector Search\n",
    "            product.similarity_score = p.get(\"score\", 0.85)\n",
    "            products.append(product)\n",
    "        \n",
    "        logger.info(f\"Text search returned {len(products)} results\")\n",
    "        \n",
    "        return SearchResponse(\n",
    "            products=products,\n",
    "            query=request.query,\n",
    "            search_type=\"text\",\n",
    "            user_id=request.user_id\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Text search error: {type(e).__name__}: {e}\")\n",
    "        # Fallback to basic text search if Vector Search fails\n",
    "        logger.warning(\"Falling back to basic text search\")\n",
    "        repo = LakebaseRepository(db)\n",
    "        products_data = await repo.search_products_by_text(\n",
    "            query=request.query,\n",
    "            limit=request.limit\n",
    "        )\n",
    "        \n",
    "        products = []\n",
    "        for p in products_data:\n",
    "            product = ProductDetail(**p)\n",
    "            product.image_url = get_image_url(int(product.product_id))\n",
    "            product.similarity_score = 0.75\n",
    "            products.append(product)\n",
    "        \n",
    "        return SearchResponse(\n",
    "            products=products,\n",
    "            query=request.query,\n",
    "            search_type=\"text\",\n",
    "            user_id=request.user_id\n",
    "        )\n",
    "\n",
    "\n",
    "@router.post(\"/image\", response_model=SearchResponse)\n",
    "async def search_by_image(\n",
    "    image: UploadFile = File(...),\n",
    "    user_id: Optional[str] = Form(None),\n",
    "    limit: int = Form(20),\n",
    "    db: AsyncSession = Depends(get_async_db)\n",
    "):\n",
    "    \"\"\"\n",
    "    Visual search using CLIP image embeddings + Vector Search\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from services.clip_service import clip_service\n",
    "        from services.vector_search_service import vector_search_service\n",
    "        \n",
    "        logger.info(f\"Image search request: {image.filename} (limit={limit})\")\n",
    "        \n",
    "        # Read uploaded image\n",
    "        image_bytes = await image.read()\n",
    "        \n",
    "        # Generate image embedding using CLIP\n",
    "        image_embedding = await clip_service.get_image_embedding(image_bytes)\n",
    "        \n",
    "        # Search for similar products using Vector Search\n",
    "        products_data = await vector_search_service.similarity_search(\n",
    "            query_vector=image_embedding,\n",
    "            num_results=limit\n",
    "        )\n",
    "        \n",
    "        # Convert to ProductDetail\n",
    "        products = []\n",
    "        for p in products_data:\n",
    "            product = ProductDetail(**p)\n",
    "            product.image_url = get_image_url(int(product.product_id))\n",
    "            # Similarity score comes from Vector Search\n",
    "            product.similarity_score = p.get(\"score\", 0.85)\n",
    "            products.append(product)\n",
    "        \n",
    "        logger.info(f\"Image search returned {len(products)} results\")\n",
    "        \n",
    "        return SearchResponse(\n",
    "            products=products,\n",
    "            query=None,\n",
    "            search_type=\"image\",\n",
    "            user_id=user_id\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Image search error: {type(e).__name__}: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=f\"Image search failed: {str(e)}\")\n",
    "\n",
    "\n",
    "@router.get(\"/recommendations/{user_id}\", response_model=SearchResponse)\n",
    "async def get_recommendations(\n",
    "    user_id: str,\n",
    "    limit: int = 20,\n",
    "    db: AsyncSession = Depends(get_async_db)\n",
    "):\n",
    "    \"\"\"\n",
    "    Hybrid personalized recommendations:\n",
    "    - Vector similarity using user embeddings (60%)\n",
    "    - Rule-based filtering by preferences (40%)\n",
    "    \"\"\"\n",
    "    repo = LakebaseRepository(db)\n",
    "\n",
    "    # Load persona to get preferences\n",
    "    from routes.v1.users import load_personas\n",
    "\n",
    "    personas = load_personas()\n",
    "    persona = next((p for p in personas if p[\"user_id\"] == user_id), None)\n",
    "\n",
    "    if not persona:\n",
    "        raise HTTPException(status_code=404, detail=f\"User {user_id} not found\")\n",
    "\n",
    "    logger.info(f\"Getting recommendations for user {user_id} - {persona.get('name', 'Unknown')}\")\n",
    "    logger.info(f\"Persona preferences: categories={persona.get('preferred_categories')}, colors={persona.get('color_prefs')}\")\n",
    "\n",
    "    try:\n",
    "        # Try to get user embedding from user_style_features table\n",
    "        user_features = await repo.get_user_style_features(user_id)\n",
    "        \n",
    "        if user_features and user_features.get(\"user_embedding\"):\n",
    "            # Use Vector Search with user embedding\n",
    "            from services.vector_search_service import vector_search_service\n",
    "            \n",
    "            user_embedding = np.array(user_features[\"user_embedding\"], dtype=np.float32)\n",
    "            \n",
    "            # Build price filters for Vector Search\n",
    "            min_price = persona[\"p25_price\"] * 0.8\n",
    "            max_price = persona[\"p75_price\"] * 1.2\n",
    "            \n",
    "            # Vector Search with filters\n",
    "            products_data = await vector_search_service.similarity_search(\n",
    "                query_vector=user_embedding,\n",
    "                num_results=limit * 2,  # Get more for additional filtering\n",
    "                filters={\"price >= \": min_price, \"price <= \": max_price}\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"Vector Search returned {len(products_data)} products\")\n",
    "            \n",
    "        else:\n",
    "            # Fallback to rule-based if no user embedding\n",
    "            logger.warning(f\"No user embedding found for {user_id}, using rule-based recommendations\")\n",
    "            raise Exception(\"No user embedding - use fallback\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Vector Search failed, using rule-based fallback: {e}\")\n",
    "        \n",
    "        # Fallback: Rule-based recommendations\n",
    "        filters = {}\n",
    "        filters[\"min_price\"] = persona[\"p25_price\"] * 0.8\n",
    "        filters[\"max_price\"] = persona[\"p75_price\"] * 1.2\n",
    "        \n",
    "        # ✅ ADD: Filter by preferred master_category\n",
    "        if persona.get(\"preferred_categories\"):\n",
    "            filters[\"master_category\"] = persona[\"preferred_categories\"][0]\n",
    "            logger.info(f\"Filtering by category: {filters['master_category']}\")\n",
    "        \n",
    "        products_data = await repo.get_products(\n",
    "            limit=limit * 3,\n",
    "            filters=filters\n",
    "        )\n",
    "\n",
    "    # Normalize preferred colors to Title Case for matching\n",
    "    preferred_colors = set(c.title() for c in persona[\"color_prefs\"])\n",
    "    logger.info(f\"Normalized color preferences: {preferred_colors}\")\n",
    "    \n",
    "    filtered_products = []\n",
    "\n",
    "    for p in products_data:\n",
    "        # Normalize product color to Title Case\n",
    "        product_color = (p[\"base_color\"] or \"\").title()\n",
    "        color_match = product_color in preferred_colors\n",
    "        \n",
    "        # Check category match\n",
    "        category_match = p.get(\"master_category\") in persona.get(\"preferred_categories\", [])\n",
    "\n",
    "        product = ProductDetail(**p)\n",
    "        product.image_url = get_image_url(int(product.product_id))\n",
    "\n",
    "        # Calculate hybrid score\n",
    "        # If from Vector Search, use that score (60%) + rules (40%)\n",
    "        # If rule-based only, use rules (100%)\n",
    "        vector_score = p.get(\"score\", 0.5)  # From Vector Search or default\n",
    "        rule_score = 0.0\n",
    "        \n",
    "        # Category match bonus\n",
    "        if category_match:\n",
    "            rule_score += 0.3\n",
    "        \n",
    "        # Color match bonus\n",
    "        if color_match:\n",
    "            rule_score += 0.4\n",
    "        \n",
    "        # Price match bonus\n",
    "        price_diff = abs(p[\"price\"] - persona[\"avg_price\"])\n",
    "        price_range = persona[\"max_price\"] - persona[\"min_price\"]\n",
    "        if price_range > 0:\n",
    "            price_score = 1 - (price_diff / price_range)\n",
    "            rule_score += 0.3 * max(0, price_score)\n",
    "        \n",
    "        # Hybrid score: 60% vector + 40% rules\n",
    "        if \"score\" in p:  # Has vector similarity\n",
    "            product.similarity_score = 0.6 * vector_score + 0.4 * rule_score\n",
    "        else:  # Rule-based only\n",
    "            product.similarity_score = rule_score\n",
    "\n",
    "        # Add personalization reasons\n",
    "        reasons = []\n",
    "        if category_match:\n",
    "            reasons.append(f\"Matches your interest in {p['master_category']}\")\n",
    "        if color_match:\n",
    "            reasons.append(f\"Matches your preference for {product_color} items\")\n",
    "        if persona[\"min_price\"] <= p[\"price\"] <= persona[\"max_price\"]:\n",
    "            reasons.append(f\"Within your typical price range (${persona['min_price']:.0f}-${persona['max_price']:.0f})\")\n",
    "        if \"score\" in p and p[\"score\"] > 0.8:\n",
    "            reasons.append(\"Similar to items you've liked before\")\n",
    "\n",
    "        if reasons:\n",
    "            product.personalization_reason = \" • \".join(reasons)\n",
    "\n",
    "        filtered_products.append(product)\n",
    "\n",
    "    # Sort by hybrid score and limit\n",
    "    filtered_products.sort(key=lambda x: x.similarity_score or 0, reverse=True)\n",
    "    products = filtered_products[:limit]\n",
    "    \n",
    "    logger.info(f\"Returning {len(products)} personalized recommendations (avg score: {np.mean([p.similarity_score for p in products]):.2f})\")\n",
    "\n",
    "    return SearchResponse(\n",
    "        products=products,\n",
    "        query=None,\n",
    "        search_type=\"personalized\",\n",
    "        user_id=user_id\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0954d2a3-f187-4141-ae3c-265967fe6006",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Add to core/config.py"
    }
   },
   "outputs": [],
   "source": [
    "# Add these to your Settings class in core/config.py\n",
    "\n",
    "# CLIP Model Serving\n",
    "CLIP_ENDPOINT: str = \"clip-image-encoder\"\n",
    "CLIP_EMBEDDING_DIM: int = 512\n",
    "\n",
    "# Vector Search\n",
    "VS_ENDPOINT_NAME: str = \"fashion_vector_search\"\n",
    "VS_ENDPOINT_ID: str = \"4d329fc8-1924-4131-ace8-14b542f8c14b\"\n",
    "VS_INDEX_NAME: str = \"main.fashion_demo.product_embeddings_index\"\n",
    "\n",
    "# Workspace (already exists, just verify)\n",
    "WORKSPACE_HOST: str = os.getenv(\"DATABRICKS_HOST\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "064c9e20-10a9-43aa-80c2-2f3255b6ff7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Add these dependencies to requirements.txt\n",
    "\n",
    "# Vector Search\n",
    "databricks-vector-search>=0.22\n",
    "\n",
    "# HTTP client for async requests\n",
    "aiohttp>=3.9.0\n",
    "\n",
    "# Already have these (verify):\n",
    "# numpy>=1.24.0\n",
    "# databricks-sdk>=0.18.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdb637cb-ee84-43b1-8868-d4490e59c951",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Implementation Steps\n",
    "\n",
    "## Step 1: Create services/ directory\n",
    "```bash\n",
    "cd fashion-ecom-site\n",
    "mkdir -p services\n",
    "touch services/__init__.py\n",
    "```\n",
    "\n",
    "## Step 2: Copy service modules\n",
    "- **Cell 1** → `services/clip_service.py`\n",
    "- **Cell 2** → `services/vector_search_service.py`\n",
    "\n",
    "## Step 3: Update existing files\n",
    "- **Cell 3** → Replace `routes/v1/search.py`\n",
    "- **Cell 4** → Add to `core/config.py` (Settings class)\n",
    "- **Cell 5** → Add to `requirements.txt`\n",
    "\n",
    "## Step 4: Install dependencies\n",
    "```bash\n",
    "pip install databricks-vector-search>=0.22 aiohttp>=3.9.0\n",
    "```\n",
    "\n",
    "## Step 5: Fix personas.json (CRITICAL!)\n",
    "The file is currently empty. Copy the corrected JSON from the other notebook.\n",
    "\n",
    "## Step 6: Redeploy app\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Get\n",
    "\n",
    "### Text Search (Semantic)\n",
    "- Query: \"red summer dress\"\n",
    "- Uses CLIP to understand meaning\n",
    "- Finds \"Scarlet Sundress\", \"Coral Maxi Dress\" (semantic matches!)\n",
    "- Ranked by vector similarity\n",
    "\n",
    "### Image Search (Visual)\n",
    "- Upload photo of a dress\n",
    "- CLIP generates image embedding\n",
    "- Finds visually similar products\n",
    "- Works even if colors/styles differ slightly\n",
    "\n",
    "### Recommendations (Hybrid)\n",
    "- **60%** Vector similarity (user embedding vs product embeddings)\n",
    "- **40%** Rule-based (category, color, price preferences)\n",
    "- Personalized reasons: \"Similar to items you've liked before\"\n",
    "- Each persona gets truly different results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "666353e8-da3e-4700-a39b-dbfc6407894f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6160301153463628,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Untitled Notebook 2025-12-08 16_01_03",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
