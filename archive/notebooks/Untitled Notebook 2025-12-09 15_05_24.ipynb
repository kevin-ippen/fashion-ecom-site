{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb2df6e2-7908-42e0-9b42-c9af897ad7cd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 1: Create Enriched Product Embeddings Table"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create enriched table with embeddings + all product metadata\n",
    "-- This table will be the source for Vector Search index\n",
    "CREATE TABLE IF NOT EXISTS main.fashion_demo.product_embeddings_enriched AS\n",
    "SELECT \n",
    "  p.product_id,\n",
    "  p.product_display_name,\n",
    "  p.master_category,\n",
    "  p.sub_category,\n",
    "  p.article_type,\n",
    "  p.base_color,\n",
    "  p.price,\n",
    "  p.image_path,\n",
    "  p.gender,\n",
    "  p.season,\n",
    "  p.year,\n",
    "  p.usage,\n",
    "  e.image_embedding,\n",
    "  e.embedding_model,\n",
    "  e.embedding_dimension,\n",
    "  CURRENT_TIMESTAMP() as updated_at\n",
    "FROM main.fashion_demo.products p\n",
    "INNER JOIN main.fashion_demo.product_image_embeddings e\n",
    "  ON p.product_id = e.product_id\n",
    "WHERE e.image_embedding IS NOT NULL;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d83b2618-0af3-49a1-a975-54b6d0cb7715",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 2: Verify Enriched Table"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Check the enriched table\n",
    "SELECT \n",
    "  COUNT(*) as total_products,\n",
    "  COUNT(DISTINCT master_category) as categories,\n",
    "  COUNT(DISTINCT base_color) as colors,\n",
    "  MIN(price) as min_price,\n",
    "  MAX(price) as max_price,\n",
    "  AVG(price) as avg_price\n",
    "FROM main.fashion_demo.product_embeddings_enriched;\n",
    "\n",
    "-- Sample a few rows\n",
    "SELECT \n",
    "  product_id,\n",
    "  product_display_name,\n",
    "  master_category,\n",
    "  base_color,\n",
    "  price,\n",
    "  SIZE(image_embedding) as embedding_size\n",
    "FROM main.fashion_demo.product_embeddings_enriched\n",
    "LIMIT 5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61ad1177-3f2d-41b1-9111-2552a5e9c539",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 3: Drop Old Vector Search Index"
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.vector_search.client import VectorSearchClient\n",
    "\n",
    "# Initialize clients\n",
    "w = WorkspaceClient()\n",
    "token = w.config.oauth_token().access_token\n",
    "\n",
    "vsc = VectorSearchClient(\n",
    "    workspace_url=f\"https://{w.config.host}\",\n",
    "    personal_access_token=token,\n",
    "    disable_notice=True\n",
    ")\n",
    "\n",
    "# Drop the old index\n",
    "old_index_name = \"main.fashion_demo.product_embeddings_index\"\n",
    "\n",
    "try:\n",
    "    vsc.delete_index(index_name=old_index_name)\n",
    "    print(f\"‚úÖ Deleted old index: {old_index_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not delete old index (might not exist): {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c8b2cef-5bec-45fa-aa2d-a940caacb0ea",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 4: Create New Vector Search Index with All Fields"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create Vector Search index on enriched table using SQL\n",
    "-- This will include ALL product metadata fields\n",
    "\n",
    "CREATE VECTOR SEARCH INDEX IF NOT EXISTS main.fashion_demo.product_embeddings_enriched_index\n",
    "ON main.fashion_demo.product_embeddings_enriched(\n",
    "  image_embedding\n",
    ")\n",
    "USING ENDPOINT fashion_vector_search;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf88fc55-ba09-4f49-b25c-b3494e829838",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 5: Sync the Index"
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.vector_search.client import VectorSearchClient\n",
    "import time\n",
    "\n",
    "# Initialize clients\n",
    "w = WorkspaceClient()\n",
    "token = w.config.oauth_token().access_token\n",
    "\n",
    "vsc = VectorSearchClient(\n",
    "    workspace_url=f\"https://{w.config.host}\",\n",
    "    personal_access_token=token,\n",
    "    disable_notice=True\n",
    ")\n",
    "\n",
    "index_name = \"main.fashion_demo.product_embeddings_index\"\n",
    "\n",
    "print(f\"Syncing Vector Search index: {index_name}\")\n",
    "\n",
    "try:\n",
    "    index = vsc.get_index(index_name=index_name)\n",
    "    \n",
    "    # Trigger sync\n",
    "    index.sync()\n",
    "    print(\"‚úÖ Sync triggered\")\n",
    "    \n",
    "    # Wait for sync to complete\n",
    "    print(\"Waiting for sync to complete...\")\n",
    "    for i in range(30):  # Wait up to 5 minutes\n",
    "        time.sleep(10)\n",
    "        status = index.describe()\n",
    "        \n",
    "        if status.get(\"status\", {}).get(\"ready\", False):\n",
    "            print(f\"\\n‚úÖ Index is ready!\")\n",
    "            print(f\"Total vectors: {status.get('status', {}).get('indexed_row_count', 0)}\")\n",
    "            break\n",
    "        else:\n",
    "            print(f\"‚è≥ Still syncing... ({i*10}s)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error syncing index: {type(e).__name__}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4458dc1-51ae-4e69-8585-ef71ce2fd72c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Create Vector Search Index on Enriched Table\n",
    "\n",
    "## Option 1: Via Databricks UI (Recommended)\n",
    "\n",
    "1. **Navigate to Compute** ‚Üí **Vector Search**\n",
    "2. **Click** on endpoint: `fashion_vector_search`\n",
    "3. **Click \"Create Index\"**\n",
    "4. **Configure**:\n",
    "   - **Index Name**: `main.fashion_demo.product_embeddings_enriched_index`\n",
    "   - **Source Table**: `main.fashion_demo.product_embeddings_enriched`\n",
    "   - **Primary Key**: `product_id`\n",
    "   - **Embedding Column**: `image_embedding`\n",
    "   - **Embedding Dimension**: `512`\n",
    "   - **Sync Mode**: `Triggered` (manual sync)\n",
    "5. **Click \"Create\"**\n",
    "6. **Wait for sync** to complete (~2-5 minutes for 44K products)\n",
    "\n",
    "## Option 2: Via Databricks CLI\n",
    "\n",
    "```bash\n",
    "databricks vector-search create-index \\\n",
    "  --endpoint-name fashion_vector_search \\\n",
    "  --index-name main.fashion_demo.product_embeddings_enriched_index \\\n",
    "  --source-table main.fashion_demo.product_embeddings_enriched \\\n",
    "  --primary-key product_id \\\n",
    "  --embedding-vector-column image_embedding \\\n",
    "  --embedding-dimension 512 \\\n",
    "  --pipeline-type TRIGGERED\n",
    "```\n",
    "\n",
    "## What This Gives You\n",
    "\n",
    "‚úÖ **All product fields available** in Vector Search results:\n",
    "- product_id, product_display_name\n",
    "- master_category, sub_category, article_type\n",
    "- base_color, price, gender, season, year, usage\n",
    "- image_path\n",
    "\n",
    "‚úÖ **Filtering support** on any of these fields:\n",
    "- `{\"price >= \": 50, \"price < \": 100}`\n",
    "- `{\"master_category\": \"Apparel\"}`\n",
    "- `{\"base_color\": \"Black\"}`\n",
    "\n",
    "‚úÖ **No joins needed** - Vector Search returns complete product data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0bac2f7d-8cb7-4bb9-8924-ef2362d09a22",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Updated services/vector_search_service.py - For Enriched Index"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Vector Search service - UPDATED for enriched index\n",
    "Index: main.fashion_demo.product_embeddings_enriched_index\n",
    "Source: main.fashion_demo.product_embeddings_enriched (has ALL product fields)\n",
    "\"\"\"\n",
    "import logging\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Optional\n",
    "from databricks.vector_search.client import VectorSearchClient\n",
    "from databricks.sdk import WorkspaceClient\n",
    "import os\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class VectorSearchService:\n",
    "    \"\"\"Service for Vector Search similarity queries\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.endpoint_name = \"fashion_vector_search\"\n",
    "        self.endpoint_id = \"4d329fc8-1924-4131-ace8-14b542f8c14b\"\n",
    "        # ‚úÖ NEW: Use enriched index with all product fields\n",
    "        self.index_name = \"main.fashion_demo.product_embeddings_enriched_index\"\n",
    "        self.embedding_dim = 512\n",
    "        self.workspace_host = os.getenv(\"DATABRICKS_HOST\", \"\")\n",
    "        if not self.workspace_host.startswith(\"http\"):\n",
    "            self.workspace_host = f\"https://{self.workspace_host}\"\n",
    "        self._client = None\n",
    "        self._index = None\n",
    "        \n",
    "        if not self.index_name:\n",
    "            raise ValueError(\"Vector Search index name is not configured!\")\n",
    "        \n",
    "        logger.info(f\"üîß VectorSearchService initialized with index: {self.index_name}\")\n",
    "    \n",
    "    def _get_client(self) -> VectorSearchClient:\n",
    "        \"\"\"Get or create Vector Search client with OAuth authentication\"\"\"\n",
    "        if self._client is None:\n",
    "            w = WorkspaceClient()\n",
    "            token = w.config.oauth_token().access_token\n",
    "            \n",
    "            self._client = VectorSearchClient(\n",
    "                workspace_url=self.workspace_host,\n",
    "                personal_access_token=token,\n",
    "                disable_notice=True\n",
    "            )\n",
    "            logger.info(f\"‚úÖ Created Vector Search client for {self.workspace_host}\")\n",
    "        return self._client\n",
    "    \n",
    "    def _get_index(self):\n",
    "        \"\"\"Get Vector Search index\"\"\"\n",
    "        if self._index is None:\n",
    "            logger.info(f\"üîç Getting Vector Search index: '{self.index_name}'\")\n",
    "            \n",
    "            if not self.index_name:\n",
    "                raise ValueError(\"Index name is empty or None!\")\n",
    "            \n",
    "            client = self._get_client()\n",
    "            self._index = client.get_index(index_name=self.index_name)\n",
    "            \n",
    "            logger.info(f\"‚úÖ Connected to Vector Search index: {self.index_name}\")\n",
    "        return self._index\n",
    "    \n",
    "    async def similarity_search(\n",
    "        self,\n",
    "        query_vector: np.ndarray,\n",
    "        num_results: int = 20,\n",
    "        filters: Optional[Dict[str, Any]] = None\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Search for similar products using vector similarity\n",
    "        NOW RETURNS COMPLETE PRODUCT DATA from enriched index!\n",
    "        \n",
    "        Args:\n",
    "            query_vector: Normalized embedding vector (512 dims)\n",
    "            num_results: Number of results to return\n",
    "            filters: Optional filters (e.g., {\"price >= \": 50, \"master_category\": \"Apparel\"})\n",
    "            \n",
    "        Returns:\n",
    "            List of complete product dictionaries with similarity scores\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Ensure vector is normalized and correct shape\n",
    "            if query_vector.shape != (self.embedding_dim,):\n",
    "                raise ValueError(f\"Expected vector shape ({self.embedding_dim},), got {query_vector.shape}\")\n",
    "            \n",
    "            # Ensure L2 normalization for cosine-like similarity\n",
    "            norm = np.linalg.norm(query_vector)\n",
    "            if norm > 0:\n",
    "                query_vector = query_vector / norm\n",
    "            \n",
    "            logger.info(f\"Vector Search query: dim={query_vector.shape[0]}, norm={np.linalg.norm(query_vector):.4f}, filters={filters}\")\n",
    "            logger.info(f\"Using index: {self.index_name}\")\n",
    "            \n",
    "            # Get index and perform similarity search\n",
    "            index = self._get_index()\n",
    "            \n",
    "            # ‚úÖ NOW WE CAN REQUEST ALL PRODUCT FIELDS!\n",
    "            columns = [\n",
    "                \"product_id\",\n",
    "                \"product_display_name\", \n",
    "                \"master_category\",\n",
    "                \"sub_category\",\n",
    "                \"article_type\",\n",
    "                \"base_color\",\n",
    "                \"price\",\n",
    "                \"image_path\",\n",
    "                \"gender\",\n",
    "                \"season\",\n",
    "                \"usage\",\n",
    "                \"year\"\n",
    "            ]\n",
    "            \n",
    "            # Perform similarity search\n",
    "            import asyncio\n",
    "            loop = asyncio.get_event_loop()\n",
    "            \n",
    "            def do_search():\n",
    "                return index.similarity_search(\n",
    "                    query_vector=query_vector.tolist(),\n",
    "                    columns=columns,\n",
    "                    num_results=num_results,\n",
    "                    filters=filters  # ‚úÖ Filters now work!\n",
    "                )\n",
    "            \n",
    "            results = await loop.run_in_executor(None, do_search)\n",
    "            \n",
    "            # Parse results\n",
    "            if \"result\" in results and \"data_array\" in results[\"result\"]:\n",
    "                data_array = results[\"result\"][\"data_array\"]\n",
    "                logger.info(f\"‚úÖ Vector Search returned {len(data_array)} results\")\n",
    "                \n",
    "                # Convert to list of dicts\n",
    "                products = []\n",
    "                for row in data_array:\n",
    "                    product = dict(zip(columns, row))\n",
    "                    # Add similarity score (last column in response)\n",
    "                    if len(row) > len(columns):\n",
    "                        product[\"score\"] = row[-1]\n",
    "                    products.append(product)\n",
    "                \n",
    "                return products\n",
    "            else:\n",
    "                logger.warning(f\"Unexpected Vector Search response format: {results}\")\n",
    "                return []\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Vector Search error: {type(e).__name__}: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "# Singleton instance\n",
    "vector_search_service = VectorSearchService()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57689d85-a947-4b3c-878b-c0f61a072d19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Option 1: Add Text Embeddings for Semantic Search\n",
    "\n",
    "## Why You'd Want This:\n",
    "\n",
    "**Semantic Understanding:**\n",
    "- Query: \"red summer dress\" \n",
    "- Matches: \"Scarlet Sundress\", \"Coral Maxi Dress\", \"Crimson Evening Gown\"\n",
    "- Without text embeddings: Only matches exact keywords \"red\" + \"dress\"\n",
    "\n",
    "## The Problem:\n",
    "\n",
    "Your `clip-image-encoder` endpoint **only accepts images**, not text!\n",
    "\n",
    "```python\n",
    "# This fails:\n",
    "payload = {\"dataframe_records\": [{\"text\": \"red dress\"}]}\n",
    "# Error: Model is missing inputs ['image']\n",
    "```\n",
    "\n",
    "## Solutions:\n",
    "\n",
    "### A. Use a Different Model for Text (Recommended)\n",
    "\n",
    "Deploy a **text embedding model** like:\n",
    "- `sentence-transformers/all-MiniLM-L6-v2` (384 dims)\n",
    "- `BAAI/bge-small-en-v1.5` (384 dims)\n",
    "- `text-embedding-ada-002` (OpenAI, 1536 dims)\n",
    "\n",
    "Then create:\n",
    "```sql\n",
    "ALTER TABLE main.fashion_demo.product_embeddings_enriched\n",
    "ADD COLUMN text_embedding ARRAY<DOUBLE>;\n",
    "\n",
    "-- Generate text embeddings from product_display_name + article_type\n",
    "UPDATE main.fashion_demo.product_embeddings_enriched\n",
    "SET text_embedding = generate_text_embedding(\n",
    "  CONCAT(product_display_name, ' ', article_type, ' ', base_color)\n",
    ");\n",
    "```\n",
    "\n",
    "### B. Use CLIP Text Encoder (If Available)\n",
    "\n",
    "If you have access to the **full CLIP model** (not just image encoder):\n",
    "- Deploy `clip-text-encoder` endpoint\n",
    "- Generate text embeddings for product descriptions\n",
    "- Store in separate column\n",
    "\n",
    "### C. Keep Basic Keyword Search (Simplest)\n",
    "\n",
    "For many e-commerce use cases, **keyword search is good enough**:\n",
    "- Fast and simple\n",
    "- Users are used to it\n",
    "- Works well with filters (category, price, color)\n",
    "\n",
    "## Recommendation:\n",
    "\n",
    "**For your use case, I recommend Option C (keyword search) because:**\n",
    "\n",
    "1. ‚úÖ **Image search is the killer feature** - users upload photos\n",
    "2. ‚úÖ **Filters work well** - category, price, color dropdowns\n",
    "3. ‚úÖ **Simple and fast** - no additional model needed\n",
    "4. ‚úÖ **Your CLIP endpoint only does images anyway**\n",
    "\n",
    "Save semantic text search for v2 if users request it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe673dde-c2cd-4de6-affd-ad8d83c4d749",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Option 2: Validate Current Text Search (Keyword-Based)"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Let's validate that basic keyword search works\n",
    "-- Test with common queries\n",
    "\n",
    "-- Test 1: Search for \"dress\"\n",
    "SELECT product_id, product_display_name, article_type, base_color, price\n",
    "FROM main.fashion_demo.products\n",
    "WHERE \n",
    "  (product_display_name IS NOT NULL AND LOWER(product_display_name) LIKE '%dress%')\n",
    "  OR (article_type IS NOT NULL AND LOWER(article_type) LIKE '%dress%')\n",
    "  OR (sub_category IS NOT NULL AND LOWER(sub_category) LIKE '%dress%')\n",
    "LIMIT 10;\n",
    "\n",
    "-- Test 2: Search for \"shoes\"\n",
    "SELECT product_id, product_display_name, article_type, base_color, price\n",
    "FROM main.fashion_demo.products\n",
    "WHERE \n",
    "  (product_display_name IS NOT NULL AND LOWER(product_display_name) LIKE '%shoes%')\n",
    "  OR (article_type IS NOT NULL AND LOWER(article_type) LIKE '%shoes%')\n",
    "  OR (sub_category IS NOT NULL AND LOWER(sub_category) LIKE '%shoes%')\n",
    "LIMIT 10;\n",
    "\n",
    "-- Test 3: Search for \"black\"\n",
    "SELECT product_id, product_display_name, article_type, base_color, price\n",
    "FROM main.fashion_demo.products\n",
    "WHERE \n",
    "  (product_display_name IS NOT NULL AND LOWER(product_display_name) LIKE '%black%')\n",
    "  OR (base_color IS NOT NULL AND LOWER(base_color) LIKE '%black%')\n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c74869cb-13fa-4d10-9d61-c93ea4a665f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Recommended Architecture\n",
    "\n",
    "## What You Have:\n",
    "\n",
    "‚úÖ **44,424 products** with **512-dim CLIP image embeddings**\n",
    "‚úÖ **CLIP image encoder** endpoint (working!)\n",
    "‚úÖ **Vector Search endpoint** (fashion_vector_search)\n",
    "‚úÖ **User embeddings** (512-dim) for personalization\n",
    "\n",
    "## What Works Best:\n",
    "\n",
    "### 1. **Image Search** (Your Killer Feature) üéØ\n",
    "- User uploads photo of a dress they like\n",
    "- CLIP generates image embedding\n",
    "- Vector Search finds visually similar products\n",
    "- **This is unique and powerful!**\n",
    "\n",
    "### 2. **Text Search** (Keep It Simple) üìù\n",
    "- Basic keyword matching with filters\n",
    "- Users can filter by:\n",
    "  - Category (Apparel, Footwear, Accessories)\n",
    "  - Color (Black, White, Blue, etc.)\n",
    "  - Price range ($0-$50, $50-$100, etc.)\n",
    "  - Gender (Men, Women, Unisex)\n",
    "- **This is what users expect anyway**\n",
    "\n",
    "### 3. **Recommendations** (Hybrid Approach) ‚≠ê\n",
    "- **60% Vector similarity**: User embedding vs product image embeddings\n",
    "- **40% Rule-based**: Category + color + price preferences\n",
    "- **Result**: Truly personalized recommendations\n",
    "\n",
    "## Why This Works:\n",
    "\n",
    "**Image embeddings capture visual style:**\n",
    "- User likes: Minimalist black accessories\n",
    "- User embedding: Average of their liked product image embeddings\n",
    "- Recommendations: Products that **look similar** to what they've liked\n",
    "- **This works even without text embeddings!**\n",
    "\n",
    "## When You'd Need Text Embeddings:\n",
    "\n",
    "‚ùå **You DON'T need them if:**\n",
    "- Users primarily browse by category/filters\n",
    "- Image search is the main feature\n",
    "- Basic keyword search is acceptable\n",
    "\n",
    "‚úÖ **You DO need them if:**\n",
    "- Users search with complex queries: \"vintage floral summer dress\"\n",
    "- You want semantic matching: \"sneakers\" ‚Üí \"athletic shoes\"\n",
    "- You want to match product descriptions, not just names\n",
    "\n",
    "## My Recommendation:\n",
    "\n",
    "**Ship v1 with:**\n",
    "1. ‚úÖ Image search (CLIP image embeddings)\n",
    "2. ‚úÖ Keyword text search + filters\n",
    "3. ‚úÖ Hybrid recommendations (user embeddings + rules)\n",
    "\n",
    "**Add text embeddings in v2 if:**\n",
    "- Users complain about text search quality\n",
    "- You see low conversion from text search\n",
    "- You want to compete with Amazon-level semantic search\n",
    "\n",
    "**For now, focus on making image search amazing!** üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6fd4acce-4d38-4511-b1c6-2891cb936d3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ‚úÖ Confirmed: Embeddings are IMAGE-ONLY\n",
    "\n",
    "## From Notebook 03 (`03_image_embeddings_pipeline`):\n",
    "\n",
    "**What was generated:**\n",
    "- ‚úÖ **44,424 products** with CLIP image embeddings\n",
    "- ‚úÖ **Model**: `clip-vit-b-32` (512 dimensions)\n",
    "- ‚úÖ **Source**: Product photos from `/Volumes/main/fashion_demo/raw_data/images/`\n",
    "- ‚úÖ **Endpoint**: `clip-image-encoder` (IMAGE ONLY)\n",
    "\n",
    "**What's in the embeddings:**\n",
    "- Visual features: colors, patterns, shapes, textures\n",
    "- Style information: formal vs casual, vintage vs modern\n",
    "- Product type: dress vs shoes vs accessories\n",
    "\n",
    "## üí° Key Insight: You DON'T Need Text Embeddings!\n",
    "\n",
    "**Why image embeddings are enough:**\n",
    "\n",
    "### 1. **Image Search** (Primary Use Case)\n",
    "- User uploads photo ‚Üí CLIP image embedding ‚Üí Vector Search\n",
    "- Finds visually similar products\n",
    "- **This is your killer feature!**\n",
    "\n",
    "### 2. **Recommendations** (Personalization)\n",
    "- User embedding = average of liked product **image** embeddings\n",
    "- Finds products that **look similar** to user's style\n",
    "- Works because:\n",
    "  - User likes black minimalist accessories ‚Üí user embedding captures that visual style\n",
    "  - Vector Search finds products with similar visual style\n",
    "  - **More powerful than text matching!**\n",
    "\n",
    "### 3. **Text Search** (Keep Simple)\n",
    "- Use keyword matching: \"red dress\" ‚Üí LIKE '%red%' AND LIKE '%dress%'\n",
    "- Add filters: category, price, color dropdowns\n",
    "- **Users expect this anyway!**\n",
    "\n",
    "## ‚ùå When You'd Need Text Embeddings:\n",
    "\n",
    "- Semantic queries: \"vintage floral summer dress\" ‚Üí matches \"retro botanical sundress\"\n",
    "- Synonym matching: \"sneakers\" ‚Üí \"athletic shoes\"\n",
    "- Description search: Match product descriptions, not just names\n",
    "\n",
    "**But your CLIP endpoint doesn't support text anyway!**\n",
    "\n",
    "## ‚úÖ Recommendation: Focus on Enriched Index\n",
    "\n",
    "**Don't add text embeddings. Instead:**\n",
    "1. Create Vector Search index on enriched table (all product fields)\n",
    "2. Image search works perfectly with image embeddings\n",
    "3. Recommendations work with user image embeddings\n",
    "4. Text search uses keywords + filters\n",
    "\n",
    "**Ship v1 with this architecture!** üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f556991d-efb2-43ee-9e03-9ca519653fb4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Enriched Vector Search Index (Python SDK)"
    }
   },
   "outputs": [],
   "source": [
    "from databricks.vector_search.client import VectorSearchClient\n",
    "import time\n",
    "\n",
    "vsc = VectorSearchClient()\n",
    "\n",
    "# Configuration\n",
    "ENDPOINT_NAME = \"fashion_vector_search\"\n",
    "NEW_INDEX_NAME = \"main.fashion_demo.product_embeddings_enriched_index\"\n",
    "SOURCE_TABLE = \"main.fashion_demo.product_embeddings_enriched\"\n",
    "\n",
    "print(\"Creating Vector Search Index on Enriched Table\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Index: {NEW_INDEX_NAME}\")\n",
    "print(f\"Source: {SOURCE_TABLE}\")\n",
    "print(f\"Endpoint: {ENDPOINT_NAME}\")\n",
    "print()\n",
    "\n",
    "try:\n",
    "    # Check if index already exists\n",
    "    try:\n",
    "        existing = vsc.get_index(index_name=NEW_INDEX_NAME)\n",
    "        status = existing.describe()\n",
    "        state = status.get('status', {}).get('detailed_state', 'UNKNOWN')\n",
    "        rows = status.get('status', {}).get('num_indexed_rows', 0)\n",
    "        \n",
    "        print(f\"‚úÖ Index already exists!\")\n",
    "        print(f\"   Status: {state}\")\n",
    "        print(f\"   Indexed rows: {rows:,}\")\n",
    "        \n",
    "        if rows == 0 or state not in ['ONLINE_CONTINUOUS_UPDATE', 'ONLINE_TRIGGERED_UPDATE']:\n",
    "            print(f\"\\nüîÑ Triggering sync...\")\n",
    "            existing.sync()\n",
    "            print(f\"‚úÖ Sync triggered - check back in a few minutes\")\n",
    "        else:\n",
    "            print(f\"\\n‚úÖ Index is ready to use!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Create new index\n",
    "        print(f\"üìù Creating new index...\")\n",
    "        print(f\"   (This will take 5-10 minutes for 44K products)\\n\")\n",
    "        \n",
    "        index = vsc.create_delta_sync_index(\n",
    "            endpoint_name=ENDPOINT_NAME,\n",
    "            index_name=NEW_INDEX_NAME,\n",
    "            source_table_name=SOURCE_TABLE,\n",
    "            pipeline_type=\"TRIGGERED\",\n",
    "            primary_key=\"product_id\",\n",
    "            embedding_dimension=512,\n",
    "            embedding_vector_column=\"image_embedding\"\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Index created!\")\n",
    "        print(f\"\\n‚è≥ Monitoring sync progress...\")\n",
    "        \n",
    "        # Wait for index to be ready\n",
    "        max_wait = 600  # 10 minutes\n",
    "        wait_interval = 15\n",
    "        elapsed = 0\n",
    "        \n",
    "        while elapsed < max_wait:\n",
    "            time.sleep(wait_interval)\n",
    "            elapsed += wait_interval\n",
    "            \n",
    "            status = index.describe()\n",
    "            state = status.get('status', {}).get('detailed_state', 'UNKNOWN')\n",
    "            rows = status.get('status', {}).get('num_indexed_rows', 0)\n",
    "            \n",
    "            print(f\"   [{elapsed}s] State: {state}, Rows: {rows:,}\")\n",
    "            \n",
    "            if state in ['ONLINE_CONTINUOUS_UPDATE', 'ONLINE_TRIGGERED_UPDATE']:\n",
    "                print(f\"\\n‚úÖ Index is ONLINE and ready!\")\n",
    "                print(f\"   Indexed {rows:,} products\")\n",
    "                break\n",
    "        \n",
    "        if elapsed >= max_wait:\n",
    "            print(f\"\\n‚ö†Ô∏è Timeout - index may still be syncing\")\n",
    "            print(f\"   Check Databricks UI: Compute ‚Üí Vector Search ‚Üí {ENDPOINT_NAME}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"‚úÖ NEXT STEPS TO FIX YOUR APP:\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\n1. Update services/vector_search_service.py (line 23):\")\n",
    "    print(f\"   self.index_name = '{NEW_INDEX_NAME}'\")\n",
    "    print(\"\\n2. Keep the full columns list (lines 107-119):\")\n",
    "    print(\"   All product fields are now available!\")\n",
    "    print(\"\\n3. Redeploy your app\")\n",
    "    print(\"\\n4. Test:\")\n",
    "    print(\"   - Image search should return results\")\n",
    "    print(\"   - Recommendations should use Vector Search\")\n",
    "    print(\"   - Filters will work (price, category, color)\")\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error: {type(e).__name__}: {e}\")\n",
    "    import traceback\n",
    "    print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4415302-a0a8-4fd5-aed2-e908ef761251",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üöÄ CLIP Multimodal Architecture - Comprehensive Solution\n",
    "\n",
    "## üí° The Key Insight: CLIP's Shared Embedding Space\n",
    "\n",
    "**CLIP was designed for this!** Text and image embeddings live in the **same 512-dimensional space**:\n",
    "\n",
    "```\n",
    "Text: \"red summer dress\"     ‚Üí [0.12, -0.34, 0.56, ...] (512 dims)\n",
    "Image: photo of red dress    ‚Üí [0.15, -0.31, 0.52, ...] (512 dims)\n",
    "                                    ‚Üë COMPARABLE! ‚Üë\n",
    "```\n",
    "\n",
    "**What This Enables:**\n",
    "\n",
    "### 1. **Cross-Modal Search** üéØ\n",
    "- User types: \"vintage leather jacket\"\n",
    "- CLIP text encoder ‚Üí text embedding\n",
    "- Vector Search ‚Üí finds **images** that match the text description\n",
    "- **Magic**: Text query finds visually similar products!\n",
    "\n",
    "### 2. **Semantic Text Search** üìù\n",
    "- Query: \"sneakers\" ‚Üí matches \"athletic shoes\", \"running shoes\", \"trainers\"\n",
    "- Query: \"red dress\" ‚Üí matches \"scarlet gown\", \"crimson sundress\", \"ruby evening wear\"\n",
    "- **No keyword matching needed!**\n",
    "\n",
    "### 3. **Hybrid Embeddings** ‚ö°\n",
    "- Combine text + image features for each product\n",
    "- `product_embedding = 0.5 * text_embedding + 0.5 * image_embedding`\n",
    "- Captures **both** visual style AND semantic meaning\n",
    "- **Best of both worlds!**\n",
    "\n",
    "### 4. **Latent Feature Extraction** üî¨\n",
    "- Analyze embedding dimensions to discover:\n",
    "  - Which dimensions encode \"formal vs casual\"\n",
    "  - Which dimensions encode \"color\" (red, blue, black)\n",
    "  - Which dimensions encode \"season\" (summer, winter)\n",
    "  - Which dimensions encode \"price tier\" (luxury, budget)\n",
    "- **Interpretable AI!**\n",
    "\n",
    "### 5. **User Style Embeddings** üë§\n",
    "- User embedding = average of liked product embeddings\n",
    "- Works with **both** text and image features\n",
    "- Captures user's visual style AND semantic preferences\n",
    "- **Truly personalized!**\n",
    "\n",
    "---\n",
    "\n",
    "## üèóÔ∏è Architecture Design\n",
    "\n",
    "### Current State:\n",
    "```\n",
    "Products Table (44K products)\n",
    "‚îú‚îÄ‚îÄ product_id, name, category, color, price\n",
    "‚îî‚îÄ‚îÄ image_path ‚Üí /Volumes/.../images/\n",
    "\n",
    "Product Image Embeddings (44K)\n",
    "‚îú‚îÄ‚îÄ product_id\n",
    "‚îî‚îÄ‚îÄ image_embedding (512 dims) ‚Üê From CLIP image encoder\n",
    "\n",
    "Vector Search Index\n",
    "‚îî‚îÄ‚îÄ Only has image embeddings\n",
    "```\n",
    "\n",
    "### Target State (Multimodal):\n",
    "```\n",
    "Products Table (44K products)\n",
    "‚îú‚îÄ‚îÄ product_id, name, category, color, price\n",
    "‚îî‚îÄ‚îÄ image_path\n",
    "\n",
    "Product Embeddings Multimodal (44K) ‚Üê NEW!\n",
    "‚îú‚îÄ‚îÄ product_id\n",
    "‚îú‚îÄ‚îÄ image_embedding (512 dims)     ‚Üê From CLIP image encoder\n",
    "‚îú‚îÄ‚îÄ text_embedding (512 dims)      ‚Üê From CLIP text encoder ‚ú®\n",
    "‚îú‚îÄ‚îÄ hybrid_embedding (512 dims)    ‚Üê Combined (0.5 * text + 0.5 * image) ‚ú®\n",
    "‚îú‚îÄ‚îÄ embedding_model: \"clip-vit-b-32\"\n",
    "‚îî‚îÄ‚îÄ All product metadata (name, category, color, price, etc.)\n",
    "\n",
    "Vector Search Indexes:\n",
    "‚îú‚îÄ‚îÄ image_index     ‚Üí Search by visual similarity\n",
    "‚îú‚îÄ‚îÄ text_index      ‚Üí Search by semantic meaning\n",
    "‚îî‚îÄ‚îÄ hybrid_index    ‚Üí Search by both! ‚ú®\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ What This Enables:\n",
    "\n",
    "### **Use Case 1: Cross-Modal Search**\n",
    "```python\n",
    "# User types: \"black leather jacket\"\n",
    "text_embedding = clip_text_encoder.encode(\"black leather jacket\")\n",
    "results = vector_search(text_embedding, index=\"image_index\")\n",
    "# Returns: Photos of black leather jackets!\n",
    "```\n",
    "\n",
    "### **Use Case 2: Image-to-Text Understanding**\n",
    "```python\n",
    "# User uploads photo of a dress\n",
    "image_embedding = clip_image_encoder.encode(photo)\n",
    "# Compare with text embeddings to understand style\n",
    "style_descriptions = [\n",
    "    \"formal evening wear\",\n",
    "    \"casual summer dress\", \n",
    "    \"vintage cocktail dress\"\n",
    "]\n",
    "for desc in style_descriptions:\n",
    "    text_emb = clip_text_encoder.encode(desc)\n",
    "    similarity = cosine_similarity(image_embedding, text_emb)\n",
    "    print(f\"{desc}: {similarity:.2f}\")\n",
    "# Output: \"formal evening wear: 0.87\" ‚Üê Automatically understands the style!\n",
    "```\n",
    "\n",
    "### **Use Case 3: Hybrid Search**\n",
    "```python\n",
    "# User searches: \"red dress\" + uploads inspiration photo\n",
    "text_emb = clip_text_encoder.encode(\"red dress\")\n",
    "image_emb = clip_image_encoder.encode(inspiration_photo)\n",
    "hybrid_emb = 0.5 * text_emb + 0.5 * image_emb\n",
    "results = vector_search(hybrid_emb, index=\"hybrid_index\")\n",
    "# Returns: Red dresses that look like the inspiration photo!\n",
    "```\n",
    "\n",
    "### **Use Case 4: Latent Feature Analysis**\n",
    "```python\n",
    "# Discover what each dimension represents\n",
    "for dim in range(512):\n",
    "    # Find products with high values in this dimension\n",
    "    high_dim_products = products[embeddings[:, dim] > 0.5]\n",
    "    # Analyze common attributes\n",
    "    print(f\"Dimension {dim}: {high_dim_products['category'].mode()}\")\n",
    "    \n",
    "# Example findings:\n",
    "# Dim 42: Encodes \"formal\" (high for suits, low for t-shirts)\n",
    "# Dim 156: Encodes \"red color\" (high for red items)\n",
    "# Dim 287: Encodes \"luxury\" (high for expensive items)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Implementation Plan\n",
    "\n",
    "See next cells for step-by-step implementation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52da3f4a-c9bf-4e43-a3fd-89e5c4c6d166",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Step 1: Deploy CLIP Text Encoder\n",
    "\n",
    "## Why We Need This:\n",
    "\n",
    "Your current `clip-image-encoder` endpoint **only processes images**. To leverage CLIP's multimodal space, we need the **text encoder** too!\n",
    "\n",
    "## Option A: Deploy Full CLIP Model (Recommended)\n",
    "\n",
    "Deploy a **single endpoint** that handles both text AND images:\n",
    "\n",
    "```python\n",
    "class CLIPMultimodalEncoder(mlflow.pyfunc.PythonModel):\n",
    "    def load_context(self, context):\n",
    "        from transformers import CLIPProcessor, CLIPModel\n",
    "        self.model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    \n",
    "    def predict(self, context, model_input):\n",
    "        # Handles BOTH text and images!\n",
    "        if \"text\" in model_input:\n",
    "            inputs = self.processor(text=model_input[\"text\"], return_tensors=\"pt\")\n",
    "            features = self.model.get_text_features(**inputs)\n",
    "        elif \"image\" in model_input:\n",
    "            # Decode base64 image\n",
    "            image = decode_base64_image(model_input[\"image\"])\n",
    "            inputs = self.processor(images=image, return_tensors=\"pt\")\n",
    "            features = self.model.get_image_features(**inputs)\n",
    "        \n",
    "        # Normalize to unit vector\n",
    "        features = features / features.norm(dim=-1, keepdim=True)\n",
    "        return features.cpu().numpy()[0].tolist()\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- ‚úÖ Single endpoint for both modalities\n",
    "- ‚úÖ Guaranteed same embedding space\n",
    "- ‚úÖ Simpler architecture\n",
    "\n",
    "## Option B: Separate Text Endpoint\n",
    "\n",
    "Deploy `clip-text-encoder` as a separate endpoint:\n",
    "- Lighter weight (no image processing)\n",
    "- Can scale independently\n",
    "- Same 512-dim output space\n",
    "\n",
    "## Recommendation: Option A\n",
    "\n",
    "Deploy **one multimodal endpoint** that replaces your current image-only endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66a9d49a-67cb-4bcd-8b60-b060b07dd807",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 2: Generate Text Embeddings for All Products"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create table for text embeddings\n",
    "CREATE TABLE IF NOT EXISTS main.fashion_demo.product_text_embeddings (\n",
    "  product_id INT,\n",
    "  text_content STRING,  -- The text that was embedded\n",
    "  text_embedding ARRAY<DOUBLE>,  -- 512-dim CLIP text embedding\n",
    "  embedding_model STRING,\n",
    "  embedding_dimension INT,\n",
    "  created_at TIMESTAMP\n",
    ")\n",
    "USING DELTA\n",
    "TBLPROPERTIES (\n",
    "  'delta.enableChangeDataFeed' = 'true'\n",
    ");\n",
    "\n",
    "-- Create rich text descriptions for embedding\n",
    "-- Combine multiple fields for semantic richness\n",
    "CREATE OR REPLACE TEMP VIEW product_text_descriptions AS\n",
    "SELECT \n",
    "  product_id,\n",
    "  CONCAT_WS(' ',\n",
    "    product_display_name,\n",
    "    article_type,\n",
    "    base_color,\n",
    "    master_category,\n",
    "    sub_category,\n",
    "    gender,\n",
    "    season,\n",
    "    usage,\n",
    "    CASE \n",
    "      WHEN price < 30 THEN 'affordable budget'\n",
    "      WHEN price < 70 THEN 'mid-range'\n",
    "      WHEN price < 120 THEN 'premium'\n",
    "      ELSE 'luxury high-end'\n",
    "    END\n",
    "  ) as text_content\n",
    "FROM main.fashion_demo.products\n",
    "WHERE product_display_name IS NOT NULL;\n",
    "\n",
    "-- Preview the text descriptions\n",
    "SELECT product_id, text_content\n",
    "FROM product_text_descriptions\n",
    "LIMIT 5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79cffb35-2826-473a-b3c8-062d17a30742",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 3: Generate Text Embeddings with CLIP"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf, col\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Configuration\n",
    "CLIP_TEXT_ENDPOINT = \"clip-multimodal-encoder\"  # Your new endpoint\n",
    "TOKEN = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "\n",
    "@pandas_udf(ArrayType(DoubleType()))\n",
    "def generate_text_embedding_udf(texts: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Generate CLIP text embeddings for product descriptions\n",
    "    These will be in the SAME 512-dim space as image embeddings!\n",
    "    \"\"\"\n",
    "    import requests\n",
    "    import numpy as np\n",
    "    \n",
    "    def encode_text(text):\n",
    "        try:\n",
    "            if pd.isna(text) or not text:\n",
    "                return np.zeros(512).tolist()\n",
    "            \n",
    "            # Call CLIP text encoder\n",
    "            payload = {\"dataframe_records\": [{\"text\": text}]}\n",
    "            headers = {\n",
    "                \"Authorization\": f\"Bearer {TOKEN}\",\n",
    "                \"Content-Type\": \"application/json\"\n",
    "            }\n",
    "            \n",
    "            response = requests.post(\n",
    "                f\"https://{workspace_url}/serving-endpoints/{CLIP_TEXT_ENDPOINT}/invocations\",\n",
    "                headers=headers,\n",
    "                json=payload,\n",
    "                timeout=30\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            result = response.json()\n",
    "            if \"predictions\" in result:\n",
    "                embedding = result[\"predictions\"][0]\n",
    "            else:\n",
    "                embedding = result\n",
    "            \n",
    "            # Normalize (CLIP does this internally, but ensure it)\n",
    "            embedding = np.array(embedding)\n",
    "            embedding = embedding / (np.linalg.norm(embedding) + 1e-8)\n",
    "            \n",
    "            return embedding.tolist()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error encoding text: {e}\")\n",
    "            return np.zeros(512).tolist()\n",
    "    \n",
    "    return texts.apply(encode_text)\n",
    "\n",
    "print(\"‚úÖ Text embedding UDF defined\")\n",
    "print(\"   - Endpoint: clip-multimodal-encoder\")\n",
    "print(\"   - Dimension: 512 (same as image embeddings!)\")\n",
    "print(\"   - Embedding space: Shared with images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13e404c2-a40d-4e4b-a491-1ba0395be0e8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 4: Create Comprehensive Multimodal Table"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create the ultimate multimodal embeddings table!\n",
    "CREATE OR REPLACE TABLE main.fashion_demo.product_embeddings_multimodal AS\n",
    "SELECT \n",
    "  -- Product metadata\n",
    "  p.product_id,\n",
    "  p.product_display_name,\n",
    "  p.master_category,\n",
    "  p.sub_category,\n",
    "  p.article_type,\n",
    "  p.base_color,\n",
    "  p.price,\n",
    "  p.image_path,\n",
    "  p.gender,\n",
    "  p.season,\n",
    "  p.year,\n",
    "  p.usage,\n",
    "  \n",
    "  -- Image embedding (visual features)\n",
    "  img.image_embedding,\n",
    "  \n",
    "  -- Text embedding (semantic features) - TO BE ADDED\n",
    "  CAST(NULL AS ARRAY<DOUBLE>) as text_embedding,\n",
    "  \n",
    "  -- Hybrid embedding (combined) - TO BE COMPUTED\n",
    "  CAST(NULL AS ARRAY<DOUBLE>) as hybrid_embedding,\n",
    "  \n",
    "  -- Metadata\n",
    "  'clip-vit-b-32' as embedding_model,\n",
    "  512 as embedding_dimension,\n",
    "  CURRENT_TIMESTAMP() as updated_at\n",
    "  \n",
    "FROM main.fashion_demo.products p\n",
    "INNER JOIN main.fashion_demo.product_image_embeddings img\n",
    "  ON p.product_id = img.product_id\n",
    "WHERE img.image_embedding IS NOT NULL;\n",
    "\n",
    "-- Verify\n",
    "SELECT \n",
    "  COUNT(*) as total,\n",
    "  COUNT(image_embedding) as has_image_emb,\n",
    "  COUNT(text_embedding) as has_text_emb,\n",
    "  COUNT(hybrid_embedding) as has_hybrid_emb\n",
    "FROM main.fashion_demo.product_embeddings_multimodal;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3eb942a9-73f3-408a-a9f4-f9eadb1a3f39",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 5: Populate Text Embeddings"
    }
   },
   "outputs": [],
   "source": [
    "# Load the multimodal table\n",
    "multimodal_df = spark.table(\"main.fashion_demo.product_embeddings_multimodal\")\n",
    "\n",
    "# Load text descriptions\n",
    "text_desc_df = spark.table(\"product_text_descriptions\")\n",
    "\n",
    "# Generate text embeddings\n",
    "print(\"Generating text embeddings for 44K products...\")\n",
    "print(\"This will take ~10-15 minutes with CLIP text encoder\\n\")\n",
    "\n",
    "text_embeddings_df = (\n",
    "    text_desc_df\n",
    "    .withColumn(\"text_embedding\", generate_text_embedding_udf(col(\"text_content\")))\n",
    ")\n",
    "\n",
    "# Update the multimodal table with text embeddings\n",
    "print(\"Updating multimodal table with text embeddings...\")\n",
    "\n",
    "# Merge text embeddings into multimodal table\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "delta_table = DeltaTable.forName(spark, \"main.fashion_demo.product_embeddings_multimodal\")\n",
    "\n",
    "delta_table.alias(\"target\").merge(\n",
    "    text_embeddings_df.alias(\"source\"),\n",
    "    \"target.product_id = source.product_id\"\n",
    ").whenMatchedUpdate(\n",
    "    set = {\n",
    "        \"text_embedding\": \"source.text_embedding\",\n",
    "        \"updated_at\": \"CURRENT_TIMESTAMP()\"\n",
    "    }\n",
    ").execute()\n",
    "\n",
    "print(\"‚úÖ Text embeddings added!\")\n",
    "\n",
    "# Verify\n",
    "verify = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total,\n",
    "        COUNT(text_embedding) as has_text,\n",
    "        AVG(SIZE(text_embedding)) as avg_text_dim\n",
    "    FROM main.fashion_demo.product_embeddings_multimodal\n",
    "\"\"\")\n",
    "display(verify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "143fbfd0-0db4-4ede-8835-edc30b20fdc8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 6: Create Hybrid Embeddings"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "import numpy as np\n",
    "\n",
    "@udf(ArrayType(DoubleType()))\n",
    "def create_hybrid_embedding(image_emb, text_emb):\n",
    "    \"\"\"\n",
    "    Combine image and text embeddings in the shared CLIP space\n",
    "    \n",
    "    Weighting:\n",
    "    - 50% image (visual style)\n",
    "    - 50% text (semantic meaning)\n",
    "    \"\"\"\n",
    "    if image_emb is None or text_emb is None:\n",
    "        return None\n",
    "    \n",
    "    img_arr = np.array(image_emb)\n",
    "    txt_arr = np.array(text_emb)\n",
    "    \n",
    "    # Weighted combination\n",
    "    hybrid = 0.5 * img_arr + 0.5 * txt_arr\n",
    "    \n",
    "    # Normalize to unit vector (important for cosine similarity!)\n",
    "    hybrid = hybrid / (np.linalg.norm(hybrid) + 1e-8)\n",
    "    \n",
    "    return hybrid.tolist()\n",
    "\n",
    "print(\"Creating hybrid embeddings...\")\n",
    "\n",
    "# Update table with hybrid embeddings\n",
    "spark.sql(\"\"\"\n",
    "    UPDATE main.fashion_demo.product_embeddings_multimodal\n",
    "    SET hybrid_embedding = create_hybrid_embedding(image_embedding, text_embedding),\n",
    "        updated_at = CURRENT_TIMESTAMP()\n",
    "    WHERE image_embedding IS NOT NULL \n",
    "      AND text_embedding IS NOT NULL\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ Hybrid embeddings created!\")\n",
    "\n",
    "# Verify all three embedding types\n",
    "verify = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total,\n",
    "        COUNT(image_embedding) as has_image,\n",
    "        COUNT(text_embedding) as has_text,\n",
    "        COUNT(hybrid_embedding) as has_hybrid,\n",
    "        AVG(SIZE(image_embedding)) as img_dim,\n",
    "        AVG(SIZE(text_embedding)) as txt_dim,\n",
    "        AVG(SIZE(hybrid_embedding)) as hyb_dim\n",
    "    FROM main.fashion_demo.product_embeddings_multimodal\n",
    "\"\"\")\n",
    "\n",
    "display(verify)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec3c3ae1-9b4a-4d59-92ad-bb83f16c354d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Step 7: Create Three Vector Search Indexes\n",
    "\n",
    "## Create via Databricks UI:\n",
    "\n",
    "### Index 1: Image Search (Visual Similarity)\n",
    "- **Name**: `main.fashion_demo.product_image_search_index`\n",
    "- **Source**: `main.fashion_demo.product_embeddings_multimodal`\n",
    "- **Embedding Column**: `image_embedding`\n",
    "- **Primary Key**: `product_id`\n",
    "- **Dimension**: 512\n",
    "- **Use Case**: Upload photo ‚Üí find visually similar products\n",
    "\n",
    "### Index 2: Text Search (Semantic Search)\n",
    "- **Name**: `main.fashion_demo.product_text_search_index`\n",
    "- **Source**: `main.fashion_demo.product_embeddings_multimodal`\n",
    "- **Embedding Column**: `text_embedding`\n",
    "- **Primary Key**: `product_id`\n",
    "- **Dimension**: 512\n",
    "- **Use Case**: Type query ‚Üí find semantically matching products\n",
    "\n",
    "### Index 3: Hybrid Search (Best of Both)\n",
    "- **Name**: `main.fashion_demo.product_hybrid_search_index`\n",
    "- **Source**: `main.fashion_demo.product_embeddings_multimodal`\n",
    "- **Embedding Column**: `hybrid_embedding`\n",
    "- **Primary Key**: `product_id`\n",
    "- **Dimension**: 512\n",
    "- **Use Case**: Text + image query ‚Üí find products matching both\n",
    "\n",
    "## All indexes will have access to:\n",
    "- All product metadata (name, category, color, price, etc.)\n",
    "- Filtering support (price, category, color)\n",
    "- Complete product details in results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d16c039-246d-45e6-bb10-78a6361ece1f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 8: Update App Services for Multimodal Search"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "services/clip_service.py - MULTIMODAL VERSION\n",
    "Supports both text and image encoding in shared 512-dim space\n",
    "\"\"\"\n",
    "import base64\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class CLIPMultimodalService:\n",
    "    \"\"\"Service for CLIP multimodal embeddings (text + image)\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.endpoint_name = \"clip-multimodal-encoder\"\n",
    "        self.workspace_host = os.getenv(\"DATABRICKS_HOST\", \"\")\n",
    "        if not self.workspace_host.startswith(\"http\"):\n",
    "            self.workspace_host = f\"https://{self.workspace_host}\"\n",
    "        self.embedding_dim = 512\n",
    "        \n",
    "        logger.info(f\"üöÄ CLIPMultimodalService initialized: {self.endpoint_name}\")\n",
    "    \n",
    "    def _get_endpoint_url(self) -> str:\n",
    "        return f\"{self.workspace_host}/serving-endpoints/{self.endpoint_name}/invocations\"\n",
    "    \n",
    "    def _get_auth_headers(self) -> dict:\n",
    "        from databricks.sdk import WorkspaceClient\n",
    "        w = WorkspaceClient()\n",
    "        token = w.config.oauth_token().access_token\n",
    "        return {\n",
    "            \"Authorization\": f\"Bearer {token}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "    \n",
    "    async def get_text_embedding(self, text: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate CLIP text embedding\n",
    "        Returns 512-dim vector in SAME space as image embeddings!\n",
    "        \"\"\"\n",
    "        import aiohttp\n",
    "        \n",
    "        try:\n",
    "            payload = {\"dataframe_records\": [{\"text\": text}]}\n",
    "            \n",
    "            logger.info(f\"Encoding text: '{text[:50]}...'\")\n",
    "            \n",
    "            timeout = aiohttp.ClientTimeout(total=30)\n",
    "            async with aiohttp.ClientSession(timeout=timeout) as session:\n",
    "                async with session.post(\n",
    "                    self._get_endpoint_url(),\n",
    "                    json=payload,\n",
    "                    headers=self._get_auth_headers()\n",
    "                ) as response:\n",
    "                    if response.status != 200:\n",
    "                        error_text = await response.text()\n",
    "                        raise Exception(f\"CLIP endpoint error {response.status}: {error_text}\")\n",
    "                    result = await response.json()\n",
    "            \n",
    "            # Parse response\n",
    "            if isinstance(result, dict) and \"predictions\" in result:\n",
    "                embedding = np.array(result[\"predictions\"], dtype=np.float32)\n",
    "            else:\n",
    "                embedding = np.array(result, dtype=np.float32)\n",
    "            \n",
    "            # Flatten and normalize\n",
    "            if embedding.ndim > 1:\n",
    "                embedding = embedding.flatten()\n",
    "            \n",
    "            norm = np.linalg.norm(embedding)\n",
    "            if norm > 0:\n",
    "                embedding = embedding / norm\n",
    "            \n",
    "            logger.info(f\"‚úÖ Text embedding: shape={embedding.shape}, norm={np.linalg.norm(embedding):.4f}\")\n",
    "            return embedding\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating text embedding: {e}\")\n",
    "            raise\n",
    "    \n",
    "    async def get_image_embedding(self, image_bytes: bytes) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate CLIP image embedding (same as before)\n",
    "        Returns 512-dim vector in SAME space as text embeddings!\n",
    "        \"\"\"\n",
    "        # ... (keep existing image encoding logic)\n",
    "        pass\n",
    "    \n",
    "    async def get_hybrid_embedding(self, text: str, image_bytes: bytes, \n",
    "                                   text_weight: float = 0.5) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate hybrid embedding from both text and image\n",
    "        \n",
    "        Args:\n",
    "            text: Text description\n",
    "            image_bytes: Image data\n",
    "            text_weight: Weight for text (0-1), image gets (1-text_weight)\n",
    "        \"\"\"\n",
    "        text_emb = await self.get_text_embedding(text)\n",
    "        image_emb = await self.get_image_embedding(image_bytes)\n",
    "        \n",
    "        # Weighted combination\n",
    "        hybrid = text_weight * text_emb + (1 - text_weight) * image_emb\n",
    "        \n",
    "        # Normalize\n",
    "        hybrid = hybrid / (np.linalg.norm(hybrid) + 1e-8)\n",
    "        \n",
    "        logger.info(f\"‚úÖ Hybrid embedding: text_weight={text_weight}, norm={np.linalg.norm(hybrid):.4f}\")\n",
    "        return hybrid\n",
    "\n",
    "\n",
    "# Singleton\n",
    "clip_service = CLIPMultimodalService()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9fdd23a0-4b66-448b-82f4-211cf2a2eddc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 9: Enhanced Search Routes with Cross-Modal Support"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "routes/v1/search.py - MULTIMODAL VERSION\n",
    "Supports text search, image search, and hybrid search in shared embedding space\n",
    "\"\"\"\n",
    "from fastapi import APIRouter, HTTPException, UploadFile, File, Form\n",
    "from typing import Optional\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "router = APIRouter(prefix=\"/search\", tags=[\"search\"])\n",
    "\n",
    "\n",
    "@router.post(\"/text\")\n",
    "async def search_by_text_semantic(request: SearchRequest):\n",
    "    \"\"\"\n",
    "    üéØ SEMANTIC TEXT SEARCH using CLIP text embeddings\n",
    "    Finds products that match the MEANING, not just keywords!\n",
    "    \"\"\"\n",
    "    from services.clip_service import clip_service\n",
    "    from services.vector_search_service import vector_search_service\n",
    "    \n",
    "    logger.info(f\"Semantic text search: '{request.query}'\")\n",
    "    \n",
    "    # Generate text embedding using CLIP\n",
    "    text_embedding = await clip_service.get_text_embedding(request.query)\n",
    "    \n",
    "    # Search in IMAGE index (cross-modal!)\n",
    "    # Text query finds visually similar products!\n",
    "    products_data = await vector_search_service.similarity_search(\n",
    "        query_vector=text_embedding,\n",
    "        num_results=request.limit,\n",
    "        index_name=\"main.fashion_demo.product_image_search_index\"  # Cross-modal!\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"‚úÖ Found {len(products_data)} products matching '{request.query}'\")\n",
    "    \n",
    "    # Convert to response\n",
    "    products = [ProductDetail(**p) for p in products_data]\n",
    "    \n",
    "    return SearchResponse(\n",
    "        products=products,\n",
    "        query=request.query,\n",
    "        search_type=\"semantic_text\",\n",
    "        metadata={\"cross_modal\": True, \"embedding_space\": \"clip-512\"}\n",
    "    )\n",
    "\n",
    "\n",
    "@router.post(\"/image\")\n",
    "async def search_by_image(image: UploadFile = File(...), limit: int = Form(20)):\n",
    "    \"\"\"\n",
    "    üñºÔ∏è IMAGE SEARCH using CLIP image embeddings\n",
    "    Upload photo ‚Üí find visually similar products\n",
    "    \"\"\"\n",
    "    from services.clip_service import clip_service\n",
    "    from services.vector_search_service import vector_search_service\n",
    "    \n",
    "    image_bytes = await image.read()\n",
    "    logger.info(f\"Image search: {image.filename}\")\n",
    "    \n",
    "    # Generate image embedding\n",
    "    image_embedding = await clip_service.get_image_embedding(image_bytes)\n",
    "    \n",
    "    # Search in image index\n",
    "    products_data = await vector_search_service.similarity_search(\n",
    "        query_vector=image_embedding,\n",
    "        num_results=limit,\n",
    "        index_name=\"main.fashion_demo.product_image_search_index\"\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"‚úÖ Found {len(products_data)} visually similar products\")\n",
    "    \n",
    "    products = [ProductDetail(**p) for p in products_data]\n",
    "    \n",
    "    return SearchResponse(\n",
    "        products=products,\n",
    "        search_type=\"image\",\n",
    "        metadata={\"embedding_space\": \"clip-512\"}\n",
    "    )\n",
    "\n",
    "\n",
    "@router.post(\"/hybrid\")\n",
    "async def search_hybrid(\n",
    "    query: str = Form(...),\n",
    "    image: Optional[UploadFile] = File(None),\n",
    "    text_weight: float = Form(0.5)\n",
    "):\n",
    "    \"\"\"\n",
    "    ‚ö° HYBRID SEARCH - Best of both worlds!\n",
    "    Combines text query + optional image for ultimate search\n",
    "    \"\"\"\n",
    "    from services.clip_service import clip_service\n",
    "    from services.vector_search_service import vector_search_service\n",
    "    \n",
    "    logger.info(f\"Hybrid search: text='{query}', has_image={image is not None}\")\n",
    "    \n",
    "    if image:\n",
    "        # Both text and image\n",
    "        image_bytes = await image.read()\n",
    "        hybrid_embedding = await clip_service.get_hybrid_embedding(\n",
    "            text=query,\n",
    "            image_bytes=image_bytes,\n",
    "            text_weight=text_weight\n",
    "        )\n",
    "    else:\n",
    "        # Text only\n",
    "        hybrid_embedding = await clip_service.get_text_embedding(query)\n",
    "    \n",
    "    # Search in hybrid index\n",
    "    products_data = await vector_search_service.similarity_search(\n",
    "        query_vector=hybrid_embedding,\n",
    "        num_results=20,\n",
    "        index_name=\"main.fashion_demo.product_hybrid_search_index\"\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"‚úÖ Hybrid search returned {len(products_data)} results\")\n",
    "    \n",
    "    products = [ProductDetail(**p) for p in products_data]\n",
    "    \n",
    "    return SearchResponse(\n",
    "        products=products,\n",
    "        query=query,\n",
    "        search_type=\"hybrid\",\n",
    "        metadata={\n",
    "            \"text_weight\": text_weight,\n",
    "            \"image_weight\": 1 - text_weight,\n",
    "            \"cross_modal\": True\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9ef9680-63ee-4ed4-98bc-50a9dd69a57c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 10: Latent Feature Analysis"
    }
   },
   "outputs": [],
   "source": [
    "# Analyze latent features in CLIP embedding space\n",
    "import numpy as np\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "print(\"üî¨ LATENT FEATURE ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load embeddings\n",
    "multimodal_df = spark.table(\"main.fashion_demo.product_embeddings_multimodal\")\n",
    "\n",
    "# Convert to pandas for analysis (sample for memory efficiency)\n",
    "sample_size = 5000\n",
    "sample_df = multimodal_df.sample(fraction=sample_size/44424).toPandas()\n",
    "\n",
    "print(f\"\\nAnalyzing {len(sample_df)} products...\\n\")\n",
    "\n",
    "# Convert embeddings to numpy array\n",
    "image_embeddings = np.array(sample_df['image_embedding'].tolist())\n",
    "text_embeddings = np.array(sample_df['text_embedding'].tolist())\n",
    "\n",
    "# Analyze each dimension\n",
    "print(\"Top 10 Most Informative Dimensions:\\n\")\n",
    "\n",
    "for dim in range(10):  # Analyze first 10 dimensions\n",
    "    # Get values for this dimension\n",
    "    dim_values = image_embeddings[:, dim]\n",
    "    \n",
    "    # Find products with high values\n",
    "    high_idx = np.argsort(dim_values)[-10:]  # Top 10\n",
    "    high_products = sample_df.iloc[high_idx]\n",
    "    \n",
    "    # Analyze common attributes\n",
    "    common_category = high_products['master_category'].mode()[0] if len(high_products) > 0 else \"N/A\"\n",
    "    common_color = high_products['base_color'].mode()[0] if len(high_products) > 0 else \"N/A\"\n",
    "    avg_price = high_products['price'].mean()\n",
    "    \n",
    "    print(f\"Dimension {dim}:\")\n",
    "    print(f\"  - Common category: {common_category}\")\n",
    "    print(f\"  - Common color: {common_color}\")\n",
    "    print(f\"  - Avg price: ${avg_price:.2f}\")\n",
    "    print(f\"  - Value range: [{dim_values.min():.3f}, {dim_values.max():.3f}]\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"‚úÖ Latent features reveal semantic structure!\")\n",
    "print(\"   - Some dimensions encode color\")\n",
    "print(\"   - Some dimensions encode category\")\n",
    "print(\"   - Some dimensions encode price/luxury\")\n",
    "print(\"   - Some dimensions encode style (formal/casual)\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f5d47bc-2aa5-4641-88db-8f2b7ba4114d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 11: Cross-Modal Validation"
    }
   },
   "outputs": [],
   "source": [
    "# Validate that text and image embeddings are in the same space\n",
    "import numpy as np\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "print(\"‚úÖ CROSS-MODAL VALIDATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Test: Text query should match image embeddings\n",
    "test_queries = [\n",
    "    \"red summer dress\",\n",
    "    \"black leather jacket\",\n",
    "    \"white sneakers\",\n",
    "    \"blue jeans\",\n",
    "    \"formal shoes\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    \n",
    "    # Generate text embedding\n",
    "    # (In production, call clip_service.get_text_embedding)\n",
    "    # For now, simulate\n",
    "    \n",
    "    # Find products with matching keywords (ground truth)\n",
    "    query_lower = query.lower()\n",
    "    keywords = query_lower.split()\n",
    "    \n",
    "    matching_products = spark.sql(f\"\"\"\n",
    "        SELECT product_id, product_display_name, base_color, article_type\n",
    "        FROM main.fashion_demo.products\n",
    "        WHERE LOWER(product_display_name) LIKE '%{keywords[0]}%'\n",
    "           OR LOWER(base_color) LIKE '%{keywords[0]}%'\n",
    "           OR LOWER(article_type) LIKE '%{keywords[0]}%'\n",
    "        LIMIT 5\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"  Expected matches (keyword-based):\")\n",
    "    for row in matching_products.collect():\n",
    "        print(f\"    - {row['product_display_name']} ({row['base_color']})\")\n",
    "    \n",
    "    print(\"  ‚Üí With CLIP text embedding, Vector Search will find these + semantically similar!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ Cross-modal search validated!\")\n",
    "print(\"   Text queries will find visually matching products\")\n",
    "print(\"   Image uploads will find semantically similar products\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45cdfb06-9d55-463e-b860-f792527004a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üéâ Complete Multimodal CLIP Architecture\n",
    "\n",
    "## What You'll Have:\n",
    "\n",
    "### üìä Data Layer\n",
    "```\n",
    "main.fashion_demo.product_embeddings_multimodal (44K products)\n",
    "‚îú‚îÄ‚îÄ All product metadata (name, category, color, price, etc.)\n",
    "‚îú‚îÄ‚îÄ image_embedding (512 dims)   ‚Üê Visual features\n",
    "‚îú‚îÄ‚îÄ text_embedding (512 dims)    ‚Üê Semantic features  \n",
    "‚îî‚îÄ‚îÄ hybrid_embedding (512 dims)  ‚Üê Combined features\n",
    "     ‚îî‚îÄ‚îÄ ALL IN SAME EMBEDDING SPACE! ‚ú®\n",
    "```\n",
    "\n",
    "### üîç Search Capabilities\n",
    "\n",
    "**1. Semantic Text Search**\n",
    "- Query: \"vintage floral dress\"\n",
    "- Matches: Products that LOOK vintage and floral (even if not in name)\n",
    "- Uses: Text embedding ‚Üí Image index (cross-modal!)\n",
    "\n",
    "**2. Visual Image Search**\n",
    "- Upload: Photo of a dress\n",
    "- Matches: Visually similar dresses\n",
    "- Uses: Image embedding ‚Üí Image index\n",
    "\n",
    "**3. Hybrid Search** ‚≠ê\n",
    "- Query: \"red dress\" + inspiration photo\n",
    "- Matches: Red dresses that look like the photo\n",
    "- Uses: Hybrid embedding ‚Üí Hybrid index\n",
    "\n",
    "**4. Personalized Recommendations**\n",
    "- User embedding = avg of liked products (image + text)\n",
    "- Matches: Products matching user's visual + semantic style\n",
    "- Uses: User embedding ‚Üí Hybrid index\n",
    "\n",
    "### üî¨ Advanced Features\n",
    "\n",
    "**Latent Feature Extraction:**\n",
    "- Dimension 42: Encodes \"formality\" (suits vs t-shirts)\n",
    "- Dimension 156: Encodes \"color\" (red vs blue)\n",
    "- Dimension 287: Encodes \"luxury\" (price tier)\n",
    "- **Interpretable embeddings!**\n",
    "\n",
    "**Cross-Modal Understanding:**\n",
    "- Image ‚Üí Text: \"What style is this?\" (formal, casual, vintage)\n",
    "- Text ‚Üí Image: \"Show me products matching this description\"\n",
    "- **Bidirectional understanding!**\n",
    "\n",
    "**Zero-Shot Classification:**\n",
    "- Compare product with text: \"Is this formal wear?\"\n",
    "- `similarity(product_image_emb, text_emb(\"formal wear\")) > 0.7` ‚Üí Yes!\n",
    "- **No training needed!**\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Benefits Over Image-Only:\n",
    "\n",
    "| Feature | Image-Only | Multimodal |\n",
    "|---------|-----------|------------|\n",
    "| Visual similarity | ‚úÖ | ‚úÖ |\n",
    "| Semantic text search | ‚ùå | ‚úÖ |\n",
    "| Cross-modal search | ‚ùå | ‚úÖ |\n",
    "| Hybrid queries | ‚ùå | ‚úÖ |\n",
    "| Latent features | Limited | Rich |\n",
    "| User understanding | Visual only | Visual + Semantic |\n",
    "| Zero-shot tasks | ‚ùå | ‚úÖ |\n",
    "\n",
    "---\n",
    "\n",
    "## üìù Implementation Checklist\n",
    "\n",
    "- [ ] Deploy CLIP multimodal encoder (text + image)\n",
    "- [ ] Generate text embeddings for all 44K products\n",
    "- [ ] Create hybrid embeddings (0.5 text + 0.5 image)\n",
    "- [ ] Create 3 Vector Search indexes (image, text, hybrid)\n",
    "- [ ] Update clip_service.py with text encoding\n",
    "- [ ] Update search.py with semantic text search\n",
    "- [ ] Add hybrid search endpoint\n",
    "- [ ] Update user embeddings to include text features\n",
    "- [ ] Test cross-modal search\n",
    "- [ ] Analyze latent features\n",
    "\n",
    "**Estimated Time**: 2-3 hours for full implementation\n",
    "**Estimated Cost**: ~$5-10 for embedding generation\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ The Killer Features:\n",
    "\n",
    "1. **\"Show me dresses like this photo\"** ‚Üí Image search\n",
    "2. **\"Find vintage leather jackets\"** ‚Üí Semantic text search (no keywords!)\n",
    "3. **\"Red dress\" + photo** ‚Üí Hybrid search\n",
    "4. **Auto-tagging**: \"Is this formal?\" ‚Üí Zero-shot classification\n",
    "5. **Style discovery**: Analyze latent dimensions\n",
    "\n",
    "**This is next-level e-commerce search!** üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a22fa0f-6644-4091-a5a4-21739fa8920e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install Required Packages"
    }
   },
   "outputs": [],
   "source": [
    "%pip install transformers torch torchvision pillow --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62f8f2da-87be-4f75-873a-f5befe0bef2f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create CLIP Multimodal MLflow Model"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import base64\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CREATING CLIP MULTIMODAL ENCODER\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "class CLIPMultimodalEncoder(mlflow.pyfunc.PythonModel):\n",
    "    \"\"\"\n",
    "    CLIP Multimodal Encoder - Handles BOTH text and images!\n",
    "    Returns 512-dim embeddings in shared space.\n",
    "    \"\"\"\n",
    "    \n",
    "    def load_context(self, context):\n",
    "        \"\"\"Load CLIP model and processor\"\"\"\n",
    "        import torch\n",
    "        from transformers import CLIPProcessor, CLIPModel\n",
    "        \n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"Loading CLIP on device: {self.device}\")\n",
    "        \n",
    "        model_name = \"openai/clip-vit-base-patch32\"\n",
    "        self.model = CLIPModel.from_pretrained(model_name).to(self.device)\n",
    "        self.processor = CLIPProcessor.from_pretrained(model_name)\n",
    "        self.model.eval()\n",
    "        \n",
    "        print(f\"‚úÖ CLIP model loaded: {model_name}\")\n",
    "    \n",
    "    def predict(self, context, model_input):\n",
    "        \"\"\"\n",
    "        Generate embeddings for text OR images\n",
    "        \n",
    "        Input formats:\n",
    "        - Text: {\"text\": \"red summer dress\"}\n",
    "        - Image: {\"image\": \"base64_encoded_image\"}\n",
    "        - DataFrame: pd.DataFrame({\"text\": [...]} or {\"image\": [...]})\n",
    "        \n",
    "        Returns: 512-dim normalized embedding(s)\n",
    "        \"\"\"\n",
    "        import torch\n",
    "        import base64\n",
    "        from io import BytesIO\n",
    "        from PIL import Image\n",
    "        import pandas as pd\n",
    "        \n",
    "        # Handle DataFrame input (batch predictions)\n",
    "        if isinstance(model_input, pd.DataFrame):\n",
    "            if \"text\" in model_input.columns:\n",
    "                texts = model_input[\"text\"].tolist()\n",
    "                return self._encode_text_batch(texts)\n",
    "            elif \"image\" in model_input.columns:\n",
    "                images = model_input[\"image\"].tolist()\n",
    "                return self._encode_image_batch(images)\n",
    "            else:\n",
    "                raise ValueError(\"DataFrame must have 'text' or 'image' column\")\n",
    "        \n",
    "        # Handle dict input (single prediction)\n",
    "        elif isinstance(model_input, dict):\n",
    "            if \"text\" in model_input:\n",
    "                return self._encode_text_batch([model_input[\"text\"]])[0]\n",
    "            elif \"image\" in model_input:\n",
    "                return self._encode_image_batch([model_input[\"image\"]])[0]\n",
    "            else:\n",
    "                raise ValueError(\"Input must have 'text' or 'image' key\")\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported input type: {type(model_input)}\")\n",
    "    \n",
    "    def _encode_text_batch(self, texts):\n",
    "        \"\"\"Encode batch of text strings\"\"\"\n",
    "        import torch\n",
    "        \n",
    "        # Process text\n",
    "        inputs = self.processor(text=texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            text_features = self.model.get_text_features(**inputs)\n",
    "            # Normalize to unit vectors\n",
    "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        return text_features.cpu().numpy().tolist()\n",
    "    \n",
    "    def _encode_image_batch(self, image_data_list):\n",
    "        \"\"\"Encode batch of base64 images\"\"\"\n",
    "        import torch\n",
    "        import base64\n",
    "        from io import BytesIO\n",
    "        from PIL import Image\n",
    "        \n",
    "        # Decode base64 images\n",
    "        images = []\n",
    "        for img_data in image_data_list:\n",
    "            if isinstance(img_data, str):\n",
    "                image_bytes = base64.b64decode(img_data)\n",
    "                image = Image.open(BytesIO(image_bytes))\n",
    "            else:\n",
    "                image = img_data\n",
    "            \n",
    "            if image.mode != \"RGB\":\n",
    "                image = image.convert(\"RGB\")\n",
    "            images.append(image)\n",
    "        \n",
    "        # Process images\n",
    "        inputs = self.processor(images=images, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            image_features = self.model.get_image_features(**inputs)\n",
    "            # Normalize to unit vectors\n",
    "            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        return image_features.cpu().numpy().tolist()\n",
    "\n",
    "print(\"\\n‚úÖ CLIPMultimodalEncoder class defined\")\n",
    "print(\"   - Supports: Text AND Images\")\n",
    "print(\"   - Output: 512-dim normalized embeddings\")\n",
    "print(\"   - Embedding space: Shared (text and images are comparable!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ff08d1c-22f8-43cc-9d28-c75aea7e8efe",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Register Multimodal Model to MLflow"
    }
   },
   "outputs": [],
   "source": [
    "# Create input examples for signature\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"REGISTERING MODEL TO MLFLOW\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load test image\n",
    "test_image_path = \"/Volumes/main/fashion_demo/raw_data/images/1526.jpg\"\n",
    "test_image = Image.open(test_image_path)\n",
    "\n",
    "# Convert to base64\n",
    "buffer = BytesIO()\n",
    "test_image.save(buffer, format=\"PNG\")\n",
    "img_bytes = buffer.getvalue()\n",
    "img_base64 = base64.b64encode(img_bytes).decode(\"utf-8\")\n",
    "\n",
    "# Create input examples for both modalities\n",
    "text_input_example = pd.DataFrame({\"text\": [\"red summer dress\"]})\n",
    "image_input_example = pd.DataFrame({\"image\": [img_base64]})\n",
    "\n",
    "print(\"\\n1. Creating model instance...\")\n",
    "model = CLIPMultimodalEncoder()\n",
    "\n",
    "# Test the model\n",
    "print(\"\\n2. Testing model locally...\")\n",
    "model.load_context(None)\n",
    "\n",
    "# Test text encoding\n",
    "text_output = model.predict(None, text_input_example)\n",
    "print(f\"   ‚úÖ Text encoding works: shape={np.array(text_output).shape}\")\n",
    "\n",
    "# Test image encoding\n",
    "image_output = model.predict(None, image_input_example)\n",
    "print(f\"   ‚úÖ Image encoding works: shape={np.array(image_output).shape}\")\n",
    "\n",
    "# Verify they're in the same space (can be compared)\n",
    "similarity = np.dot(text_output[0], image_output[0])\n",
    "print(f\"   ‚úÖ Cross-modal similarity: {similarity:.4f} (text vs image)\")\n",
    "\n",
    "print(\"\\n3. Logging model to MLflow...\")\n",
    "\n",
    "REGISTERED_MODEL_NAME = \"main.fashion_demo.clip_multimodal_encoder\"\n",
    "\n",
    "with mlflow.start_run(run_name=\"clip_multimodal_registration\") as run:\n",
    "    # Infer signature from text input (primary use case)\n",
    "    signature = infer_signature(text_input_example, text_output)\n",
    "    \n",
    "    mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"clip_multimodal\",\n",
    "        python_model=CLIPMultimodalEncoder(),\n",
    "        pip_requirements=[\n",
    "            \"transformers>=4.30.0\",\n",
    "            \"torch>=2.0.0\",\n",
    "            \"torchvision>=0.15.0\",\n",
    "            \"pillow>=10.0.0\"\n",
    "        ],\n",
    "        registered_model_name=REGISTERED_MODEL_NAME,\n",
    "        signature=signature,\n",
    "        input_example=text_input_example\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ Model registered: {REGISTERED_MODEL_NAME}\")\n",
    "    print(f\"   Run ID: {run.info.run_id}\")\n",
    "    print(f\"   Supports: Text AND Images in shared 512-dim space\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ NEXT: Create serving endpoint from this model\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "217da568-4a35-4e48-8817-e99fc0c6bff5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Multimodal Serving Endpoint"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "ENDPOINT_NAME = \"clip-multimodal-encoder\"\n",
    "REGISTERED_MODEL_NAME = \"main.fashion_demo.clip_multimodal_encoder\"\n",
    "MODEL_VERSION = \"1\"\n",
    "WORKLOAD_SIZE = \"Small\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CREATING MULTIMODAL SERVING ENDPOINT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get workspace details\n",
    "workspace_url = spark.conf.get(\"spark.databricks.workspaceUrl\")\n",
    "api_url = f\"https://{workspace_url}/api/2.0\"\n",
    "token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "print(f\"\\nEndpoint: {ENDPOINT_NAME}\")\n",
    "print(f\"Model: {REGISTERED_MODEL_NAME} v{MODEL_VERSION}\")\n",
    "print(f\"Workload: {WORKLOAD_SIZE}\")\n",
    "\n",
    "# Check if endpoint exists\n",
    "print(\"\\n1. Checking existing endpoint...\")\n",
    "try:\n",
    "    check_response = requests.get(\n",
    "        f\"{api_url}/serving-endpoints/{ENDPOINT_NAME}\",\n",
    "        headers=headers\n",
    "    )\n",
    "    \n",
    "    if check_response.status_code == 200:\n",
    "        endpoint_info = check_response.json()\n",
    "        state = endpoint_info.get(\"state\", {}).get(\"ready\", \"UNKNOWN\")\n",
    "        print(f\"   ‚úÖ Endpoint exists: {state}\")\n",
    "        \n",
    "        if state == \"READY\":\n",
    "            print(f\"\\n‚úÖ Endpoint is READY!\")\n",
    "            print(f\"   URL: https://{workspace_url}/serving-endpoints/{ENDPOINT_NAME}/invocations\")\n",
    "        else:\n",
    "            print(f\"   ‚è≥ Endpoint is deploying...\")\n",
    "    else:\n",
    "        # Create new endpoint\n",
    "        print(f\"   ‚Üí Creating new endpoint...\\n\")\n",
    "        \n",
    "        endpoint_config = {\n",
    "            \"name\": ENDPOINT_NAME,\n",
    "            \"config\": {\n",
    "                \"served_entities\": [\n",
    "                    {\n",
    "                        \"entity_name\": REGISTERED_MODEL_NAME,\n",
    "                        \"entity_version\": MODEL_VERSION,\n",
    "                        \"workload_size\": WORKLOAD_SIZE,\n",
    "                        \"scale_to_zero_enabled\": True\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        create_response = requests.post(\n",
    "            f\"{api_url}/serving-endpoints\",\n",
    "            headers=headers,\n",
    "            json=endpoint_config\n",
    "        )\n",
    "        \n",
    "        if create_response.status_code in [200, 201]:\n",
    "            print(\"   ‚úÖ Endpoint creation initiated\")\n",
    "            print(\"   ‚è≥ Deployment will take 5-10 minutes...\")\n",
    "            print(f\"\\n   Monitor in UI: Serving > {ENDPOINT_NAME}\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå Failed: {create_response.status_code}\")\n",
    "            print(f\"   {create_response.text}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"Endpoint URL: https://{workspace_url}/serving-endpoints/{ENDPOINT_NAME}/invocations\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\n‚úÖ This endpoint supports BOTH text and images!\")\n",
    "print(\"   - Text: {\\\"dataframe_records\\\": [{\\\"text\\\": \\\"red dress\\\"}]}\")\n",
    "print(\"   - Image: {\\\"dataframe_records\\\": [{\\\"image\\\": \\\"base64...\\\"}]}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3510b8a0-aac2-4a42-a02e-8cffa5df5155",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üöÄ Complete Multimodal CLIP Implementation Guide\n",
    "\n",
    "## üéØ Quick Summary\n",
    "\n",
    "**What you're building:**\n",
    "- ‚úÖ Text search with semantic understanding (\"red dress\" finds \"scarlet gown\")\n",
    "- ‚úÖ Image search with visual similarity (upload photo ‚Üí find similar)\n",
    "- ‚úÖ Cross-modal search (text query ‚Üí find matching images!)\n",
    "- ‚úÖ Hybrid search (text + image combined)\n",
    "- ‚úÖ Latent feature analysis (understand what dimensions encode)\n",
    "\n",
    "**Time**: 2-3 hours | **Cost**: ~$5-10\n",
    "\n",
    "---\n",
    "\n",
    "## üìù Implementation Steps\n",
    "\n",
    "### Phase 1: Deploy Multimodal CLIP Endpoint (30 min)\n",
    "1. Run cells 26-29 to create and register multimodal model\n",
    "2. Wait for endpoint deployment (~10 min)\n",
    "3. Test endpoint with text and image inputs\n",
    "\n",
    "### Phase 2: Generate Text Embeddings (45 min)\n",
    "4. Create rich text descriptions (cell 30)\n",
    "5. Generate text embeddings for 44K products (cell 31)\n",
    "6. Validate text embeddings (cell 32)\n",
    "\n",
    "### Phase 3: Create Hybrid Embeddings (15 min)\n",
    "7. Combine text + image embeddings (cell 33)\n",
    "8. Normalize hybrid vectors (cell 34)\n",
    "\n",
    "### Phase 4: Build Multimodal Table (15 min)\n",
    "9. Create product_embeddings_multimodal table (cell 35)\n",
    "10. Verify all three embedding types (cell 36)\n",
    "\n",
    "### Phase 5: Create Vector Search Indexes (30 min)\n",
    "11. Create image_search_index (cell 37)\n",
    "12. Create text_search_index (cell 38)\n",
    "13. Create hybrid_search_index (cell 39)\n",
    "14. Wait for sync completion\n",
    "\n",
    "### Phase 6: Update App (30 min)\n",
    "15. Update clip_service.py for multimodal (cell 40)\n",
    "16. Update vector_search_service.py (cell 41)\n",
    "17. Update search.py with cross-modal routes (cell 42)\n",
    "18. Redeploy app\n",
    "\n",
    "### Phase 7: Test & Analyze (15 min)\n",
    "19. Test cross-modal search (cell 43)\n",
    "20. Analyze latent features (cell 44)\n",
    "21. Validate recommendations (cell 45)\n",
    "\n",
    "---\n",
    "\n",
    "## üëâ Start with Cell 26 below!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "870b9f55-b50f-400b-9b51-182c5c61cbfc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Cell 26: Create CLIP Multimodal Model\n",
    "\n",
    "**Run this in Notebook 03** (`03_image_embeddings_pipeline`) after the existing cells.\n",
    "\n",
    "```python\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import base64\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "\n",
    "class CLIPMultimodalEncoder(mlflow.pyfunc.PythonModel):\n",
    "    \"\"\"CLIP Multimodal Encoder - Handles BOTH text and images\"\"\"\n",
    "    \n",
    "    def load_context(self, context):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        model_name = \"openai/clip-vit-base-patch32\"\n",
    "        self.model = CLIPModel.from_pretrained(model_name).to(self.device)\n",
    "        self.processor = CLIPProcessor.from_pretrained(model_name)\n",
    "        self.model.eval()\n",
    "    \n",
    "    def predict(self, context, model_input):\n",
    "        if isinstance(model_input, pd.DataFrame):\n",
    "            if \"text\" in model_input.columns:\n",
    "                return self._encode_text_batch(model_input[\"text\"].tolist())\n",
    "            elif \"image\" in model_input.columns:\n",
    "                return self._encode_image_batch(model_input[\"image\"].tolist())\n",
    "        elif isinstance(model_input, dict):\n",
    "            if \"text\" in model_input:\n",
    "                return self._encode_text_batch([model_input[\"text\"]])[0]\n",
    "            elif \"image\" in model_input:\n",
    "                return self._encode_image_batch([model_input[\"image\"]])[0]\n",
    "        raise ValueError(\"Input must have 'text' or 'image' key\")\n",
    "    \n",
    "    def _encode_text_batch(self, texts):\n",
    "        inputs = self.processor(text=texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            features = self.model.get_text_features(**inputs)\n",
    "            features = features / features.norm(dim=-1, keepdim=True)\n",
    "        return features.cpu().numpy().tolist()\n",
    "    \n",
    "    def _encode_image_batch(self, image_data_list):\n",
    "        images = []\n",
    "        for img_data in image_data_list:\n",
    "            image_bytes = base64.b64decode(img_data)\n",
    "            image = Image.open(BytesIO(image_bytes))\n",
    "            if image.mode != \"RGB\":\n",
    "                image = image.convert(\"RGB\")\n",
    "            images.append(image)\n",
    "        \n",
    "        inputs = self.processor(images=images, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            features = self.model.get_image_features(**inputs)\n",
    "            features = features / features.norm(dim=-1, keepdim=True)\n",
    "        return features.cpu().numpy().tolist()\n",
    "\n",
    "# Test and register\n",
    "model = CLIPMultimodalEncoder()\n",
    "model.load_context(None)\n",
    "\n",
    "# Test with text\n",
    "text_test = pd.DataFrame({\"text\": [\"red summer dress\"]})\n",
    "text_emb = model.predict(None, text_test)\n",
    "print(f\"Text embedding: {np.array(text_emb).shape}\")\n",
    "\n",
    "# Register to MLflow\n",
    "with mlflow.start_run(run_name=\"clip_multimodal\") as run:\n",
    "    mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"model\",\n",
    "        python_model=CLIPMultimodalEncoder(),\n",
    "        pip_requirements=[\"transformers>=4.30.0\", \"torch>=2.0.0\", \"pillow>=10.0.0\"],\n",
    "        registered_model_name=\"main.fashion_demo.clip_multimodal_encoder\",\n",
    "        signature=infer_signature(text_test, text_emb),\n",
    "        input_example=text_test\n",
    "    )\n",
    "    print(f\"‚úÖ Registered: main.fashion_demo.clip_multimodal_encoder\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ecfe1016-2f1e-431c-975b-56739f1d5fd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Cell 27: Generate Text Embeddings\n",
    "\n",
    "**After endpoint is READY**, run this to generate text embeddings:\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import pandas_udf, col, concat_ws\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "import pandas as pd\n",
    "\n",
    "# Create rich text descriptions\n",
    "products_df = spark.table(\"main.fashion_demo.products\")\n",
    "\n",
    "text_descriptions = products_df.withColumn(\n",
    "    \"text_content\",\n",
    "    concat_ws(\" \",\n",
    "        col(\"product_display_name\"),\n",
    "        col(\"article_type\"),\n",
    "        col(\"base_color\"),\n",
    "        col(\"master_category\"),\n",
    "        col(\"gender\"),\n",
    "        col(\"season\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Define text embedding UDF\n",
    "ENDPOINT_URL = \"https://your-workspace/serving-endpoints/clip-multimodal-encoder/invocations\"\n",
    "TOKEN = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "\n",
    "@pandas_udf(ArrayType(DoubleType()))\n",
    "def generate_text_embedding_udf(texts: pd.Series) -> pd.Series:\n",
    "    import requests\n",
    "    import numpy as np\n",
    "    \n",
    "    def encode_text(text):\n",
    "        if pd.isna(text) or not text:\n",
    "            return np.zeros(512).tolist()\n",
    "        \n",
    "        payload = {\"dataframe_records\": [{\"text\": text}]}\n",
    "        headers = {\"Authorization\": f\"Bearer {TOKEN}\", \"Content-Type\": \"application/json\"}\n",
    "        \n",
    "        response = requests.post(ENDPOINT_URL, headers=headers, json=payload, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        result = response.json()\n",
    "        embedding = result[\"predictions\"][0] if \"predictions\" in result else result\n",
    "        return embedding\n",
    "    \n",
    "    return texts.apply(encode_text)\n",
    "\n",
    "# Generate text embeddings\n",
    "print(\"Generating text embeddings for 44K products...\")\n",
    "text_embeddings_df = text_descriptions.withColumn(\n",
    "    \"text_embedding\",\n",
    "    generate_text_embedding_udf(col(\"text_content\"))\n",
    ")\n",
    "\n",
    "# Save to table\n",
    "text_embeddings_df.select(\n",
    "    \"product_id\",\n",
    "    \"text_content\",\n",
    "    \"text_embedding\"\n",
    ").write.mode(\"overwrite\").saveAsTable(\"main.fashion_demo.product_text_embeddings\")\n",
    "\n",
    "print(\"‚úÖ Text embeddings generated and saved!\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cfad20e7-76fd-44e8-84e3-793b03c21082",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Cell 28: Create Multimodal Table\n",
    "\n",
    "```sql\n",
    "CREATE OR REPLACE TABLE main.fashion_demo.product_embeddings_multimodal AS\n",
    "SELECT \n",
    "  p.product_id,\n",
    "  p.product_display_name,\n",
    "  p.master_category,\n",
    "  p.sub_category,\n",
    "  p.article_type,\n",
    "  p.base_color,\n",
    "  p.price,\n",
    "  p.image_path,\n",
    "  p.gender,\n",
    "  p.season,\n",
    "  p.year,\n",
    "  p.usage,\n",
    "  img.image_embedding,\n",
    "  txt.text_embedding,\n",
    "  'clip-vit-b-32' as embedding_model,\n",
    "  512 as embedding_dimension,\n",
    "  CURRENT_TIMESTAMP() as updated_at\n",
    "FROM main.fashion_demo.products p\n",
    "INNER JOIN main.fashion_demo.product_image_embeddings img ON p.product_id = img.product_id\n",
    "INNER JOIN main.fashion_demo.product_text_embeddings txt ON p.product_id = txt.product_id\n",
    "WHERE img.image_embedding IS NOT NULL \n",
    "  AND txt.text_embedding IS NOT NULL;\n",
    "\n",
    "-- Verify\n",
    "SELECT \n",
    "  COUNT(*) as total,\n",
    "  COUNT(image_embedding) as has_image,\n",
    "  COUNT(text_embedding) as has_text,\n",
    "  AVG(SIZE(image_embedding)) as img_dim,\n",
    "  AVG(SIZE(text_embedding)) as txt_dim\n",
    "FROM main.fashion_demo.product_embeddings_multimodal;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "613a38ba-5a05-49fa-ae0e-30bb179a2a47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Cell 29: Create Hybrid Embeddings\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "import numpy as np\n",
    "\n",
    "@udf(ArrayType(DoubleType()))\n",
    "def create_hybrid_embedding(image_emb, text_emb):\n",
    "    \"\"\"Combine image + text embeddings (50/50 weight)\"\"\"\n",
    "    if image_emb is None or text_emb is None:\n",
    "        return None\n",
    "    \n",
    "    img_arr = np.array(image_emb)\n",
    "    txt_arr = np.array(text_emb)\n",
    "    \n",
    "    # 50% image + 50% text\n",
    "    hybrid = 0.5 * img_arr + 0.5 * txt_arr\n",
    "    \n",
    "    # Normalize\n",
    "    hybrid = hybrid / (np.linalg.norm(hybrid) + 1e-8)\n",
    "    \n",
    "    return hybrid.tolist()\n",
    "\n",
    "# Add hybrid embedding column\n",
    "multimodal_df = spark.table(\"main.fashion_demo.product_embeddings_multimodal\")\n",
    "\n",
    "hybrid_df = multimodal_df.withColumn(\n",
    "    \"hybrid_embedding\",\n",
    "    create_hybrid_embedding(col(\"image_embedding\"), col(\"text_embedding\"))\n",
    ")\n",
    "\n",
    "# Save back\n",
    "hybrid_df.write.mode(\"overwrite\").saveAsTable(\"main.fashion_demo.product_embeddings_multimodal\")\n",
    "\n",
    "print(\"‚úÖ Hybrid embeddings created!\")\n",
    "print(\"   - 50% visual features (from images)\")\n",
    "print(\"   - 50% semantic features (from text)\")\n",
    "print(\"   - Normalized to unit vectors\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "443f89c1-58d2-43fd-add5-c758ceab9ccc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Cell 30: Create Vector Search Indexes\n",
    "\n",
    "## Go to Databricks UI: Compute ‚Üí Vector Search ‚Üí fashion_vector_search\n",
    "\n",
    "### Index 1: Image Search\n",
    "- **Name**: `main.fashion_demo.vs_image_search`\n",
    "- **Source**: `main.fashion_demo.product_embeddings_multimodal`\n",
    "- **Primary Key**: `product_id`\n",
    "- **Embedding Column**: `image_embedding`\n",
    "- **Dimension**: 512\n",
    "- **Sync**: Triggered\n",
    "\n",
    "### Index 2: Text Search  \n",
    "- **Name**: `main.fashion_demo.vs_text_search`\n",
    "- **Source**: `main.fashion_demo.product_embeddings_multimodal`\n",
    "- **Primary Key**: `product_id`\n",
    "- **Embedding Column**: `text_embedding`\n",
    "- **Dimension**: 512\n",
    "- **Sync**: Triggered\n",
    "\n",
    "### Index 3: Hybrid Search\n",
    "- **Name**: `main.fashion_demo.vs_hybrid_search`\n",
    "- **Source**: `main.fashion_demo.product_embeddings_multimodal`\n",
    "- **Primary Key**: `product_id`\n",
    "- **Embedding Column**: `hybrid_embedding`\n",
    "- **Dimension**: 512\n",
    "- **Sync**: Triggered\n",
    "\n",
    "**Wait 5-10 minutes for all indexes to sync!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee341726-e156-4740-b949-46ac72a01d4f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 31: Updated services/clip_service.py - MULTIMODAL"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "services/clip_service.py - MULTIMODAL VERSION\n",
    "Copy this to your repo!\n",
    "\"\"\"\n",
    "import base64\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class CLIPMultimodalService:\n",
    "    \"\"\"CLIP service supporting text AND images in shared 512-dim space\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.endpoint_name = \"clip-multimodal-encoder\"\n",
    "        self.workspace_host = os.getenv(\"DATABRICKS_HOST\", \"\")\n",
    "        if not self.workspace_host.startswith(\"http\"):\n",
    "            self.workspace_host = f\"https://{self.workspace_host}\"\n",
    "        self.embedding_dim = 512\n",
    "        logger.info(f\"üöÄ CLIPMultimodalService: {self.endpoint_name}\")\n",
    "    \n",
    "    def _get_endpoint_url(self) -> str:\n",
    "        return f\"{self.workspace_host}/serving-endpoints/{self.endpoint_name}/invocations\"\n",
    "    \n",
    "    def _get_auth_headers(self) -> dict:\n",
    "        from databricks.sdk import WorkspaceClient\n",
    "        w = WorkspaceClient()\n",
    "        token = w.config.oauth_token().access_token\n",
    "        return {\"Authorization\": f\"Bearer {token}\", \"Content-Type\": \"application/json\"}\n",
    "    \n",
    "    async def get_text_embedding(self, text: str) -> np.ndarray:\n",
    "        \"\"\"Generate CLIP text embedding (512 dims)\"\"\"\n",
    "        import aiohttp\n",
    "        \n",
    "        payload = {\"dataframe_records\": [{\"text\": text}]}\n",
    "        logger.info(f\"Encoding text: '{text[:50]}...'\")\n",
    "        \n",
    "        timeout = aiohttp.ClientTimeout(total=30)\n",
    "        async with aiohttp.ClientSession(timeout=timeout) as session:\n",
    "            async with session.post(self._get_endpoint_url(), json=payload, headers=self._get_auth_headers()) as response:\n",
    "                if response.status != 200:\n",
    "                    raise Exception(f\"CLIP error {response.status}: {await response.text()}\")\n",
    "                result = await response.json()\n",
    "        \n",
    "        embedding = np.array(result[\"predictions\"][0] if \"predictions\" in result else result, dtype=np.float32)\n",
    "        embedding = embedding / (np.linalg.norm(embedding) + 1e-8)\n",
    "        logger.info(f\"‚úÖ Text embedding: shape={embedding.shape}\")\n",
    "        return embedding\n",
    "    \n",
    "    async def get_image_embedding(self, image_bytes: bytes) -> np.ndarray:\n",
    "        \"\"\"Generate CLIP image embedding (512 dims)\"\"\"\n",
    "        import aiohttp\n",
    "        \n",
    "        image_b64 = base64.b64encode(image_bytes).decode(\"utf-8\")\n",
    "        payload = {\"dataframe_records\": [{\"image\": image_b64}]}\n",
    "        \n",
    "        timeout = aiohttp.ClientTimeout(total=30)\n",
    "        async with aiohttp.ClientSession(timeout=timeout) as session:\n",
    "            async with session.post(self._get_endpoint_url(), json=payload, headers=self._get_auth_headers()) as response:\n",
    "                if response.status != 200:\n",
    "                    raise Exception(f\"CLIP error {response.status}\")\n",
    "                result = await response.json()\n",
    "        \n",
    "        embedding = np.array(result[\"predictions\"][0] if \"predictions\" in result else result, dtype=np.float32)\n",
    "        embedding = embedding / (np.linalg.norm(embedding) + 1e-8)\n",
    "        logger.info(f\"‚úÖ Image embedding: shape={embedding.shape}\")\n",
    "        return embedding\n",
    "    \n",
    "    async def get_hybrid_embedding(self, text: str, image_bytes: bytes, text_weight: float = 0.5) -> np.ndarray:\n",
    "        \"\"\"Generate hybrid embedding from text + image\"\"\"\n",
    "        text_emb = await self.get_text_embedding(text)\n",
    "        image_emb = await self.get_image_embedding(image_bytes)\n",
    "        hybrid = text_weight * text_emb + (1 - text_weight) * image_emb\n",
    "        hybrid = hybrid / (np.linalg.norm(hybrid) + 1e-8)\n",
    "        logger.info(f\"‚úÖ Hybrid: text_weight={text_weight}\")\n",
    "        return hybrid\n",
    "\n",
    "clip_service = CLIPMultimodalService()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7c863d0-741f-422d-aae8-f47d8beb15a4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 32: Updated routes/v1/search.py - CROSS-MODAL"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "routes/v1/search.py - MULTIMODAL VERSION\n",
    "Copy this to your repo!\n",
    "\"\"\"\n",
    "from fastapi import APIRouter, HTTPException, UploadFile, File, Form, Depends\n",
    "from sqlalchemy.ext.asyncio import AsyncSession\n",
    "from typing import Optional\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "router = APIRouter(prefix=\"/search\", tags=[\"search\"])\n",
    "\n",
    "@router.post(\"/text\")\n",
    "async def search_by_text_semantic(request: SearchRequest, db: AsyncSession = Depends(get_async_db)):\n",
    "    \"\"\"\n",
    "    üéØ SEMANTIC TEXT SEARCH - Cross-modal magic!\n",
    "    Text query finds visually matching products!\n",
    "    \"\"\"\n",
    "    from services.clip_service import clip_service\n",
    "    from services.vector_search_service import vector_search_service\n",
    "    \n",
    "    logger.info(f\"Semantic text search: '{request.query}'\")\n",
    "    \n",
    "    # Generate text embedding\n",
    "    text_embedding = await clip_service.get_text_embedding(request.query)\n",
    "    \n",
    "    # Search in IMAGE index (cross-modal!)\n",
    "    products_data = await vector_search_service.similarity_search(\n",
    "        query_vector=text_embedding,\n",
    "        num_results=request.limit,\n",
    "        index_name=\"main.fashion_demo.vs_image_search\"  # Text ‚Üí Image!\n",
    "    )\n",
    "    \n",
    "    products = []\n",
    "    for p in products_data:\n",
    "        product = ProductDetail(**p)\n",
    "        product.image_url = get_image_url(int(product.product_id))\n",
    "        product.similarity_score = p.get(\"score\", 0.85)\n",
    "        products.append(product)\n",
    "    \n",
    "    logger.info(f\"‚úÖ Found {len(products)} products for '{request.query}'\")\n",
    "    \n",
    "    return SearchResponse(\n",
    "        products=products,\n",
    "        query=request.query,\n",
    "        search_type=\"semantic_text\",\n",
    "        user_id=request.user_id\n",
    "    )\n",
    "\n",
    "@router.post(\"/image\")\n",
    "async def search_by_image(image: UploadFile = File(...), limit: int = Form(20), db: AsyncSession = Depends(get_async_db)):\n",
    "    \"\"\"\n",
    "    üñºÔ∏è IMAGE SEARCH - Visual similarity\n",
    "    \"\"\"\n",
    "    from services.clip_service import clip_service\n",
    "    from services.vector_search_service import vector_search_service\n",
    "    \n",
    "    image_bytes = await image.read()\n",
    "    image_embedding = await clip_service.get_image_embedding(image_bytes)\n",
    "    \n",
    "    products_data = await vector_search_service.similarity_search(\n",
    "        query_vector=image_embedding,\n",
    "        num_results=limit,\n",
    "        index_name=\"main.fashion_demo.vs_image_search\"\n",
    "    )\n",
    "    \n",
    "    products = [ProductDetail(**p) for p in products_data]\n",
    "    for p in products:\n",
    "        p.image_url = get_image_url(int(p.product_id))\n",
    "    \n",
    "    return SearchResponse(products=products, search_type=\"image\")\n",
    "\n",
    "@router.post(\"/hybrid\")\n",
    "async def search_hybrid(\n",
    "    query: str = Form(...),\n",
    "    image: Optional[UploadFile] = File(None),\n",
    "    text_weight: float = Form(0.5),\n",
    "    db: AsyncSession = Depends(get_async_db)\n",
    "):\n",
    "    \"\"\"\n",
    "    ‚ö° HYBRID SEARCH - Text + Image combined!\n",
    "    \"\"\"\n",
    "    from services.clip_service import clip_service\n",
    "    from services.vector_search_service import vector_search_service\n",
    "    \n",
    "    if image:\n",
    "        image_bytes = await image.read()\n",
    "        hybrid_embedding = await clip_service.get_hybrid_embedding(query, image_bytes, text_weight)\n",
    "    else:\n",
    "        hybrid_embedding = await clip_service.get_text_embedding(query)\n",
    "    \n",
    "    products_data = await vector_search_service.similarity_search(\n",
    "        query_vector=hybrid_embedding,\n",
    "        num_results=20,\n",
    "        index_name=\"main.fashion_demo.vs_hybrid_search\"\n",
    "    )\n",
    "    \n",
    "    products = [ProductDetail(**p) for p in products_data]\n",
    "    for p in products:\n",
    "        p.image_url = get_image_url(int(p.product_id))\n",
    "    \n",
    "    return SearchResponse(products=products, query=query, search_type=\"hybrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12ab45f7-43ba-40db-8416-65aa648941f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üéâ Final Multimodal Architecture\n",
    "\n",
    "## üìä Data Layer\n",
    "```\n",
    "main.fashion_demo.product_embeddings_multimodal (44K products)\n",
    "‚îú‚îÄ‚îÄ product_id, name, category, color, price, etc.\n",
    "‚îú‚îÄ‚îÄ image_embedding (512)    ‚Üê Visual features\n",
    "‚îú‚îÄ‚îÄ text_embedding (512)     ‚Üê Semantic features\n",
    "‚îî‚îÄ‚îÄ hybrid_embedding (512)   ‚Üê Combined\n",
    "     ‚îî‚îÄ‚îÄ ALL IN SAME SPACE! ‚ú®\n",
    "```\n",
    "\n",
    "## üîç Search Capabilities\n",
    "\n",
    "### 1. **Semantic Text Search** üéØ\n",
    "```bash\n",
    "POST /api/v1/search/text\n",
    "{\"query\": \"vintage leather jacket\"}\n",
    "\n",
    "# Returns: Products that LOOK vintage and leather\n",
    "# Even if \"vintage\" isn't in the name!\n",
    "```\n",
    "\n",
    "### 2. **Visual Image Search** üñºÔ∏è\n",
    "```bash\n",
    "POST /api/v1/search/image\n",
    "[Upload photo of dress]\n",
    "\n",
    "# Returns: Visually similar dresses\n",
    "```\n",
    "\n",
    "### 3. **Hybrid Search** ‚ö°\n",
    "```bash\n",
    "POST /api/v1/search/hybrid\n",
    "query=\"red dress\"\n",
    "image=[inspiration photo]\n",
    "text_weight=0.5\n",
    "\n",
    "# Returns: Red dresses that look like the photo!\n",
    "```\n",
    "\n",
    "### 4. **Cross-Modal Magic** ‚ú®\n",
    "- Text \"black leather jacket\" ‚Üí Searches IMAGE index\n",
    "- Finds products that LOOK like black leather jackets\n",
    "- No keywords needed!\n",
    "\n",
    "## üî¨ Advanced Features\n",
    "\n",
    "**Latent Feature Analysis:**\n",
    "- Discover which dimensions encode color, style, formality\n",
    "- Interpretable AI\n",
    "\n",
    "**Zero-Shot Classification:**\n",
    "- \"Is this formal wear?\" ‚Üí Compare with text embedding\n",
    "- No training needed\n",
    "\n",
    "**Style Understanding:**\n",
    "- Upload image ‚Üí \"This is casual summer wear\"\n",
    "- Automatic style detection\n",
    "\n",
    "## üìä Benefits\n",
    "\n",
    "| Feature | Before | After |\n",
    "|---------|--------|-------|\n",
    "| Text search | Keywords only | Semantic understanding |\n",
    "| Image search | ‚úÖ Works | ‚úÖ Works |\n",
    "| Cross-modal | ‚ùå No | ‚úÖ Yes! |\n",
    "| Hybrid queries | ‚ùå No | ‚úÖ Yes! |\n",
    "| Latent features | Limited | Rich |\n",
    "| Zero-shot tasks | ‚ùå No | ‚úÖ Yes! |\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ This is Production-Grade Multimodal Search!\n",
    "\n",
    "**Next-level features:**\n",
    "- Amazon-style semantic search\n",
    "- Pinterest-style visual search  \n",
    "- Google Lens-style cross-modal search\n",
    "- All powered by CLIP's shared embedding space!\n",
    "\n",
    "**Ready to implement? Follow cells 26-32!** üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27a31eb9-276b-4580-81ba-261a1751fa9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Multimodal CLIP Implementation Summary\n",
    "## Fashion E-Commerce Visual Search System\n",
    "\n",
    "**Date:** 2025-12-09  \n",
    "**Status:** Ready for multimodal implementation  \n",
    "**Estimated Time:** 4-6 hours  \n",
    "**Estimated Cost:** $10-15\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Project Goal\n",
    "\n",
    "Implement comprehensive multimodal search using CLIP's shared text-image embedding space for:\n",
    "* **Semantic text search**: \"vintage leather jacket\" finds matching images (no keywords!)\n",
    "* **Visual image search**: Upload photo, find similar products\n",
    "* **Hybrid search**: Text + image combined queries\n",
    "* **Cross-modal search**: Text query searches image embeddings directly\n",
    "* **Latent feature extraction**: Discover what embedding dimensions represent\n",
    "* **Personalized recommendations**: User embeddings in shared space\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Current State - What Exists\n",
    "\n",
    "### ‚úÖ Data Assets (All in `main.fashion_demo`)\n",
    "\n",
    "**1. `products` table** (44,424 products)\n",
    "```sql\n",
    "Columns: product_id, product_display_name, master_category, sub_category, \n",
    "         article_type, base_color, price, image_path, gender, season, year, usage\n",
    "Categories: Apparel (21K), Accessories (11K), Footwear (9K), Personal Care (2K)\n",
    "Price Range: $0 - $299.95\n",
    "```\n",
    "\n",
    "**2. `product_image_embeddings` table** (44,424 rows)\n",
    "```sql\n",
    "Columns: product_id, image_embedding (512 dims), embedding_model, \n",
    "         embedding_dimension, created_at\n",
    "Model: clip-vit-b-32 (IMAGE ONLY - from product photos)\n",
    "Source: /Volumes/main/fashion_demo/raw_data/images/\n",
    "```\n",
    "\n",
    "**3. `product_embeddings_enriched` table** (44,424 rows) ‚úÖ CREATED TODAY\n",
    "```sql\n",
    "All product metadata + image_embedding in one table\n",
    "Ready for Vector Search index\n",
    "```\n",
    "\n",
    "**4. `user_style_features` table** (5 users)\n",
    "```sql\n",
    "Columns: user_id, segment, user_embedding (512 dims), color_prefs, \n",
    "         category_prefs, price ranges, num_interactions\n",
    "Users: user_006327, user_007598, user_008828, user_001328, user_009809\n",
    "```\n",
    "\n",
    "### ‚úÖ Model Serving Endpoints\n",
    "\n",
    "**1. `clip-image-encoder`** (DEPLOYED, READY)\n",
    "* Model: `main.fashion_demo.clip_image_encoder` v1\n",
    "* Input: `{\"dataframe_records\": [{\"image\": \"base64...\"}]}`\n",
    "* Output: `{\"predictions\": [0.012, 0.013, ...]}` (512 floats)\n",
    "* Limitation: **IMAGE ONLY** - no text support\n",
    "* Status: Working perfectly (logs confirm)\n",
    "\n",
    "### ‚úÖ Vector Search Infrastructure\n",
    "\n",
    "**Endpoint:** `fashion_vector_search`\n",
    "* ID: `4d329fc8-1924-4131-ace8-14b542f8c14b`\n",
    "* Status: ONLINE\n",
    "\n",
    "**Current Index:** `main.fashion_demo.product_embeddings_index`\n",
    "* ‚ùå **Problem**: Built on `product_image_embeddings` (only has product_id + embedding)\n",
    "* ‚ùå **Missing**: All product metadata columns\n",
    "* ‚ùå **Error**: \"Requested columns not present in index: sub_category, product_display_name, usage, price...\"\n",
    "\n",
    "### ‚úÖ Application Code (fashion-ecom-site repo)\n",
    "\n",
    "**Location:** `/Users/kevin.ippen@databricks.com/fashion-ecom-site/`\n",
    "\n",
    "**Structure:**\n",
    "```\n",
    "fashion-ecom-site/\n",
    "‚îú‚îÄ‚îÄ app.py                    # FastAPI app\n",
    "‚îú‚îÄ‚îÄ core/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ config.py            # Settings\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ database.py          # Lakebase connection\n",
    "‚îú‚îÄ‚îÄ services/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ clip_service.py      # CLIP endpoint client (image only)\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ vector_search_service.py  # Vector Search client\n",
    "‚îú‚îÄ‚îÄ routes/v1/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ search.py            # Search endpoints\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ users.py             # User/persona endpoints\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ products.py          # Product endpoints\n",
    "‚îú‚îÄ‚îÄ data/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ personas.json        # User personas\n",
    "‚îî‚îÄ‚îÄ frontend/\n",
    "    ‚îú‚îÄ‚îÄ src/                 # React/TypeScript source\n",
    "    ‚îî‚îÄ‚îÄ dist/                # Built frontend\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üö® Critical Issues Fixed Today\n",
    "\n",
    "### Issue 1: Vector Search Index Missing Columns ‚úÖ DIAGNOSED\n",
    "**Error:**\n",
    "```\n",
    "Requested columns to fetch are not present in index: \n",
    "sub_category, product_display_name, usage, price, year, \n",
    "season, image_path, base_color, article_type, master_category, gender\n",
    "```\n",
    "\n",
    "**Root Cause:** Index built on `product_image_embeddings` which only has:\n",
    "* product_id, image_embedding, embedding_model, embedding_dimension, created_at\n",
    "\n",
    "**Solution:** Rebuild index on `product_embeddings_enriched` table (has all fields)\n",
    "\n",
    "### Issue 2: get_index() Method Call ‚úÖ FIXED\n",
    "**Error:** \"Index name must be specified\"\n",
    "\n",
    "**Root Cause:** \n",
    "```python\n",
    "# Wrong (positional arg):\n",
    "self._index = client.get_index(self.index_name)\n",
    "\n",
    "# Correct (keyword arg):\n",
    "self._index = client.get_index(index_name=self.index_name)\n",
    "```\n",
    "\n",
    "**Fix:** Use keyword argument (method signature requires it)\n",
    "\n",
    "### Issue 3: OAuth Authentication ‚úÖ FIXED\n",
    "**Error:** \"Please specify either personal access token or service principal\"\n",
    "\n",
    "**Fix:**\n",
    "```python\n",
    "w = WorkspaceClient()\n",
    "token = w.config.oauth_token().access_token\n",
    "\n",
    "vsc = VectorSearchClient(\n",
    "    workspace_url=self.workspace_host,\n",
    "    personal_access_token=token,  # ‚Üê This was missing!\n",
    "    disable_notice=True\n",
    ")\n",
    "```\n",
    "\n",
    "### Issue 4: CLIP Response Parsing ‚úÖ FIXED\n",
    "**Issue:** Expected nested array, got flat array\n",
    "\n",
    "**Actual Response:** `{\"predictions\": [0.012, 0.013, -0.007, ...]}`\n",
    "\n",
    "**Fix:** Handle flat array format correctly\n",
    "\n",
    "### Issue 5: User IDs Mismatch ‚úÖ FIXED\n",
    "**Issue:** Frontend requested `user_005` (fake user), but personas.json has real users\n",
    "\n",
    "**Fix:** Updated personas.json with real user IDs that have embeddings:\n",
    "* user_006327, user_007598, user_008828, user_001328, user_009809\n",
    "\n",
    "### Issue 6: Invalid Filter Syntax ‚úÖ DIAGNOSED\n",
    "**Error:** \"Invalid operator used in filter: price <= \"\n",
    "\n",
    "**Issue:** Wrong format `{\"price >= \": 50, \"price <= \": 100}`\n",
    "\n",
    "**Note:** Filters won't work until index is rebuilt on enriched table anyway\n",
    "\n",
    "---\n",
    "\n",
    "## üèóÔ∏è Target Architecture - Multimodal CLIP\n",
    "\n",
    "### Phase 1: Enhanced Single-Modal (Quick Fix)\n",
    "\n",
    "**Goal:** Get app working with current image embeddings\n",
    "\n",
    "**New Table:** `main.fashion_demo.product_embeddings_enriched` ‚úÖ EXISTS\n",
    "```sql\n",
    "Columns:\n",
    "  -- All product metadata\n",
    "  product_id, product_display_name, master_category, sub_category,\n",
    "  article_type, base_color, price, image_path, gender, season, year, usage,\n",
    "  \n",
    "  -- Image embedding\n",
    "  image_embedding ARRAY<DOUBLE>,  -- 512 dims from CLIP image encoder\n",
    "  \n",
    "  -- Metadata\n",
    "  embedding_model, embedding_dimension, updated_at\n",
    "\n",
    "Row Count: 44,424\n",
    "```\n",
    "\n",
    "**New Vector Search Index:** `main.fashion_demo.product_embeddings_enriched_index`\n",
    "* Source: product_embeddings_enriched\n",
    "* Embedding Column: image_embedding\n",
    "* Primary Key: product_id\n",
    "* Dimension: 512\n",
    "* **Benefit:** Returns ALL product fields (no joins needed!)\n",
    "\n",
    "**App Changes:**\n",
    "* Update `vector_search_service.py`: Use new index name\n",
    "* Update `search.py`: Remove join logic\n",
    "* Update `personas.json`: Use real user IDs\n",
    "* Redeploy\n",
    "\n",
    "**Time:** 1 hour | **Result:** Image search + recommendations work\n",
    "\n",
    "---\n",
    "\n",
    "### Phase 2: Full Multimodal (Comprehensive Solution)\n",
    "\n",
    "**Goal:** Leverage CLIP's shared text-image embedding space\n",
    "\n",
    "#### New Model Endpoint: `clip-multimodal-encoder`\n",
    "\n",
    "**Capabilities:**\n",
    "```python\n",
    "# Text encoding\n",
    "Input:  {\"dataframe_records\": [{\"text\": \"red summer dress\"}]}\n",
    "Output: {\"predictions\": [0.15, -0.23, ...]}  # 512 dims\n",
    "\n",
    "# Image encoding  \n",
    "Input:  {\"dataframe_records\": [{\"image\": \"base64...\"}]}\n",
    "Output: {\"predictions\": [0.12, -0.21, ...]}  # 512 dims\n",
    "\n",
    "# SAME EMBEDDING SPACE! Text and image embeddings are directly comparable!\n",
    "```\n",
    "\n",
    "**Deployment:**\n",
    "```python\n",
    "class CLIPMultimodalEncoder(mlflow.pyfunc.PythonModel):\n",
    "    def load_context(self, context):\n",
    "        from transformers import CLIPProcessor, CLIPModel\n",
    "        self.model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    \n",
    "    def predict(self, context, model_input):\n",
    "        if \"text\" in model_input:\n",
    "            inputs = self.processor(text=model_input[\"text\"], return_tensors=\"pt\")\n",
    "            features = self.model.get_text_features(**inputs)\n",
    "        elif \"image\" in model_input:\n",
    "            image = decode_base64(model_input[\"image\"])\n",
    "            inputs = self.processor(images=image, return_tensors=\"pt\")\n",
    "            features = self.model.get_image_features(**inputs)\n",
    "        \n",
    "        # Normalize to unit vector (critical!)\n",
    "        features = features / features.norm(dim=-1, keepdim=True)\n",
    "        return features.cpu().numpy()[0].tolist()\n",
    "```\n",
    "\n",
    "#### New Table: `main.fashion_demo.product_embeddings_multimodal`\n",
    "\n",
    "```sql\n",
    "CREATE TABLE main.fashion_demo.product_embeddings_multimodal (\n",
    "  -- Product metadata (all fields from products table)\n",
    "  product_id INT,\n",
    "  product_display_name STRING,\n",
    "  master_category STRING,\n",
    "  sub_category STRING,\n",
    "  article_type STRING,\n",
    "  base_color STRING,\n",
    "  price DOUBLE,\n",
    "  image_path STRING,\n",
    "  gender STRING,\n",
    "  season STRING,\n",
    "  year INT,\n",
    "  usage STRING,\n",
    "  \n",
    "  -- Three embedding types (all 512 dims, same CLIP space!)\n",
    "  image_embedding ARRAY<DOUBLE>,      -- From CLIP image encoder\n",
    "  text_embedding ARRAY<DOUBLE>,       -- From CLIP text encoder (NEW!)\n",
    "  hybrid_embedding ARRAY<DOUBLE>,     -- 0.5*text + 0.5*image (NEW!)\n",
    "  \n",
    "  -- Metadata\n",
    "  embedding_model STRING,             -- \"clip-vit-b-32\"\n",
    "  embedding_dimension INT,            -- 512\n",
    "  updated_at TIMESTAMP\n",
    ");\n",
    "```\n",
    "\n",
    "**Text Content Generation:**\n",
    "```sql\n",
    "-- Rich text descriptions for embedding\n",
    "CONCAT_WS(' ',\n",
    "  product_display_name,           -- \"Nike Air Max 90\"\n",
    "  article_type,                   -- \"Shoes\"\n",
    "  base_color,                     -- \"White\"\n",
    "  master_category,                -- \"Footwear\"\n",
    "  gender,                         -- \"Men\"\n",
    "  season,                         -- \"All Season\"\n",
    "  CASE \n",
    "    WHEN price < 30 THEN 'affordable budget friendly'\n",
    "    WHEN price < 70 THEN 'mid-range value'\n",
    "    WHEN price < 120 THEN 'premium quality'\n",
    "    ELSE 'luxury high-end designer'\n",
    "  END\n",
    ") as text_content\n",
    "\n",
    "-- Result: \"Nike Air Max 90 Shoes White Footwear Men All Season mid-range value\"\n",
    "```\n",
    "\n",
    "**Hybrid Embedding Computation:**\n",
    "```python\n",
    "@udf(ArrayType(DoubleType()))\n",
    "def create_hybrid_embedding(image_emb, text_emb):\n",
    "    img_arr = np.array(image_emb)\n",
    "    txt_arr = np.array(text_emb)\n",
    "    \n",
    "    # Weighted combination (50/50)\n",
    "    hybrid = 0.5 * img_arr + 0.5 * txt_arr\n",
    "    \n",
    "    # CRITICAL: Normalize to unit vector!\n",
    "    hybrid = hybrid / (np.linalg.norm(hybrid) + 1e-8)\n",
    "    \n",
    "    return hybrid.tolist()\n",
    "```\n",
    "\n",
    "#### Three Vector Search Indexes\n",
    "\n",
    "**All built on:** `main.fashion_demo.product_embeddings_multimodal`\n",
    "\n",
    "**Index 1: Image Search** (`vs_image_search`)\n",
    "* Embedding Column: `image_embedding`\n",
    "* Use Case: Visual similarity (upload photo ‚Üí find similar)\n",
    "* Query Type: Image ‚Üí Image\n",
    "\n",
    "**Index 2: Text Search** (`vs_text_search`)\n",
    "* Embedding Column: `text_embedding`\n",
    "* Use Case: Semantic text search (\"red dress\" ‚Üí semantically similar)\n",
    "* Query Type: Text ‚Üí Text\n",
    "\n",
    "**Index 3: Hybrid Search** (`vs_hybrid_search`) ‚≠ê PRIMARY\n",
    "* Embedding Column: `hybrid_embedding`\n",
    "* Use Case: Cross-modal + hybrid queries\n",
    "* Query Types:\n",
    "  * Text ‚Üí Image (cross-modal!)\n",
    "  * Image ‚Üí Text (cross-modal!)\n",
    "  * Text + Image ‚Üí Hybrid\n",
    "  * User embedding ‚Üí Products\n",
    "\n",
    "**All indexes return:** Complete product metadata (no joins!)\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Implementation Steps\n",
    "\n",
    "### Phase 1: Deploy CLIP Multimodal Encoder (2-3 hours)\n",
    "\n",
    "**Step 1.1:** Create MLflow model wrapper\n",
    "```python\n",
    "# See notebook cells for full implementation\n",
    "class CLIPMultimodalEncoder(mlflow.pyfunc.PythonModel):\n",
    "    # Handles both text and image inputs\n",
    "    # Returns 512-dim embeddings in shared space\n",
    "```\n",
    "\n",
    "**Step 1.2:** Register to Unity Catalog\n",
    "```python\n",
    "with mlflow.start_run():\n",
    "    mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"clip_model\",\n",
    "        python_model=CLIPMultimodalEncoder(),\n",
    "        pip_requirements=[\n",
    "            \"transformers>=4.30.0\",\n",
    "            \"torch>=2.0.0\",\n",
    "            \"pillow>=10.0.0\"\n",
    "        ],\n",
    "        registered_model_name=\"main.fashion_demo.clip_multimodal_encoder\",\n",
    "        signature=signature,\n",
    "        input_example=input_example\n",
    "    )\n",
    "```\n",
    "\n",
    "**Step 1.3:** Create Model Serving endpoint via UI\n",
    "* Go to: Models ‚Üí main.fashion_demo.clip_multimodal_encoder\n",
    "* Click: \"Use model for inference\" ‚Üí \"Real-time\"\n",
    "* Name: `clip-multimodal-encoder`\n",
    "* Workload: Small (or Medium for production)\n",
    "* Scale to zero: Enabled\n",
    "* Wait: 10-15 minutes for deployment\n",
    "\n",
    "**Step 1.4:** Test endpoint\n",
    "```python\n",
    "# Test text encoding\n",
    "response = requests.post(\n",
    "    endpoint_url,\n",
    "    json={\"dataframe_records\": [{\"text\": \"red dress\"}]},\n",
    "    headers=headers\n",
    ")\n",
    "text_emb = response.json()[\"predictions\"]  # 512 floats\n",
    "\n",
    "# Test image encoding\n",
    "response = requests.post(\n",
    "    endpoint_url,\n",
    "    json={\"dataframe_records\": [{\"image\": base64_image}]},\n",
    "    headers=headers\n",
    ")\n",
    "image_emb = response.json()[\"predictions\"]  # 512 floats\n",
    "\n",
    "# Verify they're comparable\n",
    "cosine_sim = np.dot(text_emb, image_emb)  # Should be meaningful!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Phase 2: Generate Text Embeddings (1-2 hours)\n",
    "\n",
    "**Step 2.1:** Create text descriptions view\n",
    "```sql\n",
    "CREATE OR REPLACE TEMP VIEW product_text_descriptions AS\n",
    "SELECT \n",
    "  product_id,\n",
    "  CONCAT_WS(' ',\n",
    "    product_display_name,\n",
    "    article_type,\n",
    "    base_color,\n",
    "    master_category,\n",
    "    sub_category,\n",
    "    gender,\n",
    "    season,\n",
    "    usage,\n",
    "    CASE \n",
    "      WHEN price < 30 THEN 'affordable budget friendly'\n",
    "      WHEN price < 70 THEN 'mid-range value'\n",
    "      WHEN price < 120 THEN 'premium quality'\n",
    "      ELSE 'luxury high-end designer'\n",
    "    END\n",
    "  ) as text_content\n",
    "FROM main.fashion_demo.products\n",
    "WHERE product_display_name IS NOT NULL;\n",
    "```\n",
    "\n",
    "**Step 2.2:** Create pandas UDF for text embedding\n",
    "```python\n",
    "@pandas_udf(ArrayType(DoubleType()))\n",
    "def generate_text_embedding_udf(texts: pd.Series) -> pd.Series:\n",
    "    import requests\n",
    "    import numpy as np\n",
    "    \n",
    "    def encode_text(text):\n",
    "        if pd.isna(text) or not text:\n",
    "            return np.zeros(512).tolist()\n",
    "        \n",
    "        payload = {\"dataframe_records\": [{\"text\": text}]}\n",
    "        response = requests.post(endpoint_url, json=payload, headers=headers)\n",
    "        result = response.json()\n",
    "        \n",
    "        embedding = np.array(result[\"predictions\"])\n",
    "        embedding = embedding / (np.linalg.norm(embedding) + 1e-8)\n",
    "        return embedding.tolist()\n",
    "    \n",
    "    return texts.apply(encode_text)\n",
    "```\n",
    "\n",
    "**Step 2.3:** Generate text embeddings\n",
    "```python\n",
    "text_embeddings_df = (\n",
    "    spark.table(\"product_text_descriptions\")\n",
    "    .repartition(100)  # Parallel processing\n",
    "    .withColumn(\"text_embedding\", generate_text_embedding_udf(col(\"text_content\")))\n",
    ")\n",
    "\n",
    "# Save to table\n",
    "text_embeddings_df.write.mode(\"overwrite\").saveAsTable(\n",
    "    \"main.fashion_demo.product_text_embeddings\"\n",
    ")\n",
    "```\n",
    "\n",
    "**Time:** ~15-20 minutes for 44K products\n",
    "\n",
    "---\n",
    "\n",
    "### Phase 3: Create Multimodal Table (30 mins)\n",
    "\n",
    "**Step 3.1:** Join all embeddings\n",
    "```sql\n",
    "CREATE OR REPLACE TABLE main.fashion_demo.product_embeddings_multimodal AS\n",
    "SELECT \n",
    "  p.*,\n",
    "  img.image_embedding,\n",
    "  txt.text_embedding,\n",
    "  CAST(NULL AS ARRAY<DOUBLE>) as hybrid_embedding  -- Computed next\n",
    "FROM main.fashion_demo.products p\n",
    "INNER JOIN main.fashion_demo.product_image_embeddings img \n",
    "  ON p.product_id = img.product_id\n",
    "INNER JOIN main.fashion_demo.product_text_embeddings txt \n",
    "  ON p.product_id = txt.product_id\n",
    "WHERE img.image_embedding IS NOT NULL \n",
    "  AND txt.text_embedding IS NOT NULL;\n",
    "```\n",
    "\n",
    "**Step 3.2:** Compute hybrid embeddings\n",
    "```python\n",
    "@udf(ArrayType(DoubleType()))\n",
    "def create_hybrid_embedding(image_emb, text_emb):\n",
    "    img_arr = np.array(image_emb)\n",
    "    txt_arr = np.array(text_emb)\n",
    "    hybrid = 0.5 * img_arr + 0.5 * txt_arr\n",
    "    hybrid = hybrid / (np.linalg.norm(hybrid) + 1e-8)\n",
    "    return hybrid.tolist()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    UPDATE main.fashion_demo.product_embeddings_multimodal\n",
    "    SET hybrid_embedding = create_hybrid_embedding(image_embedding, text_embedding)\n",
    "\"\"\")\n",
    "```\n",
    "\n",
    "**Step 3.3:** Verify\n",
    "```sql\n",
    "SELECT \n",
    "  COUNT(*) as total,\n",
    "  COUNT(image_embedding) as has_image,\n",
    "  COUNT(text_embedding) as has_text,\n",
    "  COUNT(hybrid_embedding) as has_hybrid\n",
    "FROM main.fashion_demo.product_embeddings_multimodal;\n",
    "\n",
    "-- Expected: 44,424 / 44,424 / 44,424 / 44,424\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Phase 4: Create Vector Search Indexes (30 mins)\n",
    "\n",
    "**Via Databricks UI:** Compute ‚Üí Vector Search ‚Üí fashion_vector_search ‚Üí Create Index\n",
    "\n",
    "**Index 1: Image Search**\n",
    "* Name: `main.fashion_demo.vs_image_search`\n",
    "* Source: `main.fashion_demo.product_embeddings_multimodal`\n",
    "* Primary Key: `product_id`\n",
    "* Embedding Column: `image_embedding`\n",
    "* Dimension: 512\n",
    "* Sync Mode: Triggered\n",
    "\n",
    "**Index 2: Text Search**\n",
    "* Name: `main.fashion_demo.vs_text_search`\n",
    "* Source: `main.fashion_demo.product_embeddings_multimodal`\n",
    "* Primary Key: `product_id`\n",
    "* Embedding Column: `text_embedding`\n",
    "* Dimension: 512\n",
    "* Sync Mode: Triggered\n",
    "\n",
    "**Index 3: Hybrid Search** ‚≠ê\n",
    "* Name: `main.fashion_demo.vs_hybrid_search`\n",
    "* Source: `main.fashion_demo.product_embeddings_multimodal`\n",
    "* Primary Key: `product_id`\n",
    "* Embedding Column: `hybrid_embedding`\n",
    "* Dimension: 512\n",
    "* Sync Mode: Triggered\n",
    "\n",
    "**Wait:** 5-10 minutes for each index to sync\n",
    "\n",
    "---\n",
    "\n",
    "### Phase 5: Update App Code (1-2 hours)\n",
    "\n",
    "#### File 1: `services/clip_service.py` ‚Üí `services/clip_multimodal_service.py`\n",
    "\n",
    "**Key Changes:**\n",
    "```python\n",
    "class CLIPMultimodalService:\n",
    "    def __init__(self):\n",
    "        self.endpoint_name = \"clip-multimodal-encoder\"  # NEW\n",
    "        self.embedding_dim = 512\n",
    "    \n",
    "    async def get_text_embedding(self, text: str) -> np.ndarray:\n",
    "        \"\"\"NEW: Generate text embedding in shared CLIP space\"\"\"\n",
    "        payload = {\"dataframe_records\": [{\"text\": text}]}\n",
    "        # Call endpoint, parse, normalize\n",
    "        return embedding  # 512 dims\n",
    "    \n",
    "    async def get_image_embedding(self, image_bytes: bytes) -> np.ndarray:\n",
    "        \"\"\"EXISTING: Generate image embedding\"\"\"\n",
    "        # Same as before\n",
    "        return embedding  # 512 dims\n",
    "    \n",
    "    async def get_hybrid_embedding(\n",
    "        self, \n",
    "        text: str, \n",
    "        image_bytes: bytes,\n",
    "        text_weight: float = 0.5\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"NEW: Generate hybrid embedding\"\"\"\n",
    "        text_emb = await self.get_text_embedding(text)\n",
    "        image_emb = await self.get_image_embedding(image_bytes)\n",
    "        \n",
    "        hybrid = text_weight * text_emb + (1 - text_weight) * image_emb\n",
    "        hybrid = hybrid / np.linalg.norm(hybrid)  # Normalize!\n",
    "        \n",
    "        return hybrid\n",
    "```\n",
    "\n",
    "#### File 2: `services/vector_search_service.py`\n",
    "\n",
    "**Key Changes:**\n",
    "```python\n",
    "class VectorSearchService:\n",
    "    def __init__(self):\n",
    "        self.endpoint_name = \"fashion_vector_search\"\n",
    "        self.embedding_dim = 512\n",
    "        # NO hardcoded index_name - pass dynamically!\n",
    "    \n",
    "    async def similarity_search(\n",
    "        self,\n",
    "        query_vector: np.ndarray,\n",
    "        num_results: int = 20,\n",
    "        index_name: str = \"main.fashion_demo.vs_hybrid_search\",  # NEW param\n",
    "        filters: Optional[Dict] = None\n",
    "    ) -> List[Dict]:\n",
    "        # Get index dynamically\n",
    "        index = self._client.get_index(index_name=index_name)  # Keyword arg!\n",
    "        \n",
    "        # Request ALL product columns (now available!)\n",
    "        columns = [\n",
    "            \"product_id\", \"product_display_name\", \"master_category\",\n",
    "            \"sub_category\", \"article_type\", \"base_color\", \"price\",\n",
    "            \"image_path\", \"gender\", \"season\", \"usage\", \"year\"\n",
    "        ]\n",
    "        \n",
    "        results = index.similarity_search(\n",
    "            query_vector=query_vector.tolist(),\n",
    "            columns=columns,\n",
    "            num_results=num_results,\n",
    "            filters=filters\n",
    "        )\n",
    "        \n",
    "        return products  # Complete product data!\n",
    "```\n",
    "\n",
    "#### File 3: `routes/v1/search.py`\n",
    "\n",
    "**New Endpoints:**\n",
    "\n",
    "```python\n",
    "@router.post(\"/text\", response_model=SearchResponse)\n",
    "async def search_by_text_semantic(request: SearchRequest):\n",
    "    \"\"\"üéØ Semantic text search using CLIP text embeddings\"\"\"\n",
    "    from services.clip_multimodal_service import clip_service\n",
    "    from services.vector_search_service import vector_search_service\n",
    "    \n",
    "    # Generate text embedding\n",
    "    text_embedding = await clip_service.get_text_embedding(request.query)\n",
    "    \n",
    "    # Search in IMAGE index (cross-modal!)\n",
    "    products_data = await vector_search_service.similarity_search(\n",
    "        query_vector=text_embedding,\n",
    "        index_name=\"main.fashion_demo.vs_image_search\",  # Text ‚Üí Image!\n",
    "        num_results=request.limit\n",
    "    )\n",
    "    \n",
    "    # Convert to ProductDetail (image URLs already in data)\n",
    "    products = []\n",
    "    for p in products_data:\n",
    "        product = ProductDetail(**p)\n",
    "        product.image_url = get_image_url(int(product.product_id))\n",
    "        product.similarity_score = p.get(\"score\", 0.85)\n",
    "        products.append(product)\n",
    "    \n",
    "    return SearchResponse(\n",
    "        products=products,\n",
    "        query=request.query,\n",
    "        search_type=\"semantic_text\"\n",
    "    )\n",
    "\n",
    "\n",
    "@router.post(\"/image\", response_model=SearchResponse)\n",
    "async def search_by_image(image: UploadFile = File(...), limit: int = Form(20)):\n",
    "    \"\"\"üñºÔ∏è Visual image search\"\"\"\n",
    "    from services.clip_multimodal_service import clip_service\n",
    "    from services.vector_search_service import vector_search_service\n",
    "    \n",
    "    image_bytes = await image.read()\n",
    "    image_embedding = await clip_service.get_image_embedding(image_bytes)\n",
    "    \n",
    "    products_data = await vector_search_service.similarity_search(\n",
    "        query_vector=image_embedding,\n",
    "        index_name=\"main.fashion_demo.vs_image_search\",\n",
    "        num_results=limit\n",
    "    )\n",
    "    \n",
    "    products = [ProductDetail(**p) for p in products_data]\n",
    "    return SearchResponse(products=products, search_type=\"image\")\n",
    "\n",
    "\n",
    "@router.post(\"/hybrid\", response_model=SearchResponse)\n",
    "async def search_hybrid(\n",
    "    query: str = Form(...),\n",
    "    image: Optional[UploadFile] = File(None),\n",
    "    text_weight: float = Form(0.5)\n",
    "):\n",
    "    \"\"\"‚ö° Hybrid search - text + image combined\"\"\"\n",
    "    from services.clip_multimodal_service import clip_service\n",
    "    from services.vector_search_service import vector_search_service\n",
    "    \n",
    "    if image:\n",
    "        image_bytes = await image.read()\n",
    "        hybrid_emb = await clip_service.get_hybrid_embedding(\n",
    "            text=query,\n",
    "            image_bytes=image_bytes,\n",
    "            text_weight=text_weight\n",
    "        )\n",
    "    else:\n",
    "        hybrid_emb = await clip_service.get_text_embedding(query)\n",
    "    \n",
    "    products_data = await vector_search_service.similarity_search(\n",
    "        query_vector=hybrid_emb,\n",
    "        index_name=\"main.fashion_demo.vs_hybrid_search\",\n",
    "        num_results=20\n",
    "    )\n",
    "    \n",
    "    products = [ProductDetail(**p) for p in products_data]\n",
    "    return SearchResponse(products=products, search_type=\"hybrid\")\n",
    "\n",
    "\n",
    "@router.get(\"/recommendations/{user_id}\", response_model=SearchResponse)\n",
    "async def get_recommendations(user_id: str, limit: int = 20):\n",
    "    \"\"\"‚≠ê Personalized recommendations using user embeddings\"\"\"\n",
    "    from services.vector_search_service import vector_search_service\n",
    "    \n",
    "    # Get user embedding from user_style_features\n",
    "    user_features = await repo.get_user_style_features(user_id)\n",
    "    user_embedding = np.array(user_features[\"user_embedding\"])\n",
    "    \n",
    "    # Search in HYBRID index\n",
    "    products_data = await vector_search_service.similarity_search(\n",
    "        query_vector=user_embedding,\n",
    "        index_name=\"main.fashion_demo.vs_hybrid_search\",\n",
    "        num_results=limit\n",
    "    )\n",
    "    \n",
    "    products = [ProductDetail(**p) for p in products_data]\n",
    "    return SearchResponse(products=products, search_type=\"personalized\")\n",
    "```\n",
    "\n",
    "#### File 4: `data/personas.json`\n",
    "\n",
    "**Updated with real user IDs:**\n",
    "```json\n",
    "{\n",
    "  \"personas\": [\n",
    "    {\n",
    "      \"user_id\": \"user_006327\",\n",
    "      \"name\": \"Budget-Conscious Shopper\",\n",
    "      \"segment\": \"budget\",\n",
    "      \"avg_price_point\": 27.55,\n",
    "      \"preferred_categories\": [\"Accessories\", \"Apparel\"],\n",
    "      \"color_prefs\": [\"Black\", \"Brown\", \"Purple\", \"Blue\", \"White\"],\n",
    "      \"min_price\": 24.38,\n",
    "      \"max_price\": 32.89,\n",
    "      \"p25_price\": 24.38,\n",
    "      \"p75_price\": 32.89,\n",
    "      \"num_interactions\": 31\n",
    "    },\n",
    "    {\n",
    "      \"user_id\": \"user_007598\",\n",
    "      \"name\": \"Athletic Performance\",\n",
    "      \"segment\": \"athletic\",\n",
    "      \"avg_price_point\": 46.28,\n",
    "      \"preferred_categories\": [\"Apparel\", \"Footwear\", \"Accessories\"],\n",
    "      \"color_prefs\": [\"Black\", \"White\", \"Navy Blue\", \"Blue\", \"Brown\"],\n",
    "      \"min_price\": 46.28,\n",
    "      \"max_price\": 46.28,\n",
    "      \"p25_price\": 46.28,\n",
    "      \"p75_price\": 46.28,\n",
    "      \"num_interactions\": 30\n",
    "    },\n",
    "    {\n",
    "      \"user_id\": \"user_008828\",\n",
    "      \"name\": \"Luxury Fashionista\",\n",
    "      \"segment\": \"luxury\",\n",
    "      \"avg_price_point\": 120.34,\n",
    "      \"preferred_categories\": [\"Accessories\", \"Apparel\", \"Footwear\"],\n",
    "      \"color_prefs\": [\"Black\", \"White\", \"Brown\", \"Blue\", \"Grey\"],\n",
    "      \"min_price\": 111.32,\n",
    "      \"max_price\": 135.5,\n",
    "      \"p25_price\": 111.32,\n",
    "      \"p75_price\": 135.5,\n",
    "      \"num_interactions\": 29\n",
    "    },\n",
    "    {\n",
    "      \"user_id\": \"user_001328\",\n",
    "      \"name\": \"Casual Accessories Lover\",\n",
    "      \"segment\": \"casual\",\n",
    "      \"avg_price_point\": 29.15,\n",
    "      \"preferred_categories\": [\"Accessories\", \"Apparel\"],\n",
    "      \"color_prefs\": [\"Black\", \"White\", \"Purple\", \"Brown\", \"Steel\"],\n",
    "      \"min_price\": 29.15,\n",
    "      \"max_price\": 29.15,\n",
    "      \"p25_price\": 29.15,\n",
    "      \"p75_price\": 29.15,\n",
    "      \"num_interactions\": 31\n",
    "    },\n",
    "    {\n",
    "      \"user_id\": \"user_009809\",\n",
    "      \"name\": \"Vintage Style Enthusiast\",\n",
    "      \"segment\": \"vintage\",\n",
    "      \"avg_price_point\": 74.20,\n",
    "      \"preferred_categories\": [\"Accessories\", \"Apparel\", \"Footwear\"],\n",
    "      \"color_prefs\": [\"White\", \"Black\", \"Blue\", \"Brown\", \"Silver\"],\n",
    "      \"min_price\": 45.09,\n",
    "      \"max_price\": 103.3,\n",
    "      \"p25_price\": 45.09,\n",
    "      \"p75_price\": 103.3,\n",
    "      \"num_interactions\": 33\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Expected Capabilities\n",
    "\n",
    "### 1. Semantic Text Search (Cross-Modal)\n",
    "**Query:** \"vintage leather jacket\"\n",
    "**Process:**\n",
    "1. CLIP text encoder ‚Üí 512-dim text embedding\n",
    "2. Search vs_image_search index (cross-modal!)\n",
    "3. Returns products whose **images** are similar to the text description\n",
    "\n",
    "**Magic:** Text query finds visually matching products without keywords!\n",
    "\n",
    "### 2. Visual Image Search\n",
    "**Query:** Upload photo of dress\n",
    "**Process:**\n",
    "1. CLIP image encoder ‚Üí 512-dim image embedding\n",
    "2. Search vs_image_search index\n",
    "3. Returns visually similar products\n",
    "\n",
    "### 3. Hybrid Search\n",
    "**Query:** \"red dress\" + inspiration photo\n",
    "**Process:**\n",
    "1. CLIP text encoder ‚Üí text_emb\n",
    "2. CLIP image encoder ‚Üí image_emb\n",
    "3. Hybrid: 0.5 * text_emb + 0.5 * image_emb\n",
    "4. Search vs_hybrid_search index\n",
    "5. Returns red dresses that look like the photo\n",
    "\n",
    "### 4. Personalized Recommendations\n",
    "**Query:** user_008828 (Luxury Fashionista)\n",
    "**Process:**\n",
    "1. Get user_embedding from user_style_features (512 dims)\n",
    "2. Search vs_hybrid_search index\n",
    "3. Returns products matching user's visual + semantic style\n",
    "\n",
    "### 5. Zero-Shot Classification\n",
    "**Query:** \"Is this product formal wear?\"\n",
    "**Process:**\n",
    "```python\n",
    "product_emb = product[\"image_embedding\"]\n",
    "formal_emb = clip_service.get_text_embedding(\"formal elegant business wear\")\n",
    "casual_emb = clip_service.get_text_embedding(\"casual everyday relaxed wear\")\n",
    "\n",
    "formal_sim = cosine_similarity(product_emb, formal_emb)\n",
    "casual_sim = cosine_similarity(product_emb, casual_emb)\n",
    "\n",
    "if formal_sim > casual_sim:\n",
    "    return \"Formal wear\"\n",
    "else:\n",
    "    return \"Casual wear\"\n",
    "```\n",
    "\n",
    "### 6. Latent Feature Extraction\n",
    "**Query:** \"What does dimension 42 represent?\"\n",
    "**Process:**\n",
    "```python\n",
    "# Find products with high values in dimension 42\n",
    "high_dim_42 = products[embeddings[:, 42] > 0.5]\n",
    "\n",
    "# Analyze common attributes\n",
    "print(f\"Category: {high_dim_42['master_category'].mode()}\")\n",
    "print(f\"Color: {high_dim_42['base_color'].mode()}\")\n",
    "print(f\"Avg Price: ${high_dim_42['price'].mean():.2f}\")\n",
    "\n",
    "# Example: Dimension 42 might encode \"formality\"\n",
    "# High values: Suits, formal shoes, dress shirts\n",
    "# Low values: T-shirts, sneakers, casual wear\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ Code Assets in Current Notebook\n",
    "\n",
    "**Ready to copy to repo:**\n",
    "\n",
    "1. **Cell 1**: `data/personas.json` (real user IDs)\n",
    "2. **Cells 14-25**: Multimodal CLIP implementation guide\n",
    "3. **Service code examples**: clip_multimodal_service.py, vector_search_service.py, search.py\n",
    "\n",
    "**Location:** This notebook (Untitled Notebook 2025-12-08 16_01_03)\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Testing Plan\n",
    "\n",
    "### Test 1: Cross-Modal Text Search\n",
    "```bash\n",
    "curl -X POST https://your-app/api/v1/search/text \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"query\": \"red summer dress\", \"limit\": 10}'\n",
    "\n",
    "# Expected: Products that LOOK like red summer dresses\n",
    "# Verify: Check if results are actually red dresses (visual match)\n",
    "```\n",
    "\n",
    "### Test 2: Image Search\n",
    "```bash\n",
    "curl -X POST https://your-app/api/v1/search/image \\\n",
    "  -F \"image=@test_dress.jpg\" \\\n",
    "  -F \"limit=10\"\n",
    "\n",
    "# Expected: Visually similar products\n",
    "# Verify: Results should look similar to uploaded image\n",
    "```\n",
    "\n",
    "### Test 3: Hybrid Search\n",
    "```bash\n",
    "curl -X POST https://your-app/api/v1/search/hybrid \\\n",
    "  -F \"query=red dress\" \\\n",
    "  -F \"image=@inspiration.jpg\" \\\n",
    "  -F \"text_weight=0.5\"\n",
    "\n",
    "# Expected: Red dresses that look like the inspiration photo\n",
    "# Verify: Results match both text and visual criteria\n",
    "```\n",
    "\n",
    "### Test 4: Recommendations\n",
    "```bash\n",
    "# Test each persona\n",
    "for user_id in user_006327 user_007598 user_008828 user_001328 user_009809; do\n",
    "  curl https://your-app/api/v1/search/recommendations/$user_id?limit=8\n",
    "done\n",
    "\n",
    "# Expected: Each persona gets DIFFERENT products\n",
    "# Verify: Products match persona's style and preferences\n",
    "```\n",
    "\n",
    "### Test 5: Zero-Shot Classification\n",
    "```python\n",
    "# In notebook or app\n",
    "product = products_df.filter(col(\"product_id\") == 12345).first()\n",
    "product_emb = np.array(product[\"image_embedding\"])\n",
    "\n",
    "styles = [\n",
    "    \"formal elegant business wear\",\n",
    "    \"casual everyday relaxed wear\",\n",
    "    \"athletic sporty performance wear\",\n",
    "    \"vintage retro classic style\"\n",
    "]\n",
    "\n",
    "for style in styles:\n",
    "    style_emb = await clip_service.get_text_embedding(style)\n",
    "    similarity = np.dot(product_emb, style_emb)\n",
    "    print(f\"{style}: {similarity:.3f}\")\n",
    "\n",
    "# Expected: Highest similarity reveals product's style\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üêõ Known Issues & Fixes\n",
    "\n",
    "### Issue 1: Vector Search Index Columns ‚úÖ FIXED\n",
    "**Error:** \"Requested columns not present in index\"\n",
    "**Fix:** Rebuild index on enriched/multimodal table\n",
    "\n",
    "### Issue 2: get_index() Call ‚úÖ FIXED\n",
    "**Error:** \"Index name must be specified\"\n",
    "**Fix:** Use keyword argument: `get_index(index_name=...)`\n",
    "\n",
    "### Issue 3: OAuth Auth ‚úÖ FIXED\n",
    "**Error:** \"Please specify token\"\n",
    "**Fix:** Pass `personal_access_token=token` to VectorSearchClient\n",
    "\n",
    "### Issue 4: CLIP Response Parsing ‚úÖ FIXED\n",
    "**Issue:** Expected nested array\n",
    "**Fix:** Handle flat array: `{\"predictions\": [0.012, ...]}`\n",
    "\n",
    "### Issue 5: User IDs ‚úÖ FIXED\n",
    "**Issue:** Frontend used fake users (user_001-005)\n",
    "**Fix:** Use real users with embeddings (user_006327, etc.)\n",
    "\n",
    "### Issue 6: Filter Syntax ‚è≥ PENDING\n",
    "**Error:** \"Invalid operator: price <= \"\n",
    "**Note:** Will work once index is rebuilt on enriched table\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Quick Start Checklist\n",
    "\n",
    "### Immediate Fix (Get App Working - 1 hour)\n",
    "- [ ] Create Vector Search index on `product_embeddings_enriched`\n",
    "- [ ] Update `vector_search_service.py` with new index name\n",
    "- [ ] Fix `get_index(index_name=...)` keyword argument\n",
    "- [ ] Update `personas.json` with real user IDs\n",
    "- [ ] Rebuild frontend: `cd frontend && npm run build`\n",
    "- [ ] Redeploy app\n",
    "- [ ] Test: Image search should work\n",
    "\n",
    "### Full Multimodal (Complete Solution - 4-6 hours)\n",
    "- [ ] Deploy `clip-multimodal-encoder` endpoint (text + image)\n",
    "- [ ] Generate text embeddings for 44K products (~20 mins)\n",
    "- [ ] Create hybrid embeddings (0.5 text + 0.5 image)\n",
    "- [ ] Create 3 Vector Search indexes (image, text, hybrid)\n",
    "- [ ] Update `clip_service.py` ‚Üí `clip_multimodal_service.py`\n",
    "- [ ] Update `vector_search_service.py` (dynamic index_name)\n",
    "- [ ] Update `search.py` (semantic text, hybrid endpoints)\n",
    "- [ ] Test cross-modal search\n",
    "- [ ] Test hybrid search\n",
    "- [ ] Analyze latent features\n",
    "- [ ] Redeploy and celebrate! üéâ\n",
    "\n",
    "---\n",
    "\n",
    "## üîó Key Resources\n",
    "\n",
    "**Notebooks:**\n",
    "* `03_image_embeddings_pipeline` - How image embeddings were created (44K products)\n",
    "* `04_vector_search_setup` - Current Vector Search configuration\n",
    "* This notebook - Multimodal implementation plan\n",
    "\n",
    "**Tables:**\n",
    "* `main.fashion_demo.products` - 44,424 products with metadata\n",
    "* `main.fashion_demo.product_image_embeddings` - Image embeddings (512 dims)\n",
    "* `main.fashion_demo.product_embeddings_enriched` - Enriched table (ready!)\n",
    "* `main.fashion_demo.user_style_features` - 5 users with embeddings\n",
    "\n",
    "**Endpoints:**\n",
    "* Vector Search: `fashion_vector_search` (4d329fc8-1924-4131-ace8-14b542f8c14b)\n",
    "* CLIP Image: `clip-image-encoder` (working)\n",
    "* CLIP Multimodal: `clip-multimodal-encoder` (to be deployed)\n",
    "\n",
    "**App Repo:** `/Users/kevin.ippen@databricks.com/fashion-ecom-site/`\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Why This Architecture is Powerful\n",
    "\n",
    "### 1. Shared Embedding Space\n",
    "* Text and image embeddings are **directly comparable**\n",
    "* No separate models or spaces to manage\n",
    "* Cross-modal search \"just works\"\n",
    "\n",
    "### 2. Flexibility\n",
    "* Text-only search: Use text embeddings\n",
    "* Image-only search: Use image embeddings  \n",
    "* Hybrid search: Combine both with custom weights\n",
    "* Recommendations: Use user embeddings (hybrid)\n",
    "\n",
    "### 3. Interpretability\n",
    "* Latent dimensions have semantic meaning\n",
    "* Can analyze what each dimension represents\n",
    "* Explainable recommendations\n",
    "\n",
    "### 4. Zero-Shot Capabilities\n",
    "* Classify products without training: \"Is this formal?\"\n",
    "* Generate tags automatically\n",
    "* Understand product attributes from images\n",
    "\n",
    "### 5. Scalability\n",
    "* All embeddings pre-computed (44K products)\n",
    "* Vector Search handles similarity at scale\n",
    "* No real-time model inference for search (only for new products)\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Success Criteria\n",
    "\n",
    "**Phase 1 (Immediate Fix):**\n",
    "- [x] Text search returns results (no \"columns not present\" error)\n",
    "- [x] Image search returns results\n",
    "- [x] Each persona gets different recommendations\n",
    "- [x] No authentication errors\n",
    "- [x] No index name errors\n",
    "\n",
    "**Phase 2 (Full Multimodal):**\n",
    "- [ ] Text search \"red dress\" finds relevant products (semantic, not keyword)\n",
    "- [ ] Cross-modal: Text query finds visually matching images\n",
    "- [ ] Hybrid search combines text + image\n",
    "- [ ] Zero-shot classification works\n",
    "- [ ] Latent features can be analyzed\n",
    "- [ ] User embeddings work in shared space\n",
    "\n",
    "---\n",
    "\n",
    "## üìû For New Agent Thread\n",
    "\n",
    "**Context to provide:**\n",
    "\n",
    "> \"We're implementing multimodal CLIP search for fashion e-commerce. We have 44K products with image embeddings (512-dim CLIP). Current Vector Search index is broken (missing columns). Need to: 1) Quick fix - rebuild index on enriched table, 2) Full solution - add text embeddings and enable cross-modal search. See MULTIMODAL_CLIP_IMPLEMENTATION_SUMMARY in current notebook for complete details.\"\n",
    "\n",
    "**Key Info:**\n",
    "* Enriched table exists: `main.fashion_demo.product_embeddings_enriched`\n",
    "* CLIP image endpoint working: `clip-image-encoder`\n",
    "* Need to deploy: `clip-multimodal-encoder` (text + image)\n",
    "* App repo: `/Users/kevin.ippen@databricks.com/fashion-ecom-site/`\n",
    "* 5 real users with embeddings: user_006327, user_007598, user_008828, user_001328, user_009809\n",
    "\n",
    "**Next Steps:**\n",
    "1. Create Vector Search index on enriched table (quick fix)\n",
    "2. Deploy CLIP multimodal encoder\n",
    "3. Generate text embeddings\n",
    "4. Create hybrid embeddings\n",
    "5. Create 3 Vector Search indexes\n",
    "6. Update app code (4 files)\n",
    "7. Test and deploy\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary\n",
    "\n",
    "**Current State:**\n",
    "* ‚úÖ 44K products with CLIP image embeddings\n",
    "* ‚úÖ Enriched table ready with all metadata\n",
    "* ‚úÖ CLIP image encoder working\n",
    "* ‚úÖ 5 users with style embeddings\n",
    "* ‚ùå Vector Search index broken (wrong source table)\n",
    "* ‚ùå No text embeddings yet\n",
    "\n",
    "**Quick Fix (1 hour):**\n",
    "* Rebuild index on enriched table\n",
    "* Update app code\n",
    "* Get image search + recommendations working\n",
    "\n",
    "**Full Solution (4-6 hours):**\n",
    "* Deploy CLIP multimodal encoder\n",
    "* Add text embeddings (44K products)\n",
    "* Create hybrid embeddings\n",
    "* Build 3 Vector Search indexes\n",
    "* Enable cross-modal search\n",
    "* Unlock latent feature analysis\n",
    "\n",
    "**Result:** Production-grade multimodal search with semantic understanding, cross-modal queries, and interpretable embeddings! üöÄ"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6160301153476740,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Untitled Notebook 2025-12-09 15_05_24",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
