import os
import logging
from typing import Optional, List

from pydantic_settings import BaseSettings, SettingsConfigDict
from databricks import sdk
from databricks.sdk.core import Config
from sqlalchemy import create_engine, event

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

class Settings(BaseSettings):
    """Application settings with environment variable support"""

    # Pydantic v2 settings
    model_config = SettingsConfigDict(env_file=".env", case_sensitive=True)

    # App
    APP_NAME: str = os.getenv("DATABRICKS_APP_NAME", "Fashion Ecommerce API")
    APP_VERSION: str = "1.0.0"
    DEBUG: bool = bool(os.getenv("DEBUG", "0") == "1")

    # Lakebase Postgres (env is injected when the Lakebase resource is attached to the app)
    LAKEBASE_HOST: str = os.getenv("PGHOST", "")
    LAKEBASE_PORT: int = int(os.getenv("PGPORT", "5432"))
    LAKEBASE_DATABASE: str = os.getenv("PGDATABASE", "databricks_postgres")
    LAKEBASE_USER: str = os.getenv("PGUSER", "")  # SP in Apps; user locally
    LAKEBASE_SSL_MODE: str = "require"

    # Unity Catalog (source metadata)
    CATALOG: str = "ecom"
    SCHEMA: str = "fashion_demo"

    # Lakebase synced tables (Postgres side)
    LAKEBASE_SCHEMA: str = "fashion_demo"
    LAKEBASE_PRODUCTS_TABLE: str = "productsdb"
    LAKEBASE_USERS_TABLE: str = "usersdb"
    LAKEBASE_EMBEDDINGS_TABLE: str = "product_image_embeddingsdb"
    LAKEBASE_USER_FEATURES_TABLE: str = "user_style_featuresdb"

    # Model Serving
    CLIP_ENDPOINT: Optional[str] = os.getenv("CLIP_ENDPOINT")

    # API
    API_PREFIX: str = "/api"
    CORS_ORIGINS: List[str] = ["*"]

    # Pagination
    DEFAULT_PAGE_SIZE: int = 24
    MAX_PAGE_SIZE: int = 100

    @property
    def lakebase_sqlalchemy_url(self) -> str:
        """
        Build SQLAlchemy URL WITHOUT embedding a password; we set the password dynamically
        from a short-lived OAuth token in the SQLAlchemy 'do_connect' hook.
        """
        if not (self.LAKEBASE_HOST and self.LAKEBASE_USER):
            raise RuntimeError(
                "Missing PGHOST/PGUSER env. Ensure the Lakebase resource is attached to the app "
                "so the Postgres host and user are injected into the app environment."
            )
        return (
            f"postgresql+psycopg://{self.LAKEBASE_USER}@"
            f"{self.LAKEBASE_HOST}:{self.LAKEBASE_PORT}/{self.LAKEBASE_DATABASE}"
        )

    def fq_pg(self, table_name: str) -> str:
        """Fully-qualified Postgres table name."""
        return f"{self.LAKEBASE_SCHEMA}.{table_name}"

# Instantiate settings
settings = Settings()

# Databricks SDK workspace client (OAuth M2M; auto-refresh via unified auth)
app_config = Config()  # picks up DATABRICKS_HOST, CLIENT_ID/SECRET, WORKSPACE_ID from env
workspace_client = sdk.WorkspaceClient()

def get_db_engine():
    """
    Create SQLAlchemy engine and inject an OAuth token as the Postgres password
    on each new connection. Tokens are short-lived, so we refresh at connect time.
    """
    engine = create_engine(settings.lakebase_sqlalchemy_url, pool_pre_ping=True)

    @event.listens_for(engine, "do_connect")
    def provide_oauth_token(dialect, conn_rec, cargs, cparams):
        """Inject fresh OAuth token on each connection."""
        try:
            # Refresh OAuth token (~1h lifetime) and set it as Postgres password
            token = workspace_client.config.oauth_token().access_token
            cparams["password"] = token
            logger.info("✓ Injected fresh OAuth token for Lakebase connection")
        except Exception as e:
            logger.error(f"❌ Failed to get OAuth token: {e}")
            raise

    logger.info("✓ Lakebase SQLAlchemy engine initialized with OAuth token injection")
    return engine

def get_bearer_headers() -> dict:
    """
    Helper for calling Databricks services (e.g., Model Serving) with a fresh OAuth token.
    """
    token = workspace_client.config.oauth_token().access_token
    return {"Authorization": f"Bearer {token}"}